import{_ as s,c as a,o as n,e as l}from"./app.73f81c81.js";const o="/assets/shard.57043b71.png",p="/assets/loss.24c62066.png",e="/assets/tp_fsdp.89afb392.png",h=JSON.parse('{"title":"Pytorch2 Tensor Parallelism","description":"Pytorch2 Recap","frontmatter":{"title":"Pytorch2 Tensor Parallelism","description":"Pytorch2 Recap","tags":["AI"]},"headers":[{"level":2,"title":"Pytorch 张量并行 Tutorial","slug":"pytorch-张量并行-tutorial","link":"#pytorch-张量并行-tutorial","children":[{"level":3,"title":"TP 是怎么起效的","slug":"tp-是怎么起效的","link":"#tp-是怎么起效的","children":[]},{"level":3,"title":"什么时候应该使用 TP","slug":"什么时候应该使用-tp","link":"#什么时候应该使用-tp","children":[]},{"level":3,"title":"如何实施 TP","slug":"如何实施-tp","link":"#如何实施-tp","children":[]},{"level":3,"title":"SP 例子","slug":"sp-例子","link":"#sp-例子","children":[]},{"level":3,"title":"LP","slug":"lp","link":"#lp","children":[]},{"level":3,"title":"TP 和 FSDP 结合","slug":"tp-和-fsdp-结合","link":"#tp-和-fsdp-结合","children":[]}]},{"level":2,"title":"张量并行 API","slug":"张量并行-api","link":"#张量并行-api","children":[]},{"level":2,"title":"Device Mesh","slug":"device-mesh","link":"#device-mesh","children":[{"level":3,"title":"案例","slug":"案例","link":"#案例","children":[]}]},{"level":2,"title":"DTensor","slug":"dtensor","link":"#dtensor","children":[]}],"relativePath":"zh/blogs/20240518/index.md"}'),t={name:"zh/blogs/20240518/index.md"},c=l('<nav class="table-of-contents"><ul><li><a href="#pytorch-张量并行-tutorial">Pytorch 张量并行 Tutorial</a><ul><li><a href="#tp-是怎么起效的">TP 是怎么起效的</a></li><li><a href="#什么时候应该使用-tp">什么时候应该使用 TP</a></li><li><a href="#如何实施-tp">如何实施 TP</a></li><li><a href="#sp-例子">SP 例子</a></li><li><a href="#lp">LP</a></li><li><a href="#tp-和-fsdp-结合">TP 和 FSDP 结合</a></li></ul></li><li><a href="#张量并行-api">张量并行 API</a></li><li><a href="#device-mesh">Device Mesh</a><ul><li><a href="#案例">案例</a></li></ul></li><li><a href="#dtensor">DTensor</a></li></ul></nav><h2 id="pytorch-张量并行-tutorial" tabindex="-1">Pytorch 张量并行 Tutorial <a class="header-anchor" href="#pytorch-张量并行-tutorial" aria-hidden="true">#</a></h2><p><a href="https://pytorch.org/tutorials/intermediate/TP_tutorial.html" target="_blank" rel="noreferrer">https://pytorch.org/tutorials/intermediate/TP_tutorial.html</a></p><p>该文档介绍的是如何应用 FSDP (Fully Sharded Data Parallel) 和 TP (Tensor Parallel) 训练 Transformer 类模型。</p><h3 id="tp-是怎么起效的" tabindex="-1">TP 是怎么起效的 <a class="header-anchor" href="#tp-是怎么起效的" aria-hidden="true">#</a></h3><p><a href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noreferrer">Tensor Parallel（TP）</a> 是一个高效的模型并行方法，本文提到的 <a href="https://arxiv.org/abs/2205.05198" target="_blank" rel="noreferrer">Sequence Parallel (SP)</a> 是一种特殊的 TP，它在 <code>nn.LayerNorm</code> 或 <code>RMSNorm</code> 的 sequence 维度上做分片，节省训练期间激活部分的显存占用。当模型变大，这部分占用会很高，所以一般 TP 都是以 SP 的形式实施。</p><p>下图展示了 Transformer 模型的 MLP 层和 Self-Attention 层是怎么采用 TP 的方式进行分片的。原始的矩阵乘法都可以被分片处理。</p><p><img src="'+o+`" alt=""></p><p>Pytorch TP 的工作流程大致如下：</p><h4 id="分片初始化" tabindex="-1">分片初始化： <a class="header-anchor" href="#分片初始化" aria-hidden="true">#</a></h4><ul><li>决定模型各层的并行策略 <code>ParallelStyle</code>，调用 <code>parallelize_module</code>。</li><li>并行化的模型参数被转换为 DTensor，DTensor 负责使用分片的计算方法允许并行化的模型。</li></ul><h4 id="运行时的前向反向更新" tabindex="-1">运行时的前向反向更新 <a class="header-anchor" href="#运行时的前向反向更新" aria-hidden="true">#</a></h4><ul><li>根据用户给 <code>ParallelStyle</code> 指定的输入输出 DTensor 的规模，使用集合通信方法进行转换。</li><li>在并行蹭上进行分片后的计算。</li></ul><h3 id="什么时候应该使用-tp" tabindex="-1">什么时候应该使用 TP <a class="header-anchor" href="#什么时候应该使用-tp" aria-hidden="true">#</a></h3><p>Pytorch 的 FSDP 已经可以实现多卡上分片计算，但是卡数增多后，FSDP 就会有别的问题：</p><ol><li>GPU 数量过大时（128/256），FSDP 以来的 <code>allgather</code> 等集合通信会被通信耗时影响。通过在 FSDP 之上部署 TP/SP，FSDP 的尺度可以被大幅削减，我们可以在节点内部署 TP/SP，节点间使用 FSDP。</li><li>由于收敛性和 GPU 显存限制，全局的 batch size 达到了数据并行的上限，无法继续提升了。TP/SP 成为了唯一的方法去继续拓展到更多的 GPU，这也意味着 GPU 数量和模型规模可以继续扩张。</li><li>特定的模型，当本地的 batch size 变小以后，TP/SP 可以返回对 FLOPS 调优的矩阵乘法。</li></ol><p>当预训练的时候，很容易达到上限。预训练一个数亿大模型的模型会花很多月，即使使用几千个 GPU。</p><ul><li>问题 1 很容易碰到，例如 Llama 2 70B 使用 2000 张 GPU 训练了 35 天，多维并行的规模为 2000。</li><li>当 transformer 模型变大，很快就会碰到问题 2。例如在 Llama 2 的全局 batch size 为 1000 时，即使设置 local batch size 为 1，也不能仅实施 FSDP，因为有 2000 张卡。</li></ul><h3 id="如何实施-tp" tabindex="-1">如何实施 TP <a class="header-anchor" href="#如何实施-tp" aria-hidden="true">#</a></h3><p>Pytorch TP API 提供的是对模型各层进行分片策略的接口。</p><ul><li><p><code>ColwiseParallel</code> 和 <code>RowwiseParallel</code>：以行/列的方式分片 <code>nn.Linear</code> 和 <code>nn.Embedding</code></p></li><li><p><code>SequenceParallel</code>: 对 <code>nn.LayerNorm</code>, <code>nn.Dropout</code>, <code>RMSNormPython</code> 等进行分片</p></li><li><p><code>PrepareModuleInput</code> 和 <code>PrepareModuleOutput</code>: 设定模块的输入输出分片设置，以便采用合适的集合通信操作</p></li></ul><p>以 <a href="https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/llama2_model.py" target="_blank" rel="noreferrer">Llama2</a> 为例，首先借助 <code>DeviceMesh</code> 简洁地初始化 NCCL，TP 一般在节点内使用，如下面这个八卡的用例：</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#676E95;font-style:italic;"># run this via torchrun: torchrun --standalone --nproc_per_node=8 ./tp_tutorial.py</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">device_mesh </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> init_device_mesh</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">tp_mesh </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">init_device_mesh</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">8</span><span style="color:#89DDFF;">,))</span></span>
<span class="line"></span></code></pre></div><p><a href="https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/llama2_model.py" target="_blank" rel="noreferrer">Llama2</a> 的实现中，核心的 <code>TransformerBlock </code> 由 <code>Attention</code> 和 <code>FeedForward</code> 层组成，<code>FeedForward</code> 如下</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#676E95;font-style:italic;"># forward in the FeedForward layer</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">w2</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">F</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">silu</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">w1</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">))</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">w3</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">))</span></span>
<span class="line"></span></code></pre></div><p>显然可以对 <code>w1/w3</code> 按列进行分片，<code>w2</code> 按行进行分片，代码如下，不需要关心底层的集合通信，会被自动执行</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">tensor</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">parallel </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> ColwiseParallel</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> RowwiseParallel</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> parallelize_module</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">layer_tp_plan </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># by default ColwiseParallel input layouts is replicated</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># and RowwiseParallel output layouts is replicated</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_foward.w1</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward.w2</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">RowwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward.w3</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>类似地，对 <code>Attention</code> 模块，我们可以按列分片 <code>q/k/v</code> 的投影过程，然后按行对 <code>wo</code> 的线性投影进行分片。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">layer_tp_plan </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># by default ColwiseParallel input layouts is replicated</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># and RowwiseParallel output layouts is replicated</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention.wq</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention.wk</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention.wv</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention.wo</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">RowwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward.w1</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward.w2</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">RowwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward.w3</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>基本上模型的分片就完成了，值得注意的是人格对线形层进行按列分片，输出就会被按张量的最后一个维度去分片，而按行分片的线形层则接收在最后一个维度上进行分片的输入。如果在按列/按行分片的线性层间还有别的操作（如 <code>view()</code> 操作），需要去调整张量的形状。</p><p>Llama2 的 attention 层中就有 <code>view</code> 操作。<code>wq/ wk/ wv</code> 的线性层是按列分片的，激活张量是在 <code>num_heads</code> 维度上进行分片，所以需要调整 <code>num_heads</code> 为局部的 <code>num_heads</code>。</p><p>最后调用 <code>parallelize_module</code> 将模型转换到 DTensor 上，集合通信就会被自动注册到各个模块的输入和输出上。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#A6ACCD;"> layer_id</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> transformer_block </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">enumerate</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">model</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">layers</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    layer_tp_plan </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span><span style="color:#A6ACCD;">...</span><span style="color:#89DDFF;">}</span><span style="color:#A6ACCD;">  </span><span style="color:#676E95;font-style:italic;"># i.e. the plan we just generated</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># Adjust attention module to use the local number of heads</span></span>
<span class="line"><span style="color:#A6ACCD;">    attn_layer </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> transformer_block</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">attention</span></span>
<span class="line"><span style="color:#A6ACCD;">    attn_layer</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">n_heads</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> attn_layer</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">n_heads</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">//</span><span style="color:#A6ACCD;"> tp_mesh</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">    attn_layer</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">n_kv_heads</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> attn_layer</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">n_kv_heads</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">//</span><span style="color:#A6ACCD;"> tp_mesh</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">parallelize_module</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">module</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">transformer_block</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">device_mesh</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">tp_mesh</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">parallelize_plan</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">layer_tp_plan</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><p>现在我们已经详细阐述了每个 <code>TransformerBlock</code> 的分片计划，通常在第一层中有一个 <code>nn.Embedding</code>，最后一层有一个 <code>nn.Linear</code>投影层，用户可以选择对第一个 <code>nn.Embedding</code> 进行逐行或逐列分片，并对最后一个 <code>nn.Linear</code> 投影层进行逐列分片，同时指定适当的输入和输出布局。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">model </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">parallelize_module</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">    model</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    tp_mesh</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">tok_embeddings</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> RowwiseParallel</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">input_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Replicate</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">output</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> ColwiseParallel</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">output_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Replicate</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><h3 id="sp-例子" tabindex="-1">SP 例子 <a class="header-anchor" href="#sp-例子" aria-hidden="true">#</a></h3><p>SP 是在上面所示的 TP 的基础上进行的。基本的 TP 只在注意力模块和前馈模块中分片张量，会复制它们的模块输入和输出（即前向传递中的激活和反向传递中的梯度）。序列并行则在序列维度上进行分片。</p><p>在典型的 <code>TransformerBlock</code> 中，<code>forward()</code> 函数结合了用于正则化的层（<code>LayerNorm</code> 或 <code>RMSNorm</code>）、注意力层、前馈层和残差连接。例如：</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#676E95;font-style:italic;"># forward in a TransformerBlock</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    h </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> x </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">attention</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">attention_norm</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#A6ACCD;">    out </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> h </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">feed_forward</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">ffn_norm</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">h</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> out</span></span>
<span class="line"></span></code></pre></div><p>在大多数情况下，注意力和前馈模块之外的激活（和梯度）的形状为 [批次大小，序列长度，隐藏维度]。在 DTensor 的术语中，SP 使用 Shard(1) 布局来执行模块的前向传递和反向传递的激活计算。以下代码示例演示如何将序列并行应用于<code>TransformerBlock</code> 中的正则化层：</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">tensor</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">parallel </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#A6ACCD;">    PrepareModuleInput</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    SequenceParallel</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">layer_tp_plan </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># Now the input and output of SequenceParallel has Shard(1) layouts,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># to represent the input/output tensors sharded on the sequence dimension</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention_norm</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">SequenceParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">PrepareModuleInput</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">input_layouts</span><span style="color:#89DDFF;">=(</span><span style="color:#82AAFF;">Shard</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">desired_input_layouts</span><span style="color:#89DDFF;">=(</span><span style="color:#82AAFF;">Replicate</span><span style="color:#89DDFF;">(),),</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention.wq</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention.wk</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention.wv</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention.wo</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">RowwiseParallel</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">output_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Shard</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">ffn_norm</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">SequenceParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">PrepareModuleInput</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">input_layouts</span><span style="color:#89DDFF;">=(</span><span style="color:#82AAFF;">Shard</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">desired_input_layouts</span><span style="color:#89DDFF;">=(</span><span style="color:#82AAFF;">Replicate</span><span style="color:#89DDFF;">(),),</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward.w1</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward.w2</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">RowwiseParallel</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">output_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Shard</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward.w3</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>可以看到，我们现在使用 <code>PrepareModuleInput</code> 将注意力和前馈层的模块输入布局从 <code>Shard(1)</code> 修改为 <code>Replicate()</code>，并将它们的输出布局标记为 <code>Shard(1)</code>。就像 TP 中发生的那样，我们只需要指定输入和输出的张量分片布局，层之间的通信将自动发生。</p><p>需要注意的是，在 SP 中，我们假设 <code>TransformerBlock</code> 的输入和输出始终在序列维度上进行分片，以便多个 <code>TransformerBlock</code> 可以无缝连接。这可以通过显式指定起始的 <code>nn.Embedding</code> 层的输出和最终 <code>nn.Linear</code> 投影层的输入为 <code>Shard(1)</code> 来实现：</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">model </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">parallelize_module</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">    model</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    tp_mesh</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">tok_embeddings</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> RowwiseParallel</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">input_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Replicate</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">output_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Shard</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">norm</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> SequenceParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">output</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> ColwiseParallel</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">input_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Shard</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">output_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Replicate</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><h3 id="lp" tabindex="-1">LP <a class="header-anchor" href="#lp" aria-hidden="true">#</a></h3><p>损失并行 LP（Loss Parallel）用于在计算损失函数时节省内存和通信，因为模型输出通常非常大。在 LP 中，当模型输出在巨大的词汇维度上进行分片时（比如 GPT4 使用的 cl100k_base，具有 10万种 token），可以高效地计算交叉熵损失，而无需将所有模型输出聚集到每个 GPU 上。这不仅显著减少了内存消耗，还通过减少通信开销和并行进行分片计算来提高训练速度。下图简要说明了 LP 如何通过进行分片计算来避免将所有模型输出聚集到每个 GPU 上。</p><p>下图在使用 LP 进行交叉熵损失前向计算。蓝色代表分片张量，绿色代表复制张量，黄色代表具有部分值的张量（将进行全局的 reduction 集合通信）。黑色箭头表示本地计算，红色箭头表示 GPU 之间的集合通信行为。图里分两步是因为在针对类别计算损失的时候，<code>CrossEntropyLoss</code> 可以拆解成 <code>LogSoftmax</code> 和 <code>NLLLoss</code>。</p><p><img src="`+p+`" alt=""></p><p>在 PyTorch 的 TP API中，可以通过上下文管理器 <code>loss_parallel</code> 启用 LP。使用 <code>loss_parallel</code>，可以直接使用 <code>torch.nn.functional.cross_entropy</code> 或 <code>torch.nn.CrossEntropyLoss</code>，而无需修改代码中的其他部分。</p><p>要应用 LP，模型的预测结果通常是形状为[批次大小，序列长度，词汇大小]的张量，应在词汇大小的维度上进行分片。可以通过标记最后一个线性投影层输出的输出布局来轻松实现这一点。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">model </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">parallelize_module</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">    model</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    tp_mesh</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">tok_embeddings</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> RowwiseParallel</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">input_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Replicate</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">output_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Shard</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">norm</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> SequenceParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">output</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> ColwiseParallel</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">input_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Shard</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#676E95;font-style:italic;"># use DTensor as the output</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">use_local_output</span><span style="color:#89DDFF;">=False,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">},</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><p>在上述代码中，我们还在输出之前对规范化层应用了 SP。我们使用<code>use_local_output=False</code>，使输出保持为一个 DTensor，以便在<code>loss_parallel</code> 的上下文种使用。之后，可以直接调用交叉熵损失函数，如下所示。请注意，反向计算也需要在该上下文中进行。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">functional</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#A6ACCD;"> F</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">tensor</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">parallel </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> loss_parallel</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">pred </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">model</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">input_ids</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">with</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">loss_parallel</span><span style="color:#89DDFF;">():</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># assuming pred and labels are of the shape [batch, seq, vocab]</span></span>
<span class="line"><span style="color:#A6ACCD;">    loss </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> F</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">cross_entropy</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">pred</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">flatten</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> labels</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">flatten</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#A6ACCD;">    loss</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">backward</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span></code></pre></div><h3 id="tp-和-fsdp-结合" tabindex="-1">TP 和 FSDP 结合 <a class="header-anchor" href="#tp-和-fsdp-结合" aria-hidden="true">#</a></h3><p>现在我们已经展示了如何将 TP/SP 应用于模型，让我们也看一下如何将 TP 和 FSDP 结合起来使用。由于 TP 会阻塞计算的通信，我们希望确保它在本地的 NVLink 间传输。在实践中，我们通常在 host 内应用 TP，并在 host 间应用FSDP。</p><p><img src="`+e+`" alt=""></p><p>图3. FSDP 和 TP在不同的设备维度上工作，FSDP 通信在 host 间进行，TP 通信在 host 内进行。</p><p>通过 2D DeviceMesh 可以轻松表示这种 2D 并行模式，我们只需要将每个“子” DeviceMesh 传递给各个并行API：</p><p>这样我们就可以轻松地在每个 host 内应用 TP 并在 host 间应用 FSDP，而不需要对 Llama 模型进行任何代码更改。并可以继续增加模型规模，使用大量GPU进行高效训练。</p><h2 id="张量并行-api" tabindex="-1">张量并行 API <a class="header-anchor" href="#张量并行-api" aria-hidden="true">#</a></h2><p><code>torch.distributed.tensor.parallel.parallelize_module(module, device_mesh, parallelize_plan)</code> 接口用于对任意 <code>nn.Module</code> 实行张量并行。它将返回一个并行化的后的 <code>nn.Module</code>。用法如下，指定模型、Device Mesh 和具体参数的并行策略。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">tensor</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">parallel </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> parallelize_module</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> ColwiseParallel</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">device_mesh </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> init_device_mesh</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Define the module.</span></span>
<span class="line"><span style="color:#A6ACCD;">m </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">Model</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">...</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">tp_mesh </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">init_device_mesh</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">8</span><span style="color:#89DDFF;">,))</span></span>
<span class="line"><span style="color:#A6ACCD;">m </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">parallelize_module</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">m</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tp_mesh</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">{</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">w1</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> ColwiseParallel</span><span style="color:#89DDFF;">(),</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">w2</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> RowwiseParallel</span><span style="color:#89DDFF;">()})</span></span>
<span class="line"></span></code></pre></div><p>详细内容阅读<a href="https://pytorch.org/docs/stable/distributed.tensor.parallel.html" target="_blank" rel="noreferrer">文档</a>。</p><h2 id="device-mesh" tabindex="-1">Device Mesh <a class="header-anchor" href="#device-mesh" aria-hidden="true">#</a></h2><p>关于 DeviceMesh，可以参考 <a href="https://pytorch.org/tutorials/recipes/distributed_device_mesh.html" target="_blank" rel="noreferrer">相关 文档</a>，它用于简化多维并行情况下，NCCL 集合通信分组的配置。</p><p>在没有 Device Mesh 的时候，单机八卡想要分成两个节点，每个节点四张卡，做集合通信的话，初始化代码如下</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> os</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> torch</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">distributed</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#A6ACCD;"> dist</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Understand world topology</span></span>
<span class="line"><span style="color:#A6ACCD;">rank </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">int</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">os</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">environ</span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">RANK</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">])</span></span>
<span class="line"><span style="color:#A6ACCD;">world_size </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">int</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">os</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">environ</span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">WORLD_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">])</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;Running example on </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">rank</span><span style="color:#C792EA;">=</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;"> in a world with </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">world_size</span><span style="color:#C792EA;">=</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Create process groups to manage 2-D like parallel pattern</span></span>
<span class="line"><span style="color:#A6ACCD;">dist</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">init_process_group</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">nccl</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">cuda</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">set_device</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">rank</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Create shard groups (e.g. (0, 1, 2, 3), (4, 5, 6, 7))</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># and assign the correct shard group to each rank</span></span>
<span class="line"><span style="color:#A6ACCD;">num_node_devices </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">cuda</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">device_count</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">shard_rank_lists </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">list</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">range</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> num_node_devices </span><span style="color:#89DDFF;">//</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">)),</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">list</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">range</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">num_node_devices </span><span style="color:#89DDFF;">//</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> num_node_devices</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#A6ACCD;">shard_groups </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#A6ACCD;">    dist</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">new_group</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">shard_rank_lists</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">]),</span></span>
<span class="line"><span style="color:#A6ACCD;">    dist</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">new_group</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">shard_rank_lists</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">]),</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">current_shard_group </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#A6ACCD;">    shard_groups</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">]</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> rank </span><span style="color:#89DDFF;">in</span><span style="color:#A6ACCD;"> shard_rank_lists</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">]</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;font-style:italic;">else</span><span style="color:#A6ACCD;"> shard_groups</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">]</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Create replicate groups (for example, (0, 4), (1, 5), (2, 6), (3, 7))</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># and assign the correct replicate group to each rank</span></span>
<span class="line"><span style="color:#A6ACCD;">current_replicate_group </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">None</span></span>
<span class="line"><span style="color:#A6ACCD;">shard_factor </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">len</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">shard_rank_lists</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">])</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#A6ACCD;"> i </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">range</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">num_node_devices </span><span style="color:#89DDFF;">//</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    replicate_group_ranks </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">list</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">range</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">i</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> num_node_devices</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> shard_factor</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#A6ACCD;">    replicate_group </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> dist</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">new_group</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">replicate_group_ranks</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> rank </span><span style="color:#89DDFF;">in</span><span style="color:#A6ACCD;"> replicate_group_ranks</span><span style="color:#89DDFF;">:</span></span>
<span class="line"><span style="color:#A6ACCD;">        current_replicate_group </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> replicate_group</span></span>
<span class="line"></span></code></pre></div><p>可以简化成</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">device_mesh </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> init_device_mesh</span></span>
<span class="line"><span style="color:#A6ACCD;">mesh_2d </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">init_device_mesh</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">mesh_dim_names</span><span style="color:#89DDFF;">=(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">replicate</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">shard</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Users can access the underlying process group thru \`get_group\` API.</span></span>
<span class="line"><span style="color:#A6ACCD;">replicate_group </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> mesh_2d</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">get_group</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">mesh_dim</span><span style="color:#89DDFF;">=</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">replicate</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">shard_group </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> mesh_2d</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">get_group</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">mesh_dim</span><span style="color:#89DDFF;">=</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">shard</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><h3 id="案例" tabindex="-1">案例 <a class="header-anchor" href="#案例" aria-hidden="true">#</a></h3><p>Hybrid Sharding Data Parallel(HSDP) 是一个 2D 策略，在单个节点内实施 FSDP，节点间实施 DDP。</p><p>下面这段代码和上面的分组拓扑一致，分成两个节点，每个节点四张卡；两个节点间 DDP，是要 replicate；节点内 FSDP，是做了 shard。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> torch</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">nn</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#A6ACCD;"> nn</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">device_mesh </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> init_device_mesh</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">fsdp </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> FullyShardedDataParallel </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#A6ACCD;"> FSDP</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> ShardingStrategy</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#C792EA;">class</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">ToyModel</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">ToyModel</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">net1</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">relu</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">ReLU</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">net2</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">5</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">net2</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">relu</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">net1</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)))</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># HSDP: MeshShape(2, 4)</span></span>
<span class="line"><span style="color:#A6ACCD;">mesh_2d </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">init_device_mesh</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#A6ACCD;">model </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">FSDP</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">    ToyModel</span><span style="color:#89DDFF;">(),</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">device_mesh</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">mesh_2d</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">sharding_strategy</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">ShardingStrategy</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">HYBRID_SHARD</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><h2 id="dtensor" tabindex="-1">DTensor <a class="header-anchor" href="#dtensor" aria-hidden="true">#</a></h2><p><a href="https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md" target="_blank" rel="noreferrer">https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md</a></p><p>Pytorch 的 Tensor Parallel 依赖于底层的 DistributedTensor (DTCensor) 数据结构，实现 SPMD（Single Program Multiple Devices），支持 sharding 和 replication。</p><p>需要注意的是当 DTensor 直接用于 Data Parallel 的时候，可能会比 pytorch 中实现的 DDP 和 FSDP 要慢，因为他们有全局的信息，而 DTensor 只是张量级别的。</p>`,77),r=[c];function F(D,y,i,A,C,d){return n(),a("div",null,r)}const _=s(t,[["render",F]]);export{h as __pageData,_ as default};
