import{_ as s,c as n,o as a,e as l}from"./app.bbbac65d.js";const i=JSON.parse('{"title":"PyTorch2 论文笔记","description":"Pytorch2 Recap","frontmatter":{"title":"PyTorch2 论文笔记","description":"Pytorch2 Recap","tags":["AI"]},"headers":[{"level":2,"title":"摘要","slug":"摘要","link":"#摘要","children":[]},{"level":2,"title":"过去的 PyTorch 图捕获方法","slug":"过去的-pytorch-图捕获方法","link":"#过去的-pytorch-图捕获方法","children":[]},{"level":2,"title":"TorchDynamo 设计实现","slug":"torchdynamo-设计实现","link":"#torchdynamo-设计实现","children":[]},{"level":2,"title":"TorchInductor 设计实现","slug":"torchinductor-设计实现","link":"#torchinductor-设计实现","children":[]},{"level":2,"title":"动态规模张量实现","slug":"动态规模张量实现","link":"#动态规模张量实现","children":[]}],"relativePath":"zh/blogs/20240519/index.md"}'),o={name:"zh/blogs/20240519/index.md"},p=l(`<nav class="table-of-contents"><ul><li><a href="#摘要">摘要</a></li><li><a href="#过去的-pytorch-图捕获方法">过去的 PyTorch 图捕获方法</a></li><li><a href="#torchdynamo-设计实现">TorchDynamo 设计实现</a></li><li><a href="#torchinductor-设计实现">TorchInductor 设计实现</a></li><li><a href="#动态规模张量实现">动态规模张量实现</a></li></ul></nav><p>论文 &quot;PyTorch2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation&quot; 介绍了 PyTorch2 中引入的新 API，<a href="https://pytorch.org/docs/stable/torch.compiler.html#torch-compiler" target="_blank" rel="noreferrer">torch.compile</a> 背后的实现原理。它旨在解决中在 Pytorch 中构建计算图的问题，并最终通过编译技术加速代码执行。</p><p>torch.compile 基于以下底层技术：</p><ul><li><p>TorchDynamo（<code>torch._dynamo</code>）：内部API，基于 CPython 的特性，<a href="https://peps.python.org/pep-0523/" target="_blank" rel="noreferrer">PEP523 Frame Evaluation API</a>，以安全地捕获 PyTorch 计算图。</p></li><li><p>TorchInductor：默认的深度学习编译器，可以为多种加速器和后端生成快速的代码。要通过torch.compile实现加速，需要使用后端编译器。对于NVIDIA和AMD GPU，它利用OpenAI Triton作为关键构建块。</p></li><li><p>AOT （Ahead-Of-Time）Autograd：不仅捕获用户级代码，还捕获反向传播。这使得使用 TorchInductor 能够加速前向传播和反向传播。</p></li></ul><p>借助 torch.compile，Pytorch2 的算子数量也显著减少了，参见 <a href="https://pytorch.org/docs/stable/torch.compiler_ir.html" target="_blank" rel="noreferrer">https://pytorch.org/docs/stable/torch.compiler_ir.html</a></p><p><code>torch.compile</code> 的使用很简单，可以通过装饰器加在函数上</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;">@</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">compile</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">opt_foo2</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">y</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    a </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">sin</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    b </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">cos</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">y</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> a </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> b</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">opt_foo2</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">)))</span></span>
<span class="line"></span></code></pre></div><p>可以直接将 <code>torch.nn.Module</code> 作为参数</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">MyModule</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">().</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">lin</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">100</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">functional</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">relu</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">lin</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">mod </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">MyModule</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">opt_mod </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">compile</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">mod</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">opt_mod</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">100</span><span style="color:#89DDFF;">)))</span></span>
<span class="line"></span></code></pre></div><p>或者直接写 <a href="https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html" target="_blank" rel="noreferrer">triton</a> 算子</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> triton</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> triton </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> language </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#A6ACCD;"> tl</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;">@</span><span style="color:#82AAFF;">triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">autotune</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">configs</span><span style="color:#89DDFF;">=[</span></span>
<span class="line"><span style="color:#82AAFF;">        triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Config</span><span style="color:#89DDFF;">({</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">BLOCK_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">},</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_stages</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">3</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_warps</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">8</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Config</span><span style="color:#89DDFF;">({</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">BLOCK_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">},</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_stages</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_warps</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Config</span><span style="color:#89DDFF;">({</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">BLOCK_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">},</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_stages</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">3</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_warps</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">8</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Config</span><span style="color:#89DDFF;">({</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">BLOCK_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">},</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_stages</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_warps</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">],</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">key</span><span style="color:#89DDFF;">=[],</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#89DDFF;">@</span><span style="color:#82AAFF;">triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">jit</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">add_kernel_autotuned</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#A6ACCD;font-style:italic;">in_ptr0</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#A6ACCD;font-style:italic;">in_ptr1</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#A6ACCD;font-style:italic;">out_ptr</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#A6ACCD;font-style:italic;">n_elements</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#A6ACCD;font-style:italic;">BLOCK_SIZE</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">tl.constexpr</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    pid </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> tl</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">program_id</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">axis</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    block_start </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> pid </span><span style="color:#89DDFF;">*</span><span style="color:#A6ACCD;"> BLOCK_SIZE</span></span>
<span class="line"><span style="color:#A6ACCD;">    offsets </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> block_start </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> tl</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">arange</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> BLOCK_SIZE</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    mask </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> offsets </span><span style="color:#89DDFF;">&lt;</span><span style="color:#A6ACCD;"> n_elements</span></span>
<span class="line"><span style="color:#A6ACCD;">    x </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> tl</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">load</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">in_ptr0 </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> offsets</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">mask</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">mask</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    y </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> tl</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">load</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">in_ptr1 </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> offsets</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">mask</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">mask</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    output </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> x </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> y</span></span>
<span class="line"><span style="color:#A6ACCD;">    tl</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">store</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">out_ptr </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> offsets</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> output</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">mask</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">mask</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;">@</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">compile</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">fullgraph</span><span style="color:#89DDFF;">=True)</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">add_fn</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">y</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    output </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">zeros_like</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    n_elements </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> output</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">numel</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">    grid </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">lambda</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">meta</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">cdiv</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">n_elements</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> meta</span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">BLOCK_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">]),)</span></span>
<span class="line"><span style="color:#A6ACCD;">    add_kernel_autotuned</span><span style="color:#89DDFF;">[</span><span style="color:#A6ACCD;">grid</span><span style="color:#89DDFF;">](</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> y</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> output</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> n_elements</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> output</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">x </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">device</span><span style="color:#89DDFF;">=</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">y </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">device</span><span style="color:#89DDFF;">=</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">out </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">add_fn</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> y</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;Vector addition of</span><span style="color:#A6ACCD;">\\n</span><span style="color:#C3E88D;">X:</span><span style="color:#A6ACCD;">\\t</span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">x</span><span style="color:#F78C6C;">}</span><span style="color:#A6ACCD;">\\n</span><span style="color:#C3E88D;">Y:</span><span style="color:#A6ACCD;">\\t</span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">y</span><span style="color:#F78C6C;">}</span><span style="color:#A6ACCD;">\\n</span><span style="color:#C3E88D;">is equal to</span><span style="color:#A6ACCD;">\\n</span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">out</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><h1 id="论文笔记" tabindex="-1">论文笔记 <a class="header-anchor" href="#论文笔记" aria-hidden="true">#</a></h1><h2 id="摘要" tabindex="-1">摘要 <a class="header-anchor" href="#摘要" aria-hidden="true">#</a></h2><p>本文介绍了两个 PyTorch 的扩展，TorchDynamo 和 TorchInductor，它们实现了在 PyTorch2 中发布的 torch.compile 功能。TorchDynamo 是一个基于Python的即时编译器（JIT），它在不损失 Python 灵活性的情况下，使得 PyTorch 程序能够进行图编译。它通过在执行之前动态修改 Python 字节码，并将一系列 PyTorch 操作提取到一个 FX 图中来实现这一目标，然后使用可扩展的后端对其进行 JIT 编译。TorchInductor 是 TorchDynamo 的默认编译器后端，它将PyTorch 程序转换为 OpenAI 的 Triton（用于GPU）和 C++（用于CPU）。这些扩展为在 PyTorch 等 eager-mode 框架中通过编译器应用优化提供了一种新途径。</p><h2 id="过去的-pytorch-图捕获方法" tabindex="-1">过去的 PyTorch 图捕获方法 <a class="header-anchor" href="#过去的-pytorch-图捕获方法" aria-hidden="true">#</a></h2><h2 id="torchdynamo-设计实现" tabindex="-1">TorchDynamo 设计实现 <a class="header-anchor" href="#torchdynamo-设计实现" aria-hidden="true">#</a></h2><h2 id="torchinductor-设计实现" tabindex="-1">TorchInductor 设计实现 <a class="header-anchor" href="#torchinductor-设计实现" aria-hidden="true">#</a></h2><h2 id="动态规模张量实现" tabindex="-1">动态规模张量实现 <a class="header-anchor" href="#动态规模张量实现" aria-hidden="true">#</a></h2><h1 id="cuda-graph-相关缺陷" tabindex="-1">CUDA Graph 相关缺陷 <a class="header-anchor" href="#cuda-graph-相关缺陷" aria-hidden="true">#</a></h1><p><a href="https://pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html" target="_blank" rel="noreferrer">https://pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html</a></p><p>由于 <a href="https://developer.nvidia.com/blog/cuda-graphs/" target="_blank" rel="noreferrer">CUDA Graph</a> 使用了确定性的显存地址，所以前面的执行结果会被后续的执行结果覆盖。也就是说下面的用例会出错</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> torch</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;">@</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">compile</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">mode</span><span style="color:#89DDFF;">=</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">reduce-overhead</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">my_model</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    y </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">matmul</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> y</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">x </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">y1 </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">my_model</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">y2 </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">my_model</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">y1</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># RuntimeError: Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run.</span></span>
<span class="line"></span></code></pre></div><p>Pytorch 在实现 <code>torch.compile</code> 的时候，采用了一种启发式的方法来避免这个问题。在推理过程中会在每次调用 <code>torch.compile</code> 时开始新的迭代；在训练过程中也是如此，只要没有未调用的待处理反向传播。如果这些启发式算法不正确，可以使用 <code>torch.compiler.mark_step_begin()</code> 标记开始新的迭代，或在开始下一次运行之前（在 <code>torch.compile</code> 之外）克隆上一次迭代的张量。</p>`,23),t=[p];function e(c,r,F,y,D,A){return a(),n("div",null,t)}const h=s(o,[["render",e]]);export{i as __pageData,h as default};
