import{_ as s,c as a,o as n,e as o}from"./app.7e378f3b.js";const l="/assets/fig1.caca1932.png",p="/assets/fig3.234de2e1.png",d=JSON.parse('{"title":"PyTorch2 论文笔记","description":"Pytorch2 Recap","frontmatter":{"title":"PyTorch2 论文笔记","description":"Pytorch2 Recap","tags":["AI"]},"headers":[{"level":2,"title":"1 摘要/导言","slug":"_1-摘要-导言","link":"#_1-摘要-导言","children":[]},{"level":2,"title":"2 过去的 PyTorch 图捕获方法","slug":"_2-过去的-pytorch-图捕获方法","link":"#_2-过去的-pytorch-图捕获方法","children":[{"level":3,"title":"2.1 torch.jit.trace","slug":"_2-1-torch-jit-trace","link":"#_2-1-torch-jit-trace","children":[]},{"level":3,"title":"2.2 torch.jit.script","slug":"_2-2-torch-jit-script","link":"#_2-2-torch-jit-script","children":[]},{"level":3,"title":"2.3 Lazy Tensor","slug":"_2-3-lazy-tensor","link":"#_2-3-lazy-tensor","children":[]},{"level":3,"title":"2.4 torch.fx.symbolic_trace","slug":"_2-4-torch-fx-symbolic-trace","link":"#_2-4-torch-fx-symbolic-trace","children":[]},{"level":3,"title":"2.6 和 JAX 中的图捕获的对比","slug":"_2-6-和-jax-中的图捕获的对比","link":"#_2-6-和-jax-中的图捕获的对比","children":[]}]},{"level":2,"title":"3 TorchDynamo 设计实现","slug":"_3-torchdynamo-设计实现","link":"#_3-torchdynamo-设计实现","children":[{"level":3,"title":"3.1 API","slug":"_3-1-api","link":"#_3-1-api","children":[]},{"level":3,"title":"3.2 CPython Frame Evaluation Hook","slug":"_3-2-cpython-frame-evaluation-hook","link":"#_3-2-cpython-frame-evaluation-hook","children":[]},{"level":3,"title":"3.3 Guards","slug":"_3-3-guards","link":"#_3-3-guards","children":[]},{"level":3,"title":"3.4 Symbolic Evaluation","slug":"_3-4-symbolic-evaluation","link":"#_3-4-symbolic-evaluation","children":[]},{"level":3,"title":"3.5 Modeling Python Data Structures","slug":"_3-5-modeling-python-data-structures","link":"#_3-5-modeling-python-data-structures","children":[]},{"level":3,"title":"3.6 Inlining, Control Flow, and Closures","slug":"_3-6-inlining-control-flow-and-closures","link":"#_3-6-inlining-control-flow-and-closures","children":[]},{"level":3,"title":"3.7 Mutation and Side Effects","slug":"_3-7-mutation-and-side-effects","link":"#_3-7-mutation-and-side-effects","children":[]}]},{"level":2,"title":"4 TorchInductor 设计实现","slug":"_4-torchinductor-设计实现","link":"#_4-torchinductor-设计实现","children":[{"level":3,"title":"4.5 Triton 代码生成","slug":"_4-5-triton-代码生成","link":"#_4-5-triton-代码生成","children":[]},{"level":3,"title":"4.6 C++ 代码生成","slug":"_4-6-c-代码生成","link":"#_4-6-c-代码生成","children":[]},{"level":3,"title":"4.7 Wrapper Codegen","slug":"_4-7-wrapper-codegen","link":"#_4-7-wrapper-codegen","children":[]},{"level":3,"title":"4.8 相关的深度学习编译器","slug":"_4-8-相关的深度学习编译器","link":"#_4-8-相关的深度学习编译器","children":[]}]}],"relativePath":"zh/blogs/20240519/index.md"}'),t={name:"zh/blogs/20240519/index.md"},e=o(`<nav class="table-of-contents"><ul><li><a href="#_1-摘要-导言">1 摘要/导言</a></li><li><a href="#_2-过去的-pytorch-图捕获方法">2 过去的 PyTorch 图捕获方法</a><ul><li><a href="#_2-1-torch-jit-trace">2.1 torch.jit.trace</a></li><li><a href="#_2-2-torch-jit-script">2.2 torch.jit.script</a></li><li><a href="#_2-3-lazy-tensor">2.3 Lazy Tensor</a></li><li><a href="#_2-4-torch-fx-symbolic-trace">2.4 torch.fx.symbolic_trace</a></li><li><a href="#_2-6-和-jax-中的图捕获的对比">2.6 和 JAX 中的图捕获的对比</a></li></ul></li><li><a href="#_3-torchdynamo-设计实现">3 TorchDynamo 设计实现</a><ul><li><a href="#_3-1-api">3.1 API</a></li><li><a href="#_3-2-cpython-frame-evaluation-hook">3.2 CPython Frame Evaluation Hook</a></li><li><a href="#_3-3-guards">3.3 Guards</a></li><li><a href="#_3-4-symbolic-evaluation">3.4 Symbolic Evaluation</a></li><li><a href="#_3-5-modeling-python-data-structures">3.5 Modeling Python Data Structures</a></li><li><a href="#_3-6-inlining-control-flow-and-closures">3.6 Inlining, Control Flow, and Closures</a></li><li><a href="#_3-7-mutation-and-side-effects">3.7 Mutation and Side Effects</a></li></ul></li><li><a href="#_4-torchinductor-设计实现">4 TorchInductor 设计实现</a><ul><li><a href="#_4-5-triton-代码生成">4.5 Triton 代码生成</a></li><li><a href="#_4-6-c-代码生成">4.6 C++ 代码生成</a></li><li><a href="#_4-7-wrapper-codegen">4.7 Wrapper Codegen</a></li><li><a href="#_4-8-相关的深度学习编译器">4.8 相关的深度学习编译器</a></li></ul></li></ul></nav><p>论文 &quot;PyTorch2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation&quot; 介绍了 PyTorch2 中引入的新 API，<a href="https://pytorch.org/docs/stable/torch.compiler.html#torch-compiler" target="_blank" rel="noreferrer">torch.compile</a> 背后的实现原理。它旨在解决中在 Pytorch 中构建计算图的问题，并最终通过编译技术加速代码执行。</p><p>torch.compile 基于以下底层技术：</p><ul><li><p>TorchDynamo（<code>torch._dynamo</code>）：内部API，基于 CPython 的特性，<a href="https://peps.python.org/pep-0523/" target="_blank" rel="noreferrer">PEP523 Frame Evaluation API</a>，以安全地捕获 PyTorch 计算图。</p></li><li><p>TorchInductor：默认的深度学习编译器，可以为多种加速器和后端生成快速的代码。要通过torch.compile实现加速，需要使用后端编译器。对于NVIDIA和AMD GPU，它利用OpenAI Triton作为关键构建块。</p></li><li><p>AOT （Ahead-Of-Time）Autograd：不仅捕获用户级代码，还捕获反向传播。这使得使用 TorchInductor 能够加速前向传播和反向传播。</p></li></ul><p>借助 torch.compile，Pytorch2 的算子数量也显著减少了，参见 <a href="https://pytorch.org/docs/stable/torch.compiler_ir.html" target="_blank" rel="noreferrer">https://pytorch.org/docs/stable/torch.compiler_ir.html</a></p><p><code>torch.compile</code> 的使用很简单，可以通过装饰器加在函数上</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;">@</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">compile</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">opt_foo2</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">y</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    a </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">sin</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    b </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">cos</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">y</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> a </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> b</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">opt_foo2</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">)))</span></span>
<span class="line"></span></code></pre></div><p>可以直接将 <code>torch.nn.Module</code> 作为参数</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">MyModule</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">().</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">lin</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">100</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">functional</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">relu</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">lin</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">mod </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">MyModule</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">opt_mod </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">compile</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">mod</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">opt_mod</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">100</span><span style="color:#89DDFF;">)))</span></span>
<span class="line"></span></code></pre></div><p>或者直接写 <a href="https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html" target="_blank" rel="noreferrer">triton</a> 算子</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> triton</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> triton </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> language </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#A6ACCD;"> tl</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;">@</span><span style="color:#82AAFF;">triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">autotune</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">configs</span><span style="color:#89DDFF;">=[</span></span>
<span class="line"><span style="color:#82AAFF;">        triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Config</span><span style="color:#89DDFF;">({</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">BLOCK_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">},</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_stages</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">3</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_warps</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">8</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Config</span><span style="color:#89DDFF;">({</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">BLOCK_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">},</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_stages</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_warps</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Config</span><span style="color:#89DDFF;">({</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">BLOCK_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">},</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_stages</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">3</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_warps</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">8</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Config</span><span style="color:#89DDFF;">({</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">BLOCK_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">},</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_stages</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_warps</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">],</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">key</span><span style="color:#89DDFF;">=[],</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#89DDFF;">@</span><span style="color:#82AAFF;">triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">jit</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">add_kernel_autotuned</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#A6ACCD;font-style:italic;">in_ptr0</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#A6ACCD;font-style:italic;">in_ptr1</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#A6ACCD;font-style:italic;">out_ptr</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#A6ACCD;font-style:italic;">n_elements</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#A6ACCD;font-style:italic;">BLOCK_SIZE</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">tl.constexpr</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    pid </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> tl</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">program_id</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">axis</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    block_start </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> pid </span><span style="color:#89DDFF;">*</span><span style="color:#A6ACCD;"> BLOCK_SIZE</span></span>
<span class="line"><span style="color:#A6ACCD;">    offsets </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> block_start </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> tl</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">arange</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> BLOCK_SIZE</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    mask </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> offsets </span><span style="color:#89DDFF;">&lt;</span><span style="color:#A6ACCD;"> n_elements</span></span>
<span class="line"><span style="color:#A6ACCD;">    x </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> tl</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">load</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">in_ptr0 </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> offsets</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">mask</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">mask</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    y </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> tl</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">load</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">in_ptr1 </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> offsets</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">mask</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">mask</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    output </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> x </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> y</span></span>
<span class="line"><span style="color:#A6ACCD;">    tl</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">store</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">out_ptr </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> offsets</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> output</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">mask</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">mask</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;">@</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">compile</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">fullgraph</span><span style="color:#89DDFF;">=True)</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">add_fn</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">y</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    output </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">zeros_like</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    n_elements </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> output</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">numel</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">    grid </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">lambda</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">meta</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">cdiv</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">n_elements</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> meta</span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">BLOCK_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">]),)</span></span>
<span class="line"><span style="color:#A6ACCD;">    add_kernel_autotuned</span><span style="color:#89DDFF;">[</span><span style="color:#A6ACCD;">grid</span><span style="color:#89DDFF;">](</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> y</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> output</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> n_elements</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> output</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">x </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">device</span><span style="color:#89DDFF;">=</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">y </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">device</span><span style="color:#89DDFF;">=</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">out </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">add_fn</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> y</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;Vector addition of</span><span style="color:#A6ACCD;">\\n</span><span style="color:#C3E88D;">X:</span><span style="color:#A6ACCD;">\\t</span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">x</span><span style="color:#F78C6C;">}</span><span style="color:#A6ACCD;">\\n</span><span style="color:#C3E88D;">Y:</span><span style="color:#A6ACCD;">\\t</span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">y</span><span style="color:#F78C6C;">}</span><span style="color:#A6ACCD;">\\n</span><span style="color:#C3E88D;">is equal to</span><span style="color:#A6ACCD;">\\n</span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">out</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><h1 id="论文笔记" tabindex="-1">论文笔记 <a class="header-anchor" href="#论文笔记" aria-hidden="true">#</a></h1><h2 id="_1-摘要-导言" tabindex="-1">1 摘要/导言 <a class="header-anchor" href="#_1-摘要-导言" aria-hidden="true">#</a></h2><p>作为 eager 模式的框架，pytorch 易于学习和调试，但是难以借助编译器实现图级别的优化。框架只能看到局部的算子信息，无法做算子融合和调度。已有的一些尝试，基于记录/重放、python 解析、懒执行等方法，会影响 pytorch 本身的易用性。记录/重放可能导致错误结果，python 解析无法处理复杂的 python 程序，懒执行的运行时开销太大，因此都不实际。</p><p>本文介绍了两个 PyTorch 的扩展，TorchDynamo 和 TorchInductor，它们实现了在 PyTorch2 中发布的 <code>torch.compile</code> 功能。TorchDynamo 是一个基于Python的即时编译器（JIT），它在不损失 Python 灵活性的情况下，使得 PyTorch 程序能够进行图编译。它挂载在 python 的 frame evaluation API 之上，通过在执行之前动态修改 Python 字节码，并将一系列 PyTorch 操作提取到一个 FX 图中来实现这一目标，然后使用可扩展的后端对其进行 JIT 编译。TorchInductor 是 TorchDynamo 的默认编译器后端，它将PyTorch 程序转换为 OpenAI 的 Triton（用于GPU）和 C++（用于CPU）。这些扩展为在 PyTorch 等 eager-mode 框架中通过编译器应用优化提供了一种新途径。</p><h2 id="_2-过去的-pytorch-图捕获方法" tabindex="-1">2 过去的 PyTorch 图捕获方法 <a class="header-anchor" href="#_2-过去的-pytorch-图捕获方法" aria-hidden="true">#</a></h2><p>eager 模式的框架，难点不仅在于执行时仅有局部信息，还在于它的代码中可以混淆任意 python 代码、第三方库，无法像基于图的框架，限制用户的行为。下面介绍下在实现 TorchDynamo 之前，pytorch 社区的尝试。</p><h3 id="_2-1-torch-jit-trace" tabindex="-1">2.1 torch.jit.trace <a class="header-anchor" href="#_2-1-torch-jit-trace" aria-hidden="true">#</a></h3><p><code>torch.jit.trace</code> 使用记录/重放的方法来构建 TorchScript 图。在 Pytorch dispatcher 的层面进行记录，dispatcher 是用来把算子转换成特定设备的核函数，以及用于自动微分的，用 C++ 实现。因为记录是实现在 C++ 层面，因此它无法捕获到 python 中的控制流。比如下面这个例子</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">example1</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">len</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">nonzero</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">))</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">&gt;</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">:</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> x </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">1</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> x </span><span style="color:#89DDFF;">-</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">1</span></span>
<span class="line"></span></code></pre></div><p>假设输入是 <code>tensor([0,0])</code>，它会捕获到一个等价于下面代码的图</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">example1_incorrect_capture</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">nonzero</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> x </span><span style="color:#89DDFF;">-</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">1</span></span>
<span class="line"></span></code></pre></div><p>显然输入换成 <code>tensor([1,1])</code>，捕获到的图就非法了。同时，python 代码中任意非 pytorch 的部分，也是无法捕获的，如三方库、日志打印、程序执行的副作用。</p><h3 id="_2-2-torch-jit-script" tabindex="-1">2.2 torch.jit.script <a class="header-anchor" href="#_2-2-torch-jit-script" aria-hidden="true">#</a></h3><p><code>torch.jit.script</code> 也用来构建 TorchScript 图，但是是通过解析 python 语法树进行静态分析，它能够正确捕获上面的 example1。但是问题是它在尝试将 python 整个实现成一个静态语言，遇到未实现的 python 组件将会导致它无法工作。它只支持部分模型，而且支持大些模型的工作量很大。</p><h3 id="_2-3-lazy-tensor" tabindex="-1">2.3 Lazy Tensor <a class="header-anchor" href="#_2-3-lazy-tensor" aria-hidden="true">#</a></h3><p>Pytorch/XLA 使用这种方法，它是一个 C++ 层面的方法，每轮迭代它都会延后算子执行，累积成一个图，然后将图喂给 XLA 编译器。它会对图做哈希以避免重复编译。它有效而且通用性强，但是运行时开销大（额外维护图结构）、延迟高（不必要的 device/host 串行）、有时可能频繁触发重新编译。</p><p>目前 Pytorch/XLA 已经集成了 TorchDynamo，它不会在每轮迭代都采用 Lazy Tensor，同时借助 TorchDynamo 来判断是否需要重新捕获图。</p><h3 id="_2-4-torch-fx-symbolic-trace" tabindex="-1">2.4 torch.fx.symbolic_trace <a class="header-anchor" href="#_2-4-torch-fx-symbolic-trace" aria-hidden="true">#</a></h3><p><code>torch.fx.symbolic_trace</code> 也是一个基于记录/重放的图捕获技术，但是它工作在 python 层面，因此可以捕获到 python 中的条件判断。它借助一个代理对象来执行用户的代码，因此可以捕获到关于规模/值的读取行为，而不是像 <code>torch.jit.trace</code> 一样直接代入真实的张量值。它的图表示，<code>FX graph</code> 格式，也被 TorchDynamo 所采纳。</p><p>但是它无法处理下面的例子，</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">example3</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C792EA;">global</span><span style="color:#A6ACCD;"> call_count</span></span>
<span class="line"><span style="color:#A6ACCD;">    call_count </span><span style="color:#89DDFF;">+=</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">1</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">rand</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> x</span></span>
<span class="line"></span></code></pre></div><p>它会生成类似下面代码的图，随机数和全局副作用都丢失了，因为它们不会和 <code>x</code> 的代理变量交互。而且即使全局副作用被捕获了，下游的编译器也大多不支持它，因为机器学习的图结构中几乎都没有 python 中的全局变量的概念。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">example3_incorrect_capture</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> _tensor_constant0 </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> x</span></span>
<span class="line"></span></code></pre></div><h3 id="_2-6-和-jax-中的图捕获的对比" tabindex="-1">2.6 和 JAX 中的图捕获的对比 <a class="header-anchor" href="#_2-6-和-jax-中的图捕获的对比" aria-hidden="true">#</a></h3><p>JAX 天然是和 XLA 联合设计的，而且对用户程序也有限制，<code>jax.jit</code> 不支持依赖数据的 python 控制流，并且要求用户函数是纯函数（无副作用），因此它可以采用一种类似 <code>torch.fx.symbolic_trace</code> 的图捕获技术，并且还更简单。</p><p>与之相对，torch 一开始设计的时候没有编译器的概念，是一个仅支持 eager 模式的框架，已经有大量的模型基于它构建，换条路不现实了。</p><h2 id="_3-torchdynamo-设计实现" tabindex="-1">3 TorchDynamo 设计实现 <a class="header-anchor" href="#_3-torchdynamo-设计实现" aria-hidden="true">#</a></h2><p>TorchDynamo 是一个工作在 CPython 层面的 python 字节码 JIT 编译器。它将 python 字节码翻译到 python 字节码，只是会将原始的字节码中的 pytorch 运算替换成编译后的产物，从而实现 pytorch 的算子融合。下图是它的原理。</p><p><img src="`+l+'" alt=""></p><h3 id="_3-1-api" tabindex="-1">3.1 API <a class="header-anchor" href="#_3-1-api" aria-hidden="true">#</a></h3><p>当使用 <code>torch.compile</code> 运行 pytorch <code>Module</code> 时，自定义的 CPython Frame Evaluation Hook （参见 3.2）将重写正在执行的每个Python 函数的字节码，以提取和编译 PyTorch 算子。这个字节码重写可以被缓存，因为它可能依赖于程序的某些动态属性，需要使用 guards（参见 3.3）来在后续调用中进行检查。</p><h3 id="_3-2-cpython-frame-evaluation-hook" tabindex="-1">3.2 CPython Frame Evaluation Hook <a class="header-anchor" href="#_3-2-cpython-frame-evaluation-hook" aria-hidden="true">#</a></h3><h3 id="_3-3-guards" tabindex="-1">3.3 Guards <a class="header-anchor" href="#_3-3-guards" aria-hidden="true">#</a></h3><h3 id="_3-4-symbolic-evaluation" tabindex="-1">3.4 Symbolic Evaluation <a class="header-anchor" href="#_3-4-symbolic-evaluation" aria-hidden="true">#</a></h3><h3 id="_3-5-modeling-python-data-structures" tabindex="-1">3.5 Modeling Python Data Structures <a class="header-anchor" href="#_3-5-modeling-python-data-structures" aria-hidden="true">#</a></h3><h3 id="_3-6-inlining-control-flow-and-closures" tabindex="-1">3.6 Inlining, Control Flow, and Closures <a class="header-anchor" href="#_3-6-inlining-control-flow-and-closures" aria-hidden="true">#</a></h3><h3 id="_3-7-mutation-and-side-effects" tabindex="-1">3.7 Mutation and Side Effects <a class="header-anchor" href="#_3-7-mutation-and-side-effects" aria-hidden="true">#</a></h3><h2 id="_4-torchinductor-设计实现" tabindex="-1">4 TorchInductor 设计实现 <a class="header-anchor" href="#_4-torchinductor-设计实现" aria-hidden="true">#</a></h2><h3 id="_4-5-triton-代码生成" tabindex="-1">4.5 Triton 代码生成 <a class="header-anchor" href="#_4-5-triton-代码生成" aria-hidden="true">#</a></h3><p>Triton codegen 负责将 TorchInductor 的 IR 映射到 Triton 核函数。下图显示了上述 log2 示例生成的代码。</p><p><img src="'+p+`" alt=""></p><p>该核函数一次处理 <code>XBLOCK</code> 个元素的块。如果元素的数量不是<code>XBLOCK</code> 的倍数，则末尾可能会有一些元素被屏蔽。在代码生成过程中，我们简化了索引。例如，在 IR 中，2D strided 加载被转换为连续加载。代码生成还负责公共子表达式消除（CSE），通过生成代码时使用缓存，并分配以 tmp 开头的临时变量名。pointwise修饰符用于简化启发式块大小、自动调优和预先编译核函数的样板代码。修饰符是正在生成的核函数类型（pointwise、reduction或template），其参数是核函数的必需元数据，例如数据对齐方式。</p><p>在生成 reduction 核函数时，TorchInductor有两种代码生成模式。对于较小的 reduction 操作，它将生成持久 reduction，整个reduction加载到单个块中并在寄存器/共享内存中保留；在这种情况下，reduction 直接映射到 Triton 的 reduction 操作符。对于较大的 reduction，TorchInductor 生成一个循环，将整个块用作累加器，并在循环结束时调用 Triton reduction。</p><p>对于更复杂的操作（矩阵乘法和卷积），TorchInductor 有自己的模板系统，用于生成混合手写 Triton 和生成的 Triton 代码。模板使用 Jinja 编写，与 TorchInductor 的代码生成系统进行交互。</p><h3 id="_4-6-c-代码生成" tabindex="-1">4.6 C++ 代码生成 <a class="header-anchor" href="#_4-6-c-代码生成" aria-hidden="true">#</a></h3><p>CPU 后端会对应到 C++ 代码和 OpenMP。C++ 代码会有向量化和非向量化的实现。向量化的实现会讲大多数运算映射到 pytorch 的<code>at::vec::Vectorized</code> 类。这个类一次会处理 16 个元素，也是标准的 pytorch 核函数调用 SIMD 指令集的方式。非向量化的实现就是转换到 STL 标准库。两个实现都会尝试展开部分循环以实现并行。</p><h3 id="_4-7-wrapper-codegen" tabindex="-1">4.7 Wrapper Codegen <a class="header-anchor" href="#_4-7-wrapper-codegen" aria-hidden="true">#</a></h3><p>Wrapper Codegen 指的是生成调用核函数的代码，它负责计算张量规模、管理显存分配。有两个实现，一个生成 python 代码，另一个生成 C++ 代码，python 后延更加灵活，支持一些边界情况，C++ 代码开销更小。</p><p>当 <code>torch.compile</code> 指定 <code>mode=&quot;reduce-overhead&quot;</code> 时，TrochInductor 会尝试使用 CUDA Graph 来消除 wrapper 代码的开销，基于 API <code>cuStreamBeginCapture</code> 和 <code>cuStreamEndCapture</code>，属于 CUDA 自动图捕获。如果是张量是动态规模，或者不是 CUDA 张量的话，就放弃借助 CUDA Graph。</p><p>注意这里是 CUDA API 层面的图，和上面讲的算子层面的图不同。</p><h3 id="_4-8-相关的深度学习编译器" tabindex="-1">4.8 相关的深度学习编译器 <a class="header-anchor" href="#_4-8-相关的深度学习编译器" aria-hidden="true">#</a></h3><p>pytorch 选用了 OpenAI 的 Triton，因为它可以生成比手写库还快的核函数，大多编译器甚至没有考虑过这方面的工作，面对复杂算子会直接调用手写库。</p><h1 id="cuda-graph-相关缺陷" tabindex="-1">CUDA Graph 相关缺陷 <a class="header-anchor" href="#cuda-graph-相关缺陷" aria-hidden="true">#</a></h1><p><a href="https://pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html" target="_blank" rel="noreferrer">https://pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html</a></p><p>由于 <a href="https://developer.nvidia.com/blog/cuda-graphs/" target="_blank" rel="noreferrer">CUDA Graph</a> 使用了确定性的显存地址，所以前面的执行结果会被后续的执行结果覆盖。也就是说下面的用例会出错</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> torch</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;">@</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">compile</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">mode</span><span style="color:#89DDFF;">=</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">reduce-overhead</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">my_model</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    y </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">matmul</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> y</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">x </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">y1 </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">my_model</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">y2 </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">my_model</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">y1</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># RuntimeError: Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run.</span></span>
<span class="line"></span></code></pre></div><p>Pytorch 在实现 <code>torch.compile</code> 的时候，采用了一种启发式的方法来避免这个问题。在推理过程中会在每次调用 <code>torch.compile</code> 时开始新的迭代；在训练过程中也是如此，只要没有未调用的待处理反向传播。如果这些启发式算法不正确，可以使用 <code>torch.compiler.mark_step_begin()</code> 标记开始新的迭代，或在开始下一次运行之前（在 <code>torch.compile</code> 之外）克隆上一次迭代的张量。</p>`,68),r=[e];function c(y,F,D,i,A,C){return n(),a("div",null,r)}const u=s(t,[["render",c]]);export{d as __pageData,u as default};
