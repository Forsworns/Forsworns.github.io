import{_ as s,c as a,o as n,e as l}from"./app.73f81c81.js";const d=JSON.parse('{"title":"ollama/llama.cpp 源码阅读","description":"最开始是想看下为什么 cuda graph 没有被启用","frontmatter":{"title":"ollama/llama.cpp 源码阅读","description":"最开始是想看下为什么 cuda graph 没有被启用","tags":["llm","gpu"]},"headers":[{"level":2,"title":"ollama","slug":"ollama","link":"#ollama","children":[{"level":3,"title":"llm/","slug":"llm","link":"#llm","children":[]}]},{"level":2,"title":"llama.cpp","slug":"llama-cpp","link":"#llama-cpp","children":[{"level":3,"title":"llama.cpp","slug":"llama-cpp-1","link":"#llama-cpp-1","children":[]},{"level":3,"title":"ggml.c","slug":"ggml-c","link":"#ggml-c","children":[]},{"level":3,"title":"ggml-backend.c","slug":"ggml-backend-c","link":"#ggml-backend-c","children":[]},{"level":3,"title":"ggml-blas.cpp","slug":"ggml-blas-cpp","link":"#ggml-blas-cpp","children":[]},{"level":3,"title":"ggml-cuda.cu","slug":"ggml-cuda-cu","link":"#ggml-cuda-cu","children":[]},{"level":3,"title":"ggml-cuda","slug":"ggml-cuda","link":"#ggml-cuda","children":[]},{"level":3,"title":"examples","slug":"examples","link":"#examples","children":[]}]}],"relativePath":"zh/blogs/20240623/index.md"}'),p={name:"zh/blogs/20240623/index.md"},o=l(`<nav class="table-of-contents"><ul><li><a href="#ollama">ollama</a><ul><li><a href="#llm">llm/</a></li></ul></li><li><a href="#llama-cpp">llama.cpp</a><ul><li><a href="#llama-cpp-1">llama.cpp</a></li><li><a href="#ggml-c">ggml.c</a></li><li><a href="#ggml-backend-c">ggml-backend.c</a></li><li><a href="#ggml-blas-cpp">ggml-blas.cpp</a></li><li><a href="#ggml-cuda-cu">ggml-cuda.cu</a></li><li><a href="#ggml-cuda">ggml-cuda</a></li><li><a href="#examples">examples</a></li></ul></li></ul></nav><h2 id="ollama" tabindex="-1">ollama <a class="header-anchor" href="#ollama" aria-hidden="true">#</a></h2><p>ollama基于 <a href="https://github.com/ollama/ollama/blob/ccef9431c8aae4ecfd0eec6e10377d09cb42f634" target="_blank" rel="noreferrer">https://github.com/ollama/ollama/blob/ccef9431c8aae4ecfd0eec6e10377d09cb42f634</a></p><h3 id="llm" tabindex="-1">llm/ <a class="header-anchor" href="#llm" aria-hidden="true">#</a></h3><h4 id="server-go" tabindex="-1">server.go <a class="header-anchor" href="#server-go" aria-hidden="true">#</a></h4><p>go 写的 server，主体为 <a href="https://github.com/ollama/ollama/blob/ccef9431c8aae4ecfd0eec6e10377d09cb42f634/llm/server.go#L80" target="_blank" rel="noreferrer"><code>NewLlamaServer</code></a></p><p>它会拉起多个进程，分别执行下面的 ext_server/server.cpp 中，基于 llama.cpp 实现的真正做推理服务的 server。</p><h4 id="ext-server-server-cpp" tabindex="-1">ext_server/server.cpp <a class="header-anchor" href="#ext-server-server-cpp" aria-hidden="true">#</a></h4><p>把 llama.cpp 导入成了一个 submodule，基于 llama.cpp 开发的一个推理服务器。</p><h2 id="llama-cpp" tabindex="-1">llama.cpp <a class="header-anchor" href="#llama-cpp" aria-hidden="true">#</a></h2><p>llama.cpp 基于 <a href="https://github1s.com/ggerganov/llama.cpp/blob/45c0e2e4c1268c2d7c8c45536f15e3c9a731ecdc/llama.h" target="_blank" rel="noreferrer">https://github1s.com/ggerganov/llama.cpp/blob/45c0e2e4c1268c2d7c8c45536f15e3c9a731ecdc/llama.h</a></p><p>看编译脚本上是默认开 cuda graph 优化，但是用 ollama 起的服务器跑的时候没有用到。</p><p>CmakeLists.txt 里面声明了只要找到了 libcuda，就会定义 GGML_CUDA_USE_GRAPHS 开启 cuda graph 优化。Makefile 中同样，只要是声明了 <code>make LLAMA_CUDA=1</code> 就会定义 <code>GGML_CUDA_USE_GRAPHS</code> 开启 cuda graph 优化。</p><h3 id="llama-cpp-1" tabindex="-1">llama.cpp <a class="header-anchor" href="#llama-cpp-1" aria-hidden="true">#</a></h3><p>对外的 llama.cpp 库 API 实现</p><h3 id="ggml-c" tabindex="-1">ggml.c <a class="header-anchor" href="#ggml-c" aria-hidden="true">#</a></h3><p>被 llama.cpp 包了一层的内部 API</p><h3 id="ggml-backend-c" tabindex="-1">ggml-backend.c <a class="header-anchor" href="#ggml-backend-c" aria-hidden="true">#</a></h3><p>不同的后端通过 <code>ggml_backend_register</code> 注册自身，<code>ggml_backend_registry_init</code> 运行时分别调用他们，这里利用了一个技巧避免引入头文件。</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">GGML_CALL </span><span style="color:#C792EA;">static</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">void</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ggml_backend_registry_init</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">void</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#82AAFF;">ggml_backend_register</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">CPU</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> ggml_backend_reg_cpu_init</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">ggml_backend_cpu_buffer_type</span><span style="color:#89DDFF;">(),</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">NULL);</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // add forward decls here to avoid including the backend headers</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">#ifdef</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">GGML_USE_CUDA</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">extern</span><span style="color:#F07178;"> GGML_CALL </span><span style="color:#C792EA;">void</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">ggml_backend_cuda_reg_devices</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">void</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#82AAFF;">ggml_backend_cuda_reg_devices</span><span style="color:#89DDFF;">();</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">#endif</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // …</span></span>
<span class="line"><span style="color:#A6ACCD;">}</span></span>
<span class="line"></span></code></pre></div><p>实现 <code>static struct ggml_backend_i cpu_backend_i </code> 后端。</p><h3 id="ggml-blas-cpp" tabindex="-1">ggml-blas.cpp <a class="header-anchor" href="#ggml-blas-cpp" aria-hidden="true">#</a></h3><p>实现 <code>static struct ggml_backend_i blas_backend_i</code> 后端。</p><h3 id="ggml-cuda-cu" tabindex="-1"><a href="http://ggml-cuda.cu" target="_blank" rel="noreferrer">ggml-cuda.cu</a> <a class="header-anchor" href="#ggml-cuda-cu" aria-hidden="true">#</a></h3><p>实现 <code>static ggml_backend_i ggml_backend_cuda_interface</code> 后端。</p><p>cuda graph 是由 <a href="https://github.com/ggerganov/llama.cpp/commit/bc4bba364fb96d908f2698e908648df5e6f55e02" target="_blank" rel="noreferrer">https://github.com/ggerganov/llama.cpp/commit/bc4bba364fb96d908f2698e908648df5e6f55e02</a> 这个 commit-bc4b 引入的。</p><h3 id="ggml-cuda" tabindex="-1">ggml-cuda <a class="header-anchor" href="#ggml-cuda" aria-hidden="true">#</a></h3><h4 id="cpy-cuh" tabindex="-1">cpy.cuh <a class="header-anchor" href="#cpy-cuh" aria-hidden="true">#</a></h4><p>commit-bc4b 为 <code>struct ggml_backend_cuda_context</code> 新增了一个成员，<code>std::unique_ptr&lt;ggml_cuda_graph&gt; cuda_graph</code>。看上去一个 context 只会捕获出一个 cuda graph。</p><p>结构体 <code>ggml_cuda_graph</code> 在析构的时候会自动调用 <code>cudaGraphExecDestroy</code> 和 <code>cudaGraphDestroy</code> 清理之前捕获到的 cuda graph。它的定义比较简单，如下</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">struct</span><span style="color:#F07178;"> </span><span style="color:#FFCB6B;">ggml_cuda_graph</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#FFCB6B;">cudaGraph_t</span><span style="color:#F07178;"> graph </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">nullptr;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#FFCB6B;">cudaGraphExec_t</span><span style="color:#F07178;"> instance </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">nullptr;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">size_t</span><span style="color:#F07178;"> num_nodes </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#FFCB6B;">std</span><span style="color:#89DDFF;">::</span><span style="color:#F07178;">vector</span><span style="color:#89DDFF;">&lt;</span><span style="color:#FFCB6B;">cudaGraphNode_t</span><span style="color:#89DDFF;">&gt;</span><span style="color:#F07178;"> nodes</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#FFCB6B;">std</span><span style="color:#89DDFF;">::</span><span style="color:#F07178;">vector</span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;">cudaKernelNodeParams</span><span style="color:#89DDFF;">&gt;</span><span style="color:#F07178;"> params</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // 禁用该 feature 的几种可能的原因</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">bool</span><span style="color:#F07178;"> disable_due_to_gpu_arch </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">false;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // 如果当前用例中，图节点更新得太快，那图需要一直重建，建图的开销可能会大于 cuda graph 节省的开销。</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">bool</span><span style="color:#F07178;"> disable_due_to_too_many_updates </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">false;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">bool</span><span style="color:#F07178;"> disable_due_to_failed_graph_capture </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">false;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">int</span><span style="color:#F07178;"> number_consecutive_updates </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#FFCB6B;">std</span><span style="color:#89DDFF;">::</span><span style="color:#F07178;">vector</span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;">ggml_graph_node_properties</span><span style="color:#89DDFF;">&gt;</span><span style="color:#F07178;"> ggml_graph_properties</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#FFCB6B;">std</span><span style="color:#89DDFF;">::</span><span style="color:#F07178;">vector</span><span style="color:#89DDFF;">&lt;</span><span style="color:#C792EA;">char</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">**&gt;</span><span style="color:#F07178;"> updated_kernel_arg</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#89DDFF;">};</span></span>
<span class="line"></span>
<span class="line"><span style="color:#C792EA;">struct</span><span style="color:#F07178;"> </span><span style="color:#FFCB6B;">ggml_graph_node_properties</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">void</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">*</span><span style="color:#F07178;"> node_address</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // 同 \`ggml_tensor\` 上的 \`ggml_op\`，例如 \`GGML_OP_CPY\`、\`GGML_OP_VIEW\`</span></span>
<span class="line"><span style="color:#F07178;">    ggml_op node_op</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // 同 \`ggml_tensor\` 上的 \`ne\`</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">int64_t</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">ne</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">GGML_MAX_DIMS</span><span style="color:#89DDFF;">];</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // 同 \`ggml_tensor\` 上的 \`nb\`</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">size_t</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">nb</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">GGML_MAX_DIMS</span><span style="color:#89DDFF;">];</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // 同 \`ggml_tensor\` 上的 \`src[i]-&gt;data\`</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">void</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">*</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">src_address</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">GGML_MAX_SRC</span><span style="color:#89DDFF;">];</span></span>
<span class="line"><span style="color:#89DDFF;">};</span></span>
<span class="line"></span></code></pre></div><p><code>set_ggml_graph_node_properties</code> 从一个 <code>ggml_tensor</code> 构建一个 <code>ggml_graph_node_properties</code>，转换成图里的节点。 <code>ggml_graph_node_has_matching_properties</code> 比较 <code>ggml_tensor</code> 和 <code>ggml_graph_node_properties</code> 的成员，判断二者是否匹配。</p><p><code>ggml_backend_cuda_graph_compute</code> 中根据参数 <code>ggml_backend_t</code> 和 <code>ggml_cgraph</code> 去构建 <code>ggml_backend_t-&gt;ggml_backend_cuda_context-&gt;ggml_cuda_graph</code>。这个函数里面首先检查了是不是安培以下的 GPU，如果是，就不用 cuda graph 了。之前在 T4 上测试的，所以没用到 cuda graph。有点坑爹 release 模式下不开 <code>LLAMA_DEBUG</code>，这错误日志就不打了。</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">GGML_CALL </span><span style="color:#C792EA;">static</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">enum</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">ggml_status</span><span style="color:#A6ACCD;"> ggml_backend_cuda_graph_compute</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">ggml_backend_t</span><span style="color:#A6ACCD;"> backend</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> ggml_cgraph </span><span style="color:#89DDFF;">*</span><span style="color:#A6ACCD;"> cgraph</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">graph</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">nullptr)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">ggml_cuda_info</span><span style="color:#89DDFF;">().</span><span style="color:#A6ACCD;">devices</span><span style="color:#89DDFF;">[</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">device</span><span style="color:#89DDFF;">].</span><span style="color:#A6ACCD;">cc</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;"> CC_AMPERE</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">disable_due_to_gpu_arch</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">true;</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">#ifndef</span><span style="color:#F07178;"> </span><span style="color:#FFCB6B;">NDEBUG</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#82AAFF;">GGML_CUDA_LOG_WARN</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">%s: disabling CUDA graphs due to GPU architecture</span><span style="color:#A6ACCD;">\\n</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> __func__</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">#endif</span></span>
<span class="line"><span style="color:#A6ACCD;">        }</span></span>
<span class="line"><span style="color:#A6ACCD;">    }</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#A6ACCD;">}</span></span>
<span class="line"></span></code></pre></div><p>如果启用 cuda graph，则比较当前传入的 <code>ggml_cgraph</code> 和之前当前的 cuda graph 是否相同</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">GGML_CALL </span><span style="color:#C792EA;">static</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">enum</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">ggml_status</span><span style="color:#A6ACCD;"> ggml_backend_cuda_graph_compute</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">ggml_backend_t</span><span style="color:#A6ACCD;"> backend</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> ggml_cgraph </span><span style="color:#89DDFF;">*</span><span style="color:#A6ACCD;"> cgraph</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">instance</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">nullptr)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        cuda_graph_update_required </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">true;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // Check if the graph size has changed</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">ggml_graph_properties</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">!=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">size_t</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">n_nodes</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        cuda_graph_update_required </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">true;</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">ggml_graph_properties</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">resize</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">n_nodes</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // Loop over nodes in GGML graph to determine if CUDA graph update is required</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // and store properties to allow this comparison for the next token</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">int</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">n_nodes</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i</span><span style="color:#89DDFF;">++)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#C792EA;">bool</span><span style="color:#F07178;"> has_matching_properties </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">true;</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(!</span><span style="color:#F07178;">cuda_graph_update_required</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            has_matching_properties </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">ggml_graph_node_has_matching_properties</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">],</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">ggml_graph_properties</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">]);</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(!</span><span style="color:#F07178;">has_matching_properties</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            cuda_graph_update_required </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">true;</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#82AAFF;">set_ggml_graph_node_properties</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">],</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">ggml_graph_properties</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">]);</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>再次遍历当前的 <code>ggml_cgraph</code>，更新 <code>GGML_OP_CPY</code> 类型节点的信息，因为拷贝操作的地址会随着 token 变化。</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">GGML_CALL </span><span style="color:#C792EA;">static</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">enum</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">ggml_status</span><span style="color:#A6ACCD;"> ggml_backend_cuda_graph_compute</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">ggml_backend_t</span><span style="color:#A6ACCD;"> backend</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> ggml_cgraph </span><span style="color:#89DDFF;">*</span><span style="color:#A6ACCD;"> cgraph</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // Loop over nodes in GGML graph to obtain info needed for CUDA graph</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">updated_kernel_arg</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">clear</span><span style="color:#89DDFF;">();</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">int</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">n_nodes</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i</span><span style="color:#89DDFF;">++)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        ggml_tensor </span><span style="color:#89DDFF;">*</span><span style="color:#F07178;"> node </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">];</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">src</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">]</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;&amp;</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">ggml_backend_buffer_is_cuda_split</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">src</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">]-&gt;</span><span style="color:#A6ACCD;">buffer</span><span style="color:#89DDFF;">))</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            use_cuda_graph </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">false;</span><span style="color:#676E95;font-style:italic;"> // Split buffers are not supported by CUDA graph capture</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> GGML_OP_MUL_MAT_ID</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            use_cuda_graph </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">false;</span><span style="color:#676E95;font-style:italic;"> // This node type is not supported by CUDA graph capture</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> GGML_OP_ADD </span><span style="color:#89DDFF;">&amp;&amp;</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">src</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">]</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;&amp;</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">src</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">]-&gt;</span><span style="color:#A6ACCD;">ne</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">]</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&gt;</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">            // disable CUDA graphs for batch size &gt; 1 for now.</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">            // Changes in batch size or context size can cause changes to the grid size of some kernels.</span></span>
<span class="line"><span style="color:#F07178;">            use_cuda_graph </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">false;</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> GGML_OP_CPY</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">            // store the copy op parameter which changes with each token.</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">updated_kernel_arg</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">push_back</span><span style="color:#89DDFF;">((</span><span style="color:#C792EA;">char</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">**)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;(</span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">src</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">]-&gt;</span><span style="color:#A6ACCD;">data</span><span style="color:#89DDFF;">));</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">            // store a pointer to each copy op CUDA kernel to identify it later</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#C792EA;">void</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">*</span><span style="color:#F07178;"> ptr </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">ggml_cuda_cpy_fn</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">src</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">],</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">src</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">]);</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">std</span><span style="color:#89DDFF;">::</span><span style="color:#82AAFF;">find</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">ggml_cuda_cpy_fn_ptrs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">begin</span><span style="color:#89DDFF;">(),</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">ggml_cuda_cpy_fn_ptrs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">end</span><span style="color:#89DDFF;">(),</span><span style="color:#F07178;"> ptr</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">ggml_cuda_cpy_fn_ptrs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">end</span><span style="color:#89DDFF;">())</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#A6ACCD;">ggml_cuda_cpy_fn_ptrs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">push_back</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">ptr</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(!</span><span style="color:#F07178;">use_cuda_graph</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;font-style:italic;">break</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // Disable CUDA graphs (from the next token) if the use-case is demanding too many consecutive graph updates.</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">use_cuda_graph </span><span style="color:#89DDFF;">&amp;&amp;</span><span style="color:#F07178;"> cuda_graph_update_required</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">number_consecutive_updates</span><span style="color:#89DDFF;">++;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;font-style:italic;">else</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">number_consecutive_updates</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // 连续四次 token 的推理（调用 \`ggml_backend_cuda_graph_compute\`），图都发生了变化，就放弃继续使用 cuda graph</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">number_consecutive_updates</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&gt;=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">disable_due_to_too_many_updates</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">true;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>然后开始借助 cudaStreamBeginCapture 捕获下面的推理过程中的 CUDA API 调用。注意如果没有开启 cuda graph，下面的这段是每次需要 eager 地执行</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">GGML_CALL </span><span style="color:#C792EA;">static</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">enum</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">ggml_status</span><span style="color:#A6ACCD;"> ggml_backend_cuda_graph_compute</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">ggml_backend_t</span><span style="color:#A6ACCD;"> backend</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> ggml_cgraph </span><span style="color:#89DDFF;">*</span><span style="color:#A6ACCD;"> cgraph</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // Only perform the graph execution if CUDA graphs are not enabled, or we are capturing the graph.</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // With the use of CUDA graphs, the execution will be performed by the graph launch.</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(!</span><span style="color:#F07178;">use_cuda_graph </span><span style="color:#89DDFF;">||</span><span style="color:#F07178;"> cuda_graph_update_required</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">int</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">n_nodes</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i</span><span style="color:#89DDFF;">++)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            ggml_tensor </span><span style="color:#89DDFF;">*</span><span style="color:#F07178;"> node </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">];</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">ggml_is_empty</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">node</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">||</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> GGML_OP_RESHAPE </span><span style="color:#89DDFF;">||</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> GGML_OP_TRANSPOSE </span><span style="color:#89DDFF;">||</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> GGML_OP_VIEW </span><span style="color:#89DDFF;">||</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> GGML_OP_PERMUTE </span><span style="color:#89DDFF;">||</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> GGML_OP_NONE</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#89DDFF;font-style:italic;">continue</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#C792EA;">bool</span><span style="color:#F07178;"> ok </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">ggml_cuda_compute_forward</span><span style="color:#89DDFF;">(*</span><span style="color:#F07178;">cuda_ctx</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> node</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(!</span><span style="color:#F07178;">ok</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#82AAFF;">GGML_CUDA_LOG_ERROR</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">%s: op not supported %s (%s)</span><span style="color:#A6ACCD;">\\n</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> __func__</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">name</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">ggml_op_name</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#89DDFF;">));</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#82AAFF;">GGML_ASSERT</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">ok</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>捕获完成，调用 <code>cudaGraphInstantiate</code> 实例化 cuda graph 成 <code>cudaGraphExec</code>，再根据上面统计到的 <code>GGML_OP_CPY</code> 相关的信息，更新 cuda graph，最后调用 <code>cudaGraphExecUpdate</code> 更新 <code>cudaGraphExec</code>。</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">GGML_CALL </span><span style="color:#C792EA;">static</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">enum</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">ggml_status</span><span style="color:#A6ACCD;"> ggml_backend_cuda_graph_compute</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">ggml_backend_t</span><span style="color:#A6ACCD;"> backend</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> ggml_cgraph </span><span style="color:#89DDFF;">*</span><span style="color:#A6ACCD;"> cgraph</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">instance</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">nullptr)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span><span style="color:#676E95;font-style:italic;"> // Create executable graph from captured graph.</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#82AAFF;">CUDA_CHECK</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">cudaGraphInstantiate</span><span style="color:#89DDFF;">(&amp;</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">instance</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">graph</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">NULL,</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">NULL,</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">));</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // Perform update to graph (if required for this token), and change copy parameter (required for every token)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">cuda_graph_update_required</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        // Extract nodes from graph</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        // First call with null argument gets number of nodes in graph</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#82AAFF;">CUDA_CHECK</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">cudaGraphGetNodes</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">graph</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">nullptr,</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">num_nodes</span><span style="color:#89DDFF;">));</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        // Subsequent call with non-null argument gets nodes</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">resize</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">num_nodes</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">params</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">resize</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">num_nodes</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">num_nodes</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&gt;</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#82AAFF;">CUDA_CHECK</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">cudaGraphGetNodes</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">graph</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">data</span><span style="color:#89DDFF;">(),</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">num_nodes</span><span style="color:#89DDFF;">));</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">            // Loop over nodes, and extract kernel parameters from each node</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">size_t</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">num_nodes</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i</span><span style="color:#89DDFF;">++)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">                cudaGraphNodeType node_type</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#82AAFF;">CUDA_CHECK</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">cudaGraphNodeGetType</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">],</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;</span><span style="color:#F07178;">node_type</span><span style="color:#89DDFF;">));</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">node_type </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> cudaGraphNodeTypeKernel</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">                    </span><span style="color:#FFCB6B;">cudaError_t</span><span style="color:#F07178;"> stat </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">cudaGraphKernelNodeGetParams</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">],</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">params</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">]);</span><span style="color:#676E95;font-style:italic;"> // Get params using runtime</span></span>
<span class="line"><span style="color:#F07178;">                    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">stat </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> cudaErrorInvalidDeviceFunction</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">                        // Fails due to incorrect handling by CUDA runtime of CUDA BLAS node.</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">                        // We don&#39;t need to update blas nodes, so clear error and move on.</span></span>
<span class="line"><span style="color:#F07178;">                        </span><span style="color:#82AAFF;">cudaGetLastError</span><span style="color:#89DDFF;">();</span></span>
<span class="line"><span style="color:#F07178;">                    </span><span style="color:#89DDFF;">}</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;font-style:italic;">else</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">                        </span><span style="color:#82AAFF;">GGML_ASSERT</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">stat </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> cudaSuccess</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">                    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // One of the arguments to the copy kernel is updated for each token, hence we need to</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // replace that argument with the updated value in the CUDA graph</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(!</span><span style="color:#F07178;">cuda_graph_update_required</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span><span style="color:#676E95;font-style:italic;"> // on update steps, the live parameters will already be captured</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#C792EA;">int</span><span style="color:#F07178;"> k </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">size_t</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">num_nodes</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i</span><span style="color:#89DDFF;">++)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">count</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">ggml_cuda_cpy_fn_ptrs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">begin</span><span style="color:#89DDFF;">(),</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">ggml_cuda_cpy_fn_ptrs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">end</span><span style="color:#89DDFF;">(),</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">params</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">].</span><span style="color:#A6ACCD;">func</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&gt;</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#C792EA;">char</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">**</span><span style="color:#F07178;"> updated_kernel_arg_ptr </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">updated_kernel_arg</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">at</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">k</span><span style="color:#89DDFF;">++);</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">params</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">].</span><span style="color:#A6ACCD;">kernelParams</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">]</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> updated_kernel_arg_ptr</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#82AAFF;">CUDA_CHECK</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">cudaGraphKernelNodeSetParams</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">],</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">params</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">]));</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // Update graph executable</span></span>
<span class="line"><span style="color:#F07178;">    cudaGraphExecUpdateResultInfo result_info</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#FFCB6B;">cudaError_t</span><span style="color:#F07178;"> stat </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">cudaGraphExecUpdate</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">instance</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">graph</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;</span><span style="color:#F07178;">result_info</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>有了新的 <code>cudaGraphExec</code>，就可以运行这个图了。</p><h4 id="cpy-cu" tabindex="-1"><a href="http://cpy.cu" target="_blank" rel="noreferrer">cpy.cu</a> <a class="header-anchor" href="#cpy-cu" aria-hidden="true">#</a></h4><p>commit-bc4b 定义了一个通用的函数 <code>ggml_cuda_cpy_fn</code> 根据张量类型选择拷贝函数。</p><h3 id="examples" tabindex="-1">examples <a class="header-anchor" href="#examples" aria-hidden="true">#</a></h3><h4 id="simple" tabindex="-1">simple <a class="header-anchor" href="#simple" aria-hidden="true">#</a></h4><p><a href="https://github1s.com/ggerganov/llama.cpp/blob/45c0e2e4c1268c2d7c8c45536f15e3c9a731ecdc/examples/simple/simple.cpp" target="_blank" rel="noreferrer">https://github1s.com/ggerganov/llama.cpp/blob/45c0e2e4c1268c2d7c8c45536f15e3c9a731ecdc/examples/simple/simple.cpp</a></p><p>一个简单 llama.cpp 应用用例。</p>`,49),e=[o];function c(t,r,F,y,D,i){return n(),a("div",null,e)}const C=s(p,[["render",c]]);export{d as __pageData,C as default};
