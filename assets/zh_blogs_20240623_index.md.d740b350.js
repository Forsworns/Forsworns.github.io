import{_ as a,c as e,o as l,e as s}from"./app.7e53e423.js";const _=JSON.parse('{"title":"ollama/llama.cpp 源码阅读","description":"最开始是想看下为什么 cuda graph 没有被启用","frontmatter":{"title":"ollama/llama.cpp 源码阅读","description":"最开始是想看下为什么 cuda graph 没有被启用","tags":["llm","gpu"]},"headers":[{"level":2,"title":"llm/","slug":"llm","link":"#llm","children":[{"level":3,"title":"server.go","slug":"server-go","link":"#server-go","children":[]},{"level":3,"title":"ext_server/server.cpp","slug":"ext-server-server-cpp","link":"#ext-server-server-cpp","children":[]}]},{"level":2,"title":"llama.cpp","slug":"llama-cpp-1","link":"#llama-cpp-1","children":[]},{"level":2,"title":"ggml.c","slug":"ggml-c","link":"#ggml-c","children":[]},{"level":2,"title":"ggml-backend.c","slug":"ggml-backend-c","link":"#ggml-backend-c","children":[]},{"level":2,"title":"ggml-cuda.cu","slug":"ggml-cuda-cu","link":"#ggml-cuda-cu","children":[]},{"level":2,"title":"ggml-blas.cpp","slug":"ggml-blas-cpp","link":"#ggml-blas-cpp","children":[]}],"relativePath":"zh/blogs/20240623/index.md"}'),c={name:"zh/blogs/20240623/index.md"},n=s(`<nav class="table-of-contents"><ul><li><a href="#llm">llm/</a><ul><li><a href="#server-go">server.go</a></li><li><a href="#ext-server-server-cpp">ext_server/server.cpp</a></li></ul></li><li><a href="#llama-cpp-1">llama.cpp</a></li><li><a href="#ggml-c">ggml.c</a></li><li><a href="#ggml-backend-c">ggml-backend.c</a></li><li><a href="#ggml-cuda-cu">ggml-cuda.cu</a></li><li><a href="#ggml-blas-cpp">ggml-blas.cpp</a></li></ul></nav><h1 id="ollama" tabindex="-1">ollama <a class="header-anchor" href="#ollama" aria-hidden="true">#</a></h1><p>ollama基于 <a href="https://github.com/ollama/ollama/blob/ccef9431c8aae4ecfd0eec6e10377d09cb42f634" target="_blank" rel="noreferrer">https://github.com/ollama/ollama/blob/ccef9431c8aae4ecfd0eec6e10377d09cb42f634</a></p><h2 id="llm" tabindex="-1">llm/ <a class="header-anchor" href="#llm" aria-hidden="true">#</a></h2><h3 id="server-go" tabindex="-1">server.go <a class="header-anchor" href="#server-go" aria-hidden="true">#</a></h3><p>go 写的 server，主体为 <a href="https://github.com/ollama/ollama/blob/ccef9431c8aae4ecfd0eec6e10377d09cb42f634/llm/server.go#L80" target="_blank" rel="noreferrer"><code>NewLlamaServer</code></a></p><p>它会拉起多个进程，分别执行下面的 ext_server/server.cpp 中，基于 llama.cpp 实现的真正做推理服务的 server。</p><h3 id="ext-server-server-cpp" tabindex="-1">ext_server/server.cpp <a class="header-anchor" href="#ext-server-server-cpp" aria-hidden="true">#</a></h3><p>把 llama.cpp 导入成了一个 submodule，基于 llama.cpp 开发的一个推理服务器。</p><h1 id="llama-cpp" tabindex="-1">llama.cpp <a class="header-anchor" href="#llama-cpp" aria-hidden="true">#</a></h1><p>llama.cpp 基于 <a href="https://github1s.com/ggerganov/llama.cpp/blob/45c0e2e4c1268c2d7c8c45536f15e3c9a731ecdc/llama.h" target="_blank" rel="noreferrer">https://github1s.com/ggerganov/llama.cpp/blob/45c0e2e4c1268c2d7c8c45536f15e3c9a731ecdc/llama.h</a></p><p>看编译脚本上是默认开 cuda graph 优化，但是用 ollama 起的服务器跑的时候没有用到。</p><p>CmakeLists.txt 里面声明了只要找到了 libcuda，就会定义 GGML_CUDA_USE_GRAPHS 开启 cuda graph 优化。Makefile 中同样，只要是声明了 <code>make LLAMA_CUDA=1</code> 就会定义 <code>GGML_CUDA_USE_GRAPHS</code> 开启 cuda graph 优化。</p><h2 id="llama-cpp-1" tabindex="-1">llama.cpp <a class="header-anchor" href="#llama-cpp-1" aria-hidden="true">#</a></h2><p>对外的 llama.cpp 库 API 实现</p><h2 id="ggml-c" tabindex="-1">ggml.c <a class="header-anchor" href="#ggml-c" aria-hidden="true">#</a></h2><p>被 llama.cpp 包了一层的内部 API</p><h2 id="ggml-backend-c" tabindex="-1">ggml-backend.c <a class="header-anchor" href="#ggml-backend-c" aria-hidden="true">#</a></h2><p>不同的后端通过 <code>ggml_backend_register</code> 注册自身，<code>ggml_backend_registry_init</code> 运行时分别调用他们，这里利用了一个技巧避免引入头文件。</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">GGML_CALL </span><span style="color:#C792EA;">static</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">void</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ggml_backend_registry_init</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">void</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#82AAFF;">ggml_backend_register</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">CPU</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> ggml_backend_reg_cpu_init</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">ggml_backend_cpu_buffer_type</span><span style="color:#89DDFF;">(),</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">NULL);</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // add forward decls here to avoid including the backend headers</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">#ifdef</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">GGML_USE_CUDA</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">extern</span><span style="color:#F07178;"> GGML_CALL </span><span style="color:#C792EA;">void</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">ggml_backend_cuda_reg_devices</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">void</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#82AAFF;">ggml_backend_cuda_reg_devices</span><span style="color:#89DDFF;">();</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">#endif</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // …</span></span>
<span class="line"><span style="color:#A6ACCD;">}</span></span>
<span class="line"></span></code></pre></div><p>实现 <code>static struct ggml_backend_i cpu_backend_i </code> 后端。</p><h2 id="ggml-cuda-cu" tabindex="-1"><a href="http://ggml-cuda.cu" target="_blank" rel="noreferrer">ggml-cuda.cu</a> <a class="header-anchor" href="#ggml-cuda-cu" aria-hidden="true">#</a></h2><p>实现 <code>static ggml_backend_i ggml_backend_cuda_interface</code> 后端。</p><h2 id="ggml-blas-cpp" tabindex="-1">ggml-blas.cpp <a class="header-anchor" href="#ggml-blas-cpp" aria-hidden="true">#</a></h2><p>实现 <code>static struct ggml_backend_i blas_backend_i</code> 后端。</p>`,25),p=[n];function r(o,t,i,d,g,h){return l(),e("div",null,p)}const u=a(c,[["render",r]]);export{_ as __pageData,u as default};
