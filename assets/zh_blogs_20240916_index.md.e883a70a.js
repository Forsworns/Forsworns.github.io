import{_ as e,c as r,o as t,e as n}from"./app.db17c96a.js";const _=JSON.parse('{"title":"TensorRT-LLM 源码阅读","description":"再看看 cuda graph 咋开启的 ...","frontmatter":{"title":"TensorRT-LLM 源码阅读","description":"再看看 cuda graph 咋开启的 ...","tags":["LLM","AI","GPU","CUDA"]},"headers":[],"relativePath":"zh/blogs/20240916/index.md"}'),o={name:"zh/blogs/20240916/index.md"},p=n('<nav class="table-of-contents"><ul></ul></nav><p>基于 TensorRT-LLM-0.10.0，<a href="https://github.com/NVIDIA/TensorRT-LLM/releases/tag/v0.10.0" target="_blank" rel="noreferrer">https://github.com/NVIDIA/TensorRT-LLM/releases/tag/v0.10.0</a> 最开始想看下 cuda graph 怎么开启的。 trtllm-build 工具不像 trtexec，有 trtexec --useCudaGraph 这个选项</p><h1 id="cpp-tensorrt-llm-runtime-gptsession-cpp" tabindex="-1">cpp/tensorrt_llm/runtime/gptSession.cpp <a class="header-anchor" href="#cpp-tensorrt-llm-runtime-gptsession-cpp" aria-hidden="true">#</a></h1><p>cpp runtime 中关于 cuda graph API，是 <code>GptSession::mCudaGraphMode</code> 这个变量控制的，它被设置成了外部的 <code>tr::GptSession::Config::cudaGraphMode</code> 的配置值。</p><h1 id="cpp-tensorrt-llm-pybind-bindings-cpp" tabindex="-1">cpp/tensorrt_llm/pybind/bindings.cpp <a class="header-anchor" href="#cpp-tensorrt-llm-pybind-bindings-cpp" aria-hidden="true">#</a></h1><p>python binding 文件，可以看到例如 cpp 中的 <code>tr::GptSession::Config</code> 被映射成了 python 中的 <code>GptSessionConfig</code>。 它的成员被映射成了 <code>GptSessionConfig::cuda_graph_mode</code>。</p><h1 id="tensorrt-llm-runtime-model-runner-cpp-py" tabindex="-1">tensorrt_llm/runtime/model_runner_cpp.py <a class="header-anchor" href="#tensorrt-llm-runtime-model-runner-cpp-py" aria-hidden="true">#</a></h1><p>包装了 cpp 目录下的具体实现，暴露成 python 接口。从 <code>ModelRunnerCppGptSession::from_dir()</code> 的实现里面实际上可以看到 <code>GptSessionConfig</code> 这些配置项都是怎么传递进去的。实际上没有传递 cuda graph 那个参数。</p><h1 id="tensorrt-llm-runtime-generation-py" tabindex="-1">tensorrt_llm/runtime/generation.py <a class="header-anchor" href="#tensorrt-llm-runtime-generation-py" aria-hidden="true">#</a></h1><p>python runtime 中，同样是在 decode 阶段使用 cuda graph 加速。</p><p>在调用 <code>tensorrt_llm.runtime.GenerationSession()</code> 的时候，配置一下 <code>cuda_graph_mode=True</code> 即可。也就是改一下例如 <code>/examples/llama/summarize_long.py</code> 这样的示例代码。 tensorrt-llm 自己的 benchmark 里面倒是加了对应的开关，只不过是示例代码里面省略了。</p>',11),a=[p];function s(d,c,i,l,h,m){return t(),r("div",null,a)}const g=e(o,[["render",s]]);export{_ as __pageData,g as default};
