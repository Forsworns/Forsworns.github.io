import{_ as s,c as a,o as e,e as n}from"./app.db17c96a.js";const d=JSON.parse('{"title":"kTransformers 源码阅读","description":"可便捷调优的推理服务器","frontmatter":{"title":"kTransformers 源码阅读","description":"可便捷调优的推理服务器","tags":["LLM","AI","GPU","CUDA"]},"headers":[{"level":2,"title":"ktransformers/server/api/init.py","slug":"ktransformers-server-api-init-py","link":"#ktransformers-server-api-init-py","children":[]},{"level":2,"title":"ktransformers/server/api/openai/endpoints/chat.py","slug":"ktransformers-server-api-openai-endpoints-chat-py","link":"#ktransformers-server-api-openai-endpoints-chat-py","children":[]},{"level":2,"title":"ktransformers/server/utils/create_interface.py","slug":"ktransformers-server-utils-create-interface-py","link":"#ktransformers-server-utils-create-interface-py","children":[]},{"level":2,"title":"ktransformers/models/","slug":"ktransformers-models","link":"#ktransformers-models","children":[]},{"level":2,"title":"ktransformers/operators/","slug":"ktransformers-operators","link":"#ktransformers-operators","children":[]},{"level":2,"title":"ktransformers/util/cuda_graph_runner.py","slug":"ktransformers-util-cuda-graph-runner-py","link":"#ktransformers-util-cuda-graph-runner-py","children":[{"level":3,"title":"ktransformers/server/backend/interfaces/ktransformers.py","slug":"ktransformers-server-backend-interfaces-ktransformers-py","link":"#ktransformers-server-backend-interfaces-ktransformers-py","children":[]},{"level":3,"title":"ktransformers/util/utils.py","slug":"ktransformers-util-utils-py","link":"#ktransformers-util-utils-py","children":[]}]}],"relativePath":"zh/blogs/20240924/index.md"}'),r={name:"zh/blogs/20240924/index.md"},o=n(`<nav class="table-of-contents"><ul><li><a href="#ktransformers-server-api-init-py">ktransformers/server/api/init.py</a></li><li><a href="#ktransformers-server-api-openai-endpoints-chat-py">ktransformers/server/api/openai/endpoints/chat.py</a></li><li><a href="#ktransformers-server-utils-create-interface-py">ktransformers/server/utils/create_interface.py</a></li><li><a href="#ktransformers-models">ktransformers/models/</a></li><li><a href="#ktransformers-operators">ktransformers/operators/</a></li><li><a href="#ktransformers-util-cuda-graph-runner-py">ktransformers/util/cuda_graph_runner.py</a><ul><li><a href="#ktransformers-server-backend-interfaces-ktransformers-py">ktransformers/server/backend/interfaces/ktransformers.py</a></li><li><a href="#ktransformers-util-utils-py">ktransformers/util/utils.py</a></li></ul></li></ul></nav><p>基于 <a href="https://github.com/kvcache-ai/ktransformers/tree/0f054fe4ff73133378a8de8ae17f47d5f3ec680a" target="_blank" rel="noreferrer">https://github.com/kvcache-ai/ktransformers/tree/0f054fe4ff73133378a8de8ae17f47d5f3ec680a</a></p><p>kvcache-ai/ktransformers 是一个很有趣的推理服务器，只需要写一个 YAML 配置文件，就可以动态地注入借助 SIMD、<a href="https://github.com/IST-DASLab/marlin" target="_blank" rel="noreferrer">marlin</a> 等方法调优后的模块，替换原始模型中的结构。</p><p>清华的章明星老师有一个讲座视频：<a href="https://www.bilibili.com/video/BV1CAW6eiENy/" target="_blank" rel="noreferrer">https://www.bilibili.com/video/BV1CAW6eiENy/</a></p><p>先从服务端入口读起来</p><h2 id="ktransformers-server-api-init-py" tabindex="-1">ktransformers/server/api/<strong>init</strong>.py <a class="header-anchor" href="#ktransformers-server-api-init-py" aria-hidden="true">#</a></h2><p>服务端的路由定义，支持多种风格：ollama、OpenAI、web</p><h2 id="ktransformers-server-api-openai-endpoints-chat-py" tabindex="-1">ktransformers/server/api/openai/endpoints/chat.py <a class="header-anchor" href="#ktransformers-server-api-openai-endpoints-chat-py" aria-hidden="true">#</a></h2><p>OpenAI 接口实现在这里,使用的是 fastapi 构建的服务器，一些用户参数的解析可以在 ktransformers/server/schemas/endpoints/chat.py 里面看到，例如 <code>/chat/completions</code> 的参数 <code>ChatCompletionCreate</code> 现在其实只支持了 steram 类型响应，也忽略了 model 参数。</p><h2 id="ktransformers-server-utils-create-interface-py" tabindex="-1">ktransformers/server/utils/create_interface.py <a class="header-anchor" href="#ktransformers-server-utils-create-interface-py" aria-hidden="true">#</a></h2><p>从上面 OpenAI 的 <code>/chat/completions</code> 实现可以继续找到后端推理服务的实现。该文件内实现了一个全局单例 GlobalInterface，server 在处理推理请求的时候都会去获取这个单例，调用 <code>inference</code> 接口执行推理，目前支持两种后端：transformers、ktransformers，都在 ktransformers/server/backend/interfaces 路径下，还有个 <code>exllamav2</code> 看上去还未实现。</p><p>ktransformers 和 transformers 后端间是存在继承关系的：<code>KTransformersInterface -&gt; TransformersInterface -&gt; BackendInterfaceBase</code>。他们重要的成员都是 tokenizer、model、cache。只是 <code>KTransformersInterface</code> 里面的 model 是通过 <code>optimize_and_load_gguf()</code> 函数根据 YAML 配置优化过的模型。</p><p><code>TransformersInterface</code> 做一次推理的完整的调用链是</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">inference()</span></span>
<span class="line"><span style="color:#A6ACCD;">	-&gt; prefill()</span></span>
<span class="line"><span style="color:#A6ACCD;">	-&gt; generate()</span></span>
<span class="line"><span style="color:#A6ACCD;">		-&gt; decode_one_tokens()\`。</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span></code></pre></div><p><code>KTransformersInterface</code> 只重载了 <code>TransformersInterface</code> 的 <code>decode_one_tokens()</code> 这个方法，也就是只有 decode 阶段是优化过的，prefill 阶段是默认的。</p><h2 id="ktransformers-models" tabindex="-1">ktransformers/models/ <a class="header-anchor" href="#ktransformers-models" aria-hidden="true">#</a></h2><p>这个目录下面放的都是模型的具体实现。 我们测试的 deepseek 就放在了这里，我们支持了 decode 阶段现的任意保存恢复启停，但是 prefill 阶段做保存恢复就 segmentfault 了。</p><h2 id="ktransformers-operators" tabindex="-1">ktransformers/operators/ <a class="header-anchor" href="#ktransformers-operators" aria-hidden="true">#</a></h2><p>Ktransformer 的调优实现，即开头提到的注入到模型中的模块。</p><h2 id="ktransformers-util-cuda-graph-runner-py" tabindex="-1">ktransformers/util/cuda_graph_runner.py <a class="header-anchor" href="#ktransformers-util-cuda-graph-runner-py" aria-hidden="true">#</a></h2><p>一开始关注这个项目，就是听到前面的视频里面，章老师提到用到了他们用了 CUDA Graph，看了下这段用的是 torch.cuda 的接口，实现得很简洁清晰，刚好可以拿来做我们的测试用例。 直接去跑 Llama.cpp 我一直调用不到 CUDA Graph API = =</p><p>这里实现的话就是借助了 CUDA API 的自动捕获能力。因为图的输入输出 buffer 都是 capture 得到的，所以需要注意保证在执行 CUDA Graph 的时候还是固定地址，因此可以看到 <code>CUDAGraphRunner::forward</code> 需要把数据拷贝到之前已经创建好的固定的 buffer 里面。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">CUDAGraphRunner</span><span style="color:#89DDFF;">:</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#A6ACCD;font-style:italic;">cur_token</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#A6ACCD;font-style:italic;">position_ids</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#A6ACCD;font-style:italic;">cache_position</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">Tensor</span><span style="color:#89DDFF;">:</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#676E95;font-style:italic;"># Copy the input tensors to the input buffers.</span></span>
<span class="line"><span style="color:#A6ACCD;">        inputs_embeds </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">model</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">model</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">embed_tokens</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">cur_token</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">to</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cpu</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#A6ACCD;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">input_buffers</span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">inputs_embeds</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">].</span><span style="color:#82AAFF;">copy_</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">inputs_embeds</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">input_buffers</span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">position_ids</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">].</span><span style="color:#82AAFF;">copy_</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">position_ids</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">input_buffers</span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cache_position</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">].</span><span style="color:#82AAFF;">copy_</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">cache_position</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#676E95;font-style:italic;"># Run the graph.</span></span>
<span class="line"><span style="color:#A6ACCD;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">graph</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">replay</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">        torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">cuda</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">synchronize</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">main_device</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#676E95;font-style:italic;"># Return the output tensor.</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">output_buffers</span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">logits</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">]</span></span>
<span class="line"></span>
<span class="line"></span></code></pre></div><p><code>CUDAGraphRunner</code> 只用在了下面两个地方。</p><h3 id="ktransformers-server-backend-interfaces-ktransformers-py" tabindex="-1">ktransformers/server/backend/interfaces/ktransformers.py <a class="header-anchor" href="#ktransformers-server-backend-interfaces-ktransformers-py" aria-hidden="true">#</a></h3><p>上文提到的 <code>KTransformersInterface::decode_one_tokens()</code> 实现，重载了 <code>TransformersInterface</code> 对应方法。 cuda graph 相关的实现也比较简单。如果是首次调用该函数会初始化 <code>CUDAGraphRunner</code>，然后 <code>CUDAGraphRunner::capture</code> 启动图捕获，借助 model 进行推理得到 logits 转换成 token。 然后再次调用时发现 <code>CUDAGraphRunner</code> 已经构造好了，就直接把参数传给 <code>CUDAGraphRunner::forward</code>。</p><h3 id="ktransformers-util-utils-py" tabindex="-1">ktransformers/util/utils.py <a class="header-anchor" href="#ktransformers-util-utils-py" aria-hidden="true">#</a></h3><p><code>prefill_and_generate</code>，这个函数就是给 <code>ktransformers/local_chat.py</code> 用的，它是个用来调试的命令行工具。 值得注意的是整个项目目前支持的模型列表也是定义在 <code>ktransformers/local_chat.py</code> 这里的 = = 这个结构组织得有点乱，于是你可以看到在 KTransformersInterface 里面是 <code>from ktransformers.local_chat import custom_models, default_optimize_rules</code> 获取支持的模型列表和用于调优的 YAML 文件。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">custom_models </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">DeepseekV2ForCausalLM</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> DeepseekV2ForCausalLM</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">Qwen2MoeForCausalLM</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> Qwen2MoeForCausalLM</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">MixtralForCausalLM</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> MixtralForCausalLM</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">ktransformer_rules_dir </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> os</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">path</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">dirname</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">os</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">path</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">abspath</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">__file__</span><span style="color:#89DDFF;">))</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">/optimize/optimize_rules/</span><span style="color:#89DDFF;">&quot;</span></span>
<span class="line"><span style="color:#A6ACCD;">default_optimize_rules </span><span style="color:#89DDFF;">={</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">DeepseekV2ForCausalLM</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> ktransformer_rules_dir </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">DeepSeek-V2-Chat.yaml</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">Qwen2MoeForCausalLM</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> ktransformer_rules_dir </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">Qwen2-57B-A14B-Instruct.yaml</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">MixtralForCausalLM</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> ktransformer_rules_dir </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">Mixtral.yaml</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div>`,29),l=[o];function p(t,c,i,D,y,F){return e(),a("div",null,l)}const u=s(r,[["render",p]]);export{d as __pageData,u as default};
