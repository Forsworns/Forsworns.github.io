import{_ as a,c as s,o as n,e}from"./app.0f5a0ae1.js";const D=JSON.parse('{"title":"torch 源码阅读","description":"Pytorch2 recap","frontmatter":{"title":"torch 源码阅读","description":"Pytorch2 recap","tags":["LLM","AI","GPU","CUDA"]},"headers":[{"level":2,"title":"pytorch2 中打开日志","slug":"pytorch2-中打开日志","link":"#pytorch2-中打开日志","children":[]},{"level":2,"title":"nvrtc","slug":"nvrtc","link":"#nvrtc","children":[]},{"level":2,"title":"torch.compile 过程中的 nvrtc 调用","slug":"torch-compile-过程中的-nvrtc-调用","link":"#torch-compile-过程中的-nvrtc-调用","children":[]},{"level":2,"title":"torch.compile 其他 API","slug":"torch-compile-其他-api","link":"#torch-compile-其他-api","children":[{"level":3,"title":"前端 dynamo","slug":"前端-dynamo","link":"#前端-dynamo","children":[]},{"level":3,"title":"后端 inductor","slug":"后端-inductor","link":"#后端-inductor","children":[]},{"level":3,"title":"其他","slug":"其他","link":"#其他","children":[]}]},{"level":2,"title":"cuda graph 代码","slug":"cuda-graph-代码","link":"#cuda-graph-代码","children":[{"level":3,"title":"前端 dynamo","slug":"前端-dynamo-1","link":"#前端-dynamo-1","children":[]},{"level":3,"title":"在后端 inductor","slug":"在后端-inductor","link":"#在后端-inductor","children":[]}]},{"level":2,"title":"torch 日志打印","slug":"torch-日志打印","link":"#torch-日志打印","children":[]},{"level":2,"title":"c10/cuda/CUDACachingAllocator.cpp","slug":"c10-cuda-cudacachingallocator-cpp","link":"#c10-cuda-cudacachingallocator-cpp","children":[]},{"level":2,"title":"torch/nn/parallel/distributed.py","slug":"torch-nn-parallel-distributed-py","link":"#torch-nn-parallel-distributed-py","children":[]},{"level":2,"title":"torch/utils/data/distributed.py","slug":"torch-utils-data-distributed-py","link":"#torch-utils-data-distributed-py","children":[]},{"level":2,"title":"torch/distributed/","slug":"torch-distributed","link":"#torch-distributed","children":[{"level":3,"title":"tensor/parallel/ddp.py","slug":"tensor-parallel-ddp-py","link":"#tensor-parallel-ddp-py","children":[]},{"level":3,"title":"nn","slug":"nn","link":"#nn","children":[]},{"level":3,"title":"algorithms","slug":"algorithms","link":"#algorithms","children":[]}]},{"level":2,"title":"torch/nn/parallel/data_parallel.py","slug":"torch-nn-parallel-data-parallel-py","link":"#torch-nn-parallel-data-parallel-py","children":[]},{"level":2,"title":"torch.randn() 的实现","slug":"torch-randn-的实现","link":"#torch-randn-的实现","children":[]},{"level":2,"title":"/aten/src/ATen/native/TensorFactories.cpp","slug":"aten-src-aten-native-tensorfactories-cpp","link":"#aten-src-aten-native-tensorfactories-cpp","children":[]},{"level":2,"title":"/aten/src/ATen/native/Convolution.cpp","slug":"aten-src-aten-native-convolution-cpp","link":"#aten-src-aten-native-convolution-cpp","children":[]},{"level":2,"title":"/aten/src/ATen/native/cuda/CUDALoops.cuh","slug":"aten-src-aten-native-cuda-cudaloops-cuh","link":"#aten-src-aten-native-cuda-cudaloops-cuh","children":[]},{"level":2,"title":"/aten/src/ATen/native/native_functions.yaml","slug":"aten-src-aten-native-native-functions-yaml","link":"#aten-src-aten-native-native-functions-yaml","children":[]}],"relativePath":"zh/blogs/20241008/index.md"}'),o={name:"zh/blogs/20241008/index.md"},l=e(`<nav class="table-of-contents"><ul><li><a href="#pytorch2-中打开日志">pytorch2 中打开日志</a></li><li><a href="#nvrtc">nvrtc</a></li><li><a href="#torch-compile-过程中的-nvrtc-调用">torch.compile 过程中的 nvrtc 调用</a></li><li><a href="#torch-compile-其他-api">torch.compile 其他 API</a><ul><li><a href="#前端-dynamo">前端 dynamo</a></li><li><a href="#后端-inductor">后端 inductor</a></li><li><a href="#其他">其他</a></li></ul></li><li><a href="#cuda-graph-代码">cuda graph 代码</a><ul><li><a href="#前端-dynamo-1">前端 dynamo</a></li><li><a href="#在后端-inductor">在后端 inductor</a></li></ul></li><li><a href="#torch-日志打印">torch 日志打印</a></li><li><a href="#c10-cuda-cudacachingallocator-cpp">c10/cuda/CUDACachingAllocator.cpp</a></li><li><a href="#torch-nn-parallel-distributed-py">torch/nn/parallel/distributed.py</a></li><li><a href="#torch-utils-data-distributed-py">torch/utils/data/distributed.py</a></li><li><a href="#torch-distributed">torch/distributed/</a><ul><li><a href="#tensor-parallel-ddp-py">tensor/parallel/ddp.py</a></li><li><a href="#nn">nn</a></li><li><a href="#algorithms">algorithms</a></li></ul></li><li><a href="#torch-nn-parallel-data-parallel-py">torch/nn/parallel/data_parallel.py</a></li><li><a href="#torch-randn-的实现">torch.randn() 的实现</a></li><li><a href="#aten-src-aten-native-tensorfactories-cpp">/aten/src/ATen/native/TensorFactories.cpp</a></li><li><a href="#aten-src-aten-native-convolution-cpp">/aten/src/ATen/native/Convolution.cpp</a></li><li><a href="#aten-src-aten-native-cuda-cudaloops-cuh">/aten/src/ATen/native/cuda/CUDALoops.cuh</a></li><li><a href="#aten-src-aten-native-native-functions-yaml">/aten/src/ATen/native/native_functions.yaml</a></li></ul></nav><h2 id="pytorch2-中打开日志" tabindex="-1">pytorch2 中打开日志 <a class="header-anchor" href="#pytorch2-中打开日志" aria-hidden="true">#</a></h2><p>import torch import logging torch._logging.set_logs(all=logging.DEBUG)</p><h2 id="nvrtc" tabindex="-1">nvrtc <a class="header-anchor" href="#nvrtc" aria-hidden="true">#</a></h2><p>torch.compile 时大量的子进程占用 GPU 设备。已知单纯调用 libnvrtc 和 libnvJitLink 不会引用 GPU 设备。</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">import torch</span></span>
<span class="line"><span style="color:#A6ACCD;">import logging</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;">torch._logging.set_logs(all=logging.DEBUG)</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;"># “reduce-overhead” 模式才会用到 cuda graph</span></span>
<span class="line"><span style="color:#A6ACCD;">@torch.compile(mode=&quot;reduce-overhead&quot;)</span></span>
<span class="line"><span style="color:#A6ACCD;">def my_model(x):</span></span>
<span class="line"><span style="color:#A6ACCD;">    y = torch.matmul(x, x).cuda()</span></span>
<span class="line"><span style="color:#A6ACCD;">    # side effect breaks graph construction</span></span>
<span class="line"><span style="color:#A6ACCD;">    input(&quot;during capture&quot;)</span></span>
<span class="line"><span style="color:#A6ACCD;">    y = torch.matmul(y, x).cuda()</span></span>
<span class="line"><span style="color:#A6ACCD;">    return y</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;">x = torch.randn(10, 10).cuda()</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;">print(&quot;graph exec 1&quot;, flush=True)</span></span>
<span class="line"><span style="color:#A6ACCD;">y = my_model(x)</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;">print(&quot;graph exec 2&quot;, flush=True)</span></span>
<span class="line"><span style="color:#A6ACCD;">y = my_model(x)</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;">print(&quot;y&quot;, y, flush=True)</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span></code></pre></div><p>Pytorch 中通过宏 AT_CUDA_NVRTC_CHECK 调用 nvrtc 的代码处理错误。<a href="http://libnvrtc.so" target="_blank" rel="noreferrer">libnvrtc.so</a> 库中的符号类似 <a href="http://libcuda.so" target="_blank" rel="noreferrer">libcuda.so</a> 等是通过下面的函数动态获取的</p><div class="language-c++"><button title="Copy Code" class="copy"></button><span class="lang">c++</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">const at::cuda::NVRTC&amp; nvrtc() {</span></span>
<span class="line"><span style="color:#A6ACCD;">  return at::globalContext().getNVRTC();</span></span>
<span class="line"><span style="color:#A6ACCD;">}</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span></code></pre></div><p>CUDAHook 和 nvrtc 的具体的实现是在 <code>aten/src/ATen/cuda/detail/CUDAHooks.cpp</code> 和 <code>aten/src/ATen/cuda/detail/LazyNVRTC.cpp</code>。在 <code>aten/src/ATen/cuda/detail/LazyNVRTC.cpp</code> 中，<code>lazyNVRTC</code> 这个全局变量上除了 nvrtc 的 API，还会通过 <code>dlsym</code> 懒加载 <a href="http://libcuda.so" target="_blank" rel="noreferrer">libcuda.so</a> 中的部分 API。 可能就是这里 dlopen 的 <a href="http://libcuda.so" target="_blank" rel="noreferrer">libcuda.so</a> 仍占用设备。但是这里也说不通，cuInit 和 CUcontext 相关的函数不可能不先调用。</p><p>除了下面 torch.compile 相关的会调用到 nvrtc 编译 cuda 算子，aten 库中例如 <code>aten/src/ATen/native/cuda/jit_utils.cpp</code> 的 <code>jit_pwise_function</code> 也调用了 nvrtc，但是是用来做一些循环展开等算子优化的。</p><h2 id="torch-compile-过程中的-nvrtc-调用" tabindex="-1">torch.compile 过程中的 nvrtc 调用 <a class="header-anchor" href="#torch-compile-过程中的-nvrtc-调用" aria-hidden="true">#</a></h2><p>被用于代码生成</p><p>torch/_dynamo/trace_rules.py 中定义了一系列指导前端的 trace rule，其中的 “torch._C._te.construct_codegen” 是定义在 torch/csrc/jit/tensorexpr/tensorexpr_init.cpp 中的 <code>construct_codegen -&gt; CudaCodeGen</code>，<code>CudaCodeGen::Initialize -&gt; CudaCodeGen::CompileToNVRTC</code> 可以调用到 nvrtc 进行即时编译。</p><p>将 CudaAnalysis 分析出的代树，通过 CudaPrinter、GPUMetaVarRewriter 辅助结构体交给其他辅助函数做重写，结果写入到一个 ostringstream，交给 nvrtc 编译。</p><p>被用于算子融合</p><p>prim::FusionGroup （torch/csrc/jit/runtime/register_prim_ops_fulljit.cpp） -&gt; runFusion -&gt; launchFusion（torch/csrc/jit/codegen/fuser/executor.cpp） -&gt; launch_raw compileKernel (torch/csrc/jit/codegen/fuser/compiler.cpp) 调用了 getConstructor 拿到 registerFusionBackend 中注册到全局的 <code>createFusionKernel</code>（torch/csrc/jit/codegen/fuser/cuda/fused_kernel.cpp 或 <code>torch/csrc/jit/codegen/fuser/cpu/fused_kernel.cpp</code>） 。</p><h2 id="torch-compile-其他-api" tabindex="-1">torch.compile 其他 API <a class="header-anchor" href="#torch-compile-其他-api" aria-hidden="true">#</a></h2><h3 id="前端-dynamo" tabindex="-1">前端 dynamo <a class="header-anchor" href="#前端-dynamo" aria-hidden="true">#</a></h3><p>torch/_dynamo/eval_frame.py 中的 <code>_optimize()</code> 前端 TorchDynamo 的入口函数。</p><p>torch/_dynamo/convert_frame.py 定义了 <code>ConvertFrame</code>、 <code>ConvertFrameAssert</code> 等仿函数类，最终调用同文件内的 <code>_compile</code> 函数，将栈帧转换成 FX graph。</p><h3 id="后端-inductor" tabindex="-1">后端 inductor <a class="header-anchor" href="#后端-inductor" aria-hidden="true">#</a></h3><p>torch/_inductor/compile_fx.py 中的 <code>compile_fx</code>，在 torch/_dynamo/backends/inductor.py 里面通过 <code>register_backend</code> 注册到全局的后端 fx graph 入口函数。</p><p>torch/_inductor/async_compile.py 异步编译。维护了一个进程池，例如 <code>triton</code> 后端就通过下面这个函数，向进程池提交了一个编译任务然后返回了一个 future。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">triton</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">kernel_name</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">str</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">source_code</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">str</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">device_str</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">str</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">        kernel </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> TritonCodeCache</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">load</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">kernel_name</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> source_code</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">TritonFuture</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            kernel</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">process_pool</span><span style="color:#89DDFF;">().</span><span style="color:#82AAFF;">submit</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">                _worker_compile_triton</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">                kernel</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">_reload_in_subproc</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">                extra_env</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><p>所以能看出，多出来的那些进程，是通过这里生成的。可以通过下面的 <code>TORCHINDUCTOR_COMPILE_THREADS</code> 环境变量修改。 然后进程池是使用的 concurrent.futures.ProcessPoolExecutor，<strong>基于 fork，所以可能 cuda 没有用到，但是 /dev/nvidia0 等 fd 也被占用了</strong>。</p><p>torch/_inductor/codecache.py 代码编译缓存，例如 CUDACodeCache 为 cuda 代码的编译缓存。如果需要，调用 <code>cuda_compile_command</code> 函数进行编译。</p><h3 id="其他" tabindex="-1">其他 <a class="header-anchor" href="#其他" aria-hidden="true">#</a></h3><p>torch/_inductor/config.py 中，通过 <code>decide_compile_threads</code> 获取了 cpu 核心数。通过环境变量 <code>TORCHINDUCTOR_COMPILE_THREADS</code> 可以修改。</p><h2 id="cuda-graph-代码" tabindex="-1">cuda graph 代码 <a class="header-anchor" href="#cuda-graph-代码" aria-hidden="true">#</a></h2><h3 id="前端-dynamo-1" tabindex="-1">前端 dynamo <a class="header-anchor" href="#前端-dynamo-1" aria-hidden="true">#</a></h3><p>torch/_dynamo/backends/cudagraphs.py 中的 <code>CudagraphsBackend</code>，通过 <code>register_backend</code> 注册到全局的 cuda graph 后端入口。</p><h3 id="在后端-inductor" tabindex="-1">在后端 inductor <a class="header-anchor" href="#在后端-inductor" aria-hidden="true">#</a></h3><p>torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py</p><h2 id="torch-日志打印" tabindex="-1">torch 日志打印 <a class="header-anchor" href="#torch-日志打印" aria-hidden="true">#</a></h2><p>pytorch2 中， 环境变量 TORCH_LOGS=&quot;+inductor,+dynamo&quot; 或者也可以通过 API 直接设置 torch._logging.set_logs(all=logging.DEBUG)</p><h2 id="c10-cuda-cudacachingallocator-cpp" tabindex="-1">c10/cuda/CUDACachingAllocator.cpp <a class="header-anchor" href="#c10-cuda-cudacachingallocator-cpp" aria-hidden="true">#</a></h2><p>torch 的两个显存分配器实现，PYTORCH_NO_CUDA_MEMORY_CACHING 控制版本。有一个torch 版本的新分配器依赖 NVML，平台不支持 NVML 的时候可以先用这个环境变量关了新分配器。</p><h2 id="torch-nn-parallel-distributed-py" tabindex="-1">torch/nn/parallel/distributed.py <a class="header-anchor" href="#torch-nn-parallel-distributed-py" aria-hidden="true">#</a></h2><p>完全基于集合通信库的 DP 实现 <code>DistributedDataParallel</code>（DDP），各个 rank 独自持有模型。</p><p>首先看 <code>DistributedDataParallel</code> 的构造函数。<code>self._module_parameters</code> 中存了所有未被参数过滤掉的模型参数。</p><p>初始化阶段 <code>self._verify_param_shape_across_processes()</code>、<code>self._sync_module_states()</code> 用来在不同 rank 间检查、同步 module 的初始状态，这一步会用 broadcast 集合通信，buffer 大小在代码里写死了是 250 MB。最关键的是 <code>self._ddp_init_helper()</code> 去初始化 reducer，这一步完了基本初始化就结束了，剩下的就是一些混合精度 AMP 相关的代码等，该函数的注释如下，解释得很详细了</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">DDP init helper function to manage parameters, grad hooks, logging, and SyncBatchNorm.</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">Initialization helper function that does the following:</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">(1) bucketing the parameters for reductions</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">(2) resetting the bucketing states</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">(3) registering the grad hooks</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">(4) Logging construction-time DDP logging data</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">(5) passing a handle of DDP to SyncBatchNorm Layer</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span></code></pre></div><p>DDP 通过将参数划分到不同的桶里面，来实现梯度 reduction 和 反向传播的 overlap 优化，掩盖 reduction 耗时，桶的默认大小是 25MB，可以通过 <code>bucket_cap_mb</code> 参数控制，桶的初始化也在 <code>self._ddp_init_helper()</code> ，调用到的方法是 <code>torch/csrc/distributed/c10d/reducer.cpp</code> 里面定义的 <code>compute_bucket_assignment_by_size</code>，获取每个桶的 idx 和 size。划分好桶之后，再通过 <code>torch/csrc/distributed/c10d/reducer.cpp</code> 中实现的 <code>Reducer</code> 类型的构造函数，完成初始化。</p><p><code>compute_bucket_assignment_by_size()</code>的逻辑是：先构造一个桶的哈希表，每个桶内可能有多个张量，哈希表的键是通过张量的数据类型和它所在的设备哈希出来的，张量数据的大小计算方式就是张量规模乘上它的数据类型的大小。当一个键对应的桶被塞满，就要将当前的桶添加到返回列表里面，然后为相应键重建一个桶。最后再把剩余的没满的桶填充到返回列表里面。这个函数的实现上有一个技巧，就是希望尽可能让返回的列表中桶的顺序按模型中张量出现的顺序排列。在没有 torch 上层代码提供提示的情况下，这个函数里面对返回列表中的每个桶里面最小的张量序号进行了排序，假设序号小的张量是优先出现在模型中的参数。回到<code>self._ddp_init_helper()</code>中，它又将 <code>compute_bucket_assignment_by_size()</code> 返回的列表翻转了一下，希望优先处理先被反向传播过程处理到的张量。</p><p>前面提到的 <code>torch/csrc/distributed/c10d/reducer.cpp</code> 中的 <code>Reducer</code>，进行 all reduce 通信的方法实际上是 <code>Reducer::run_comm_hook()</code>。它的调用链路是 <code>Reducer::autograd_hook()-&gt; Reducer::mark_variable_ready() -&gt; Reducer::mark_bucket_ready() -&gt; Reducer::all_reduce_bucket() -&gt; Reducer::run_comm_hook()</code>。<code>Reducer::autograd_hook()</code> 被注册给了 <code>torch::autograd::impl::grad_accumulator::add_post_hook()</code>，会在梯度计算完毕累加到了梯度张量后执行，而且只会在 pytorch 的 autograd 线程上执行。</p><blockquote><p>torch 的 autograd 线程在 torch/csrc/autograd/engine.cpp 的 <code>Engine::start_device_threads()-&gt;Engine::thread_init()</code> 创建。</p></blockquote><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#FFCB6B;">c10</span><span style="color:#89DDFF;">::</span><span style="color:#FFCB6B;">intrusive_ptr</span><span style="color:#89DDFF;">&lt;</span><span style="color:#FFCB6B;">c10</span><span style="color:#89DDFF;">::</span><span style="color:#FFCB6B;">ivalue</span><span style="color:#89DDFF;">::</span><span style="color:#FFCB6B;">Future</span><span style="color:#89DDFF;">&gt;</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">Reducer</span><span style="color:#89DDFF;">::</span><span style="color:#82AAFF;">run_comm_hook</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#FFCB6B;">GradBucket</span><span style="color:#C792EA;">&amp;</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">grad_bucket</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">  </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">comm_hook_ </span><span style="color:#89DDFF;">==</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">nullptr)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // \`Reducer\` 构造时的参数没有配置过的话，会从 \`Reducer::process_group_\` 构造一个 \`_AllReduceBySumCommHook\`，</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // 然后对每个 bucket 做 all reduce。</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">run_allreduce_hook</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">grad_bucket</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">  </span><span style="color:#89DDFF;">}</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;font-style:italic;">else</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">comm_hook_</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#82AAFF;">runHook</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">grad_bucket</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">  </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>初始化函数里面还有一段有趣的代码是通过 <code>torch._dynamo.config._get_optimize_ddp_mode()</code> 获取了 torch 2 中编译器前端 torchdynamo 的设置，选择启用实验性的 python reducer，而不是用默认的 cpp 实现的 reducer。去读 <code>torch/_dynamo/config.py</code> 的话就会知道，<code>DistributedDataParallel</code> 默认会对 DDP 的通信和计算做 overlap 以掩盖通信开销。但是由于 torch 2 依赖于 PEP 523，所以纯 python 实现的 reducer 更有利于 torch 做自动分析和优化。比如通常路径上 all_reduce 操作调用的是 <code>torch.distributed.distributed_c10d.all_reduce()</code>，但是 python 版的是 <code>torch.distributed._functional_collectives.all_reduce()</code> 中的实现。通过 <code>DistributedDataParallel._get_active_ddp_module()</code> 类方法可以把 DDP 对象暴露给 torchdynamo，<code>DistributedDataParallel._inside_ddp_forward()</code> 这个 contextmanager 则是在调用 <code>DistributedDataParallel._run_ddp_forward()</code> 前就禁用了 torchdynamo。</p><p>对于 <code>torch.distributed.distributed_c10d.all_reduce()</code>，它默认调用的是 <code>_get_default_group()</code> 获取的 <code>ProcessGroup</code> 上的 <code>all_reduce()</code> 接口，这实现在 <code>torch/csrc/distributed/c10d/ProcessGroupWrapper.cpp</code> 中的公共抽象 <code>ProcessGroupWrapper</code>，它继承于 <code>Backend</code> 类型。torch 支持了多种集合通信库，每个实现也都继承了 <code>Backend</code>，如 NCCL、GLOO、UCC 等。一般我们只关心 NCCL，它实现在 <code>torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp</code>。</p><p>通过 <code>PYTORCH_DDP_USE_SIDE_STREAM</code> 环境变量可以新开一个 cuda steram 做 H2D 的拷贝。</p><h2 id="torch-utils-data-distributed-py" tabindex="-1">torch/utils/data/distributed.py <a class="header-anchor" href="#torch-utils-data-distributed-py" aria-hidden="true">#</a></h2><p><code>DistributedSampler</code> 配合 <code>DistributedDataParallel</code> 使用，对输入进行分片。 实现很简单，就是在 <code>self.__iter__</code> 函数中 <code>indices[self.rank : self.total_size : self.num_replicas]</code>，对整个 <code>self.total_size</code> 长的数据，间隔 <code>self.num_replicas</code> 按 <code>self.rank</code> 选一个。<code>self.rank</code> 可以通过 <code>dist.get_rank</code> 获取。</p><h2 id="torch-distributed" tabindex="-1">torch/distributed/ <a class="header-anchor" href="#torch-distributed" aria-hidden="true">#</a></h2><p><code>DTensor</code> 的各种 TP 方式的实现，和与其他并行方式的集成。DTensor 本身实现在 <code>torch/distributed/_tensor/api.py</code>。</p><h3 id="tensor-parallel-ddp-py" tabindex="-1">tensor/parallel/ddp.py <a class="header-anchor" href="#tensor-parallel-ddp-py" aria-hidden="true">#</a></h3><p><code>DistributedDataParallel</code> 中调用的 <code>_pre_dp_module_transform()</code> 的实现，便于 DDP 和 TP 结合（torch 的 TP 依赖于 DTensor）。它注册了两个更新 DTensor 的钩子，一个用于在前向传播之前将本地张量转换回 DTensor，另一个用于在前向传播之后将 DTensor 转换回张量。避免 DDP 对 DTensor 参数的特殊处理，并使 DTensor 的梯度能够传递回 DDP 的梯度桶。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">_pre_dp_module_transform</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">module</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">_localize_dtensor</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">module</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">None,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">None)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># Recontruct DTensor parameters from local tensors</span></span>
<span class="line"><span style="color:#A6ACCD;">    module</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">register_forward_pre_hook</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">_reconstruct_dtensor</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># Convert DTensor parameters to local tensors</span></span>
<span class="line"><span style="color:#A6ACCD;">    module</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">register_forward_hook</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">_localize_dtensor</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><h3 id="nn" tabindex="-1">nn <a class="header-anchor" href="#nn" aria-hidden="true">#</a></h3><p>定义了 torch 用户可以主动使用的集合通信接口 <code>torch.distributed.nn.functional</code>，主动创建位于远端进程的 module <code>torch.distributed.nn.RemoteModule</code>。</p><h3 id="algorithms" tabindex="-1">algorithms <a class="header-anchor" href="#algorithms" aria-hidden="true">#</a></h3><p>一些分布式下的算法实现。</p><p>如 <code>torch/distributed/algorithms/model_averaging/averagers.py</code> 定义了用户可以直接调用的对各个 rank 的参数做均值的 <code>PeriodicModelAverager</code>，可以用于主动同步模型参数、和 PostLocalSGDOptimizer 结合用于优化器等。</p><p><code>torch/distributed/optim/</code> 实现分布式的优化器，例如 <code>PostLocalSGDOptimizer</code>、<code>ZeroRedundancyOptimizer</code>。</p><p><code>torch/distributed/algorithms/_comm_hooks/default_hooks.py</code> 是分布式训练中默认的 hook，可以通过 <code>model.register_comm_hook</code> 注册别的 hook。如 <code>torch/distributed/algorithms/ddp_comm_hooks/</code> 下的 <code>allreduce_hook()</code>、<code>post_localSGD_hook()</code>。注册 hook 的函数是 <code>DistributedDataParallel.register_comm_hook()</code></p><h2 id="torch-nn-parallel-data-parallel-py" tabindex="-1">torch/nn/parallel/data_parallel.py <a class="header-anchor" href="#torch-nn-parallel-data-parallel-py" aria-hidden="true">#</a></h2><p>实现了 <code>DataParallel</code>，仅持有一份模型，每次前向更新都会在不同设备间拷贝需要并行的参数，一般已不使用。</p><p><code>torch.nn.parallel.replicate()</code> 用于在各个设备上复制模型，它主要调用了 <code>_broadcast_coalesced_reshape() -&gt; comm._broadcast_coalesced()</code>。<code>broadcast_coalesced()</code> 是个 C 函数，实现在 <code>torch/csrc/cuda/comm.cpp</code>。具体实现在 <code>_broadcast_out_impl</code>，这里通过一个宏控制了是否使用 NCCL，否则就是直接 CUDA D2D 拷贝。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">static inline std</span><span style="color:#89DDFF;">::</span><span style="color:#A6ACCD;">vector</span><span style="color:#89DDFF;">&lt;</span><span style="color:#A6ACCD;">Tensor</span><span style="color:#89DDFF;">&gt;&amp;</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">_broadcast_out_impl</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">    const Tensor</span><span style="color:#89DDFF;">&amp;</span><span style="color:#82AAFF;"> tensor</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    std</span><span style="color:#89DDFF;">::</span><span style="color:#82AAFF;">vector</span><span style="color:#89DDFF;">&lt;</span><span style="color:#82AAFF;">Tensor</span><span style="color:#89DDFF;">&gt;&amp;</span><span style="color:#82AAFF;"> out_tensors</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">#ifdef USE_NCCL</span></span>
<span class="line"><span style="color:#A6ACCD;">  std</span><span style="color:#89DDFF;">::</span><span style="color:#A6ACCD;">vector</span><span style="color:#89DDFF;">&lt;</span><span style="color:#A6ACCD;">Tensor</span><span style="color:#89DDFF;">&gt;</span><span style="color:#A6ACCD;"> nccl_list;</span></span>
<span class="line"><span style="color:#A6ACCD;">  nccl_list</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">reserve</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">out_tensors</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;">;</span></span>
<span class="line"><span style="color:#A6ACCD;">  nccl_list</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">emplace_back</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">tensor</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;">;</span></span>
<span class="line"><span style="color:#A6ACCD;">  </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">auto</span><span style="color:#89DDFF;">&amp;</span><span style="color:#A6ACCD;"> out_tensor </span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> out_tensors</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    nccl_list</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">emplace_back</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">out_tensor</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;">;</span></span>
<span class="line"><span style="color:#A6ACCD;">  </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#A6ACCD;">  </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">nccl</span><span style="color:#89DDFF;">::</span><span style="color:#82AAFF;">is_available</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">nccl_list</span><span style="color:#89DDFF;">))</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    nccl</span><span style="color:#89DDFF;">::</span><span style="color:#82AAFF;">broadcast</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">nccl_list</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;">;</span></span>
<span class="line"><span style="color:#A6ACCD;">  </span><span style="color:#89DDFF;">}</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;font-style:italic;">else</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">#else</span></span>
<span class="line"><span style="color:#A6ACCD;">  </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">#endif</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">auto</span><span style="color:#89DDFF;">&amp;</span><span style="color:#A6ACCD;"> out_tensor </span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> out_tensors</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">      out_tensor</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">copy_</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">tensor</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">/*</span><span style="color:#A6ACCD;font-style:italic;">non_blocking</span><span style="color:#89DDFF;">=*/</span><span style="color:#82AAFF;">true</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;">;</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#A6ACCD;">  }</span></span>
<span class="line"><span style="color:#A6ACCD;">  </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> out_tensors;</span></span>
<span class="line"><span style="color:#A6ACCD;">}</span></span>
<span class="line"></span></code></pre></div><p><code>torch.nn.parallel.parallel_apply()</code> 用于在不同设备上并行计算，走得就是在不同线程、不同 <code>torch.cuda.device()、torch.cuda.stream()</code> 下调用 module 的方式。</p><h2 id="torch-randn-的实现" tabindex="-1">torch.randn() 的实现 <a class="header-anchor" href="#torch-randn-的实现" aria-hidden="true">#</a></h2><p>randn的具体实现方式 /aten/src/ATen/native/TensorFactories.cpp: Tensor rand() -&gt; /aten/src/ATen/native/Distributions.cpp: Tensor&amp; uniform_() -&gt; /aten/src/ATen/native/DistributionTemplates.h: at::Tensor&amp; uniform_impl_() -&gt; /aten/src/ATen/native/cuda/DistributionUniform.cu: void uniform_kernel() -&gt; /aten/src/ATen/native/cuda/DistributionTemplates.h: void uniform_kernel(), void uniform_and_transform()，void distribution_nullary_kernel() void uniform_and_transform() 里面根据数据类型，通过 distribution_nullary_kernel()加载了一个遍历 Tensor 的核函数，逐项调用 curand API curand_uniform4 或 curand_uniform2_double进行填充。 动态获取 cuda API 符号 <a href="https://github.com/pytorch/pytorch/blob/main/c10/cuda/driver_api.cpp#L40" target="_blank" rel="noreferrer">https://github.com/pytorch/pytorch/blob/main/c10/cuda/driver_api.cpp#L40</a> 在单个单例中，搜索 cuda driver API 和 nvml API</p><h2 id="aten-src-aten-native-tensorfactories-cpp" tabindex="-1">/aten/src/ATen/native/TensorFactories.cpp <a class="header-anchor" href="#aten-src-aten-native-tensorfactories-cpp" aria-hidden="true">#</a></h2><p>张量的工厂类</p><h2 id="aten-src-aten-native-convolution-cpp" tabindex="-1">/aten/src/ATen/native/Convolution.cpp <a class="header-anchor" href="#aten-src-aten-native-convolution-cpp" aria-hidden="true">#</a></h2><p>卷积实现， 例如正向推理的 cudnn 实现 cudnn_convolution -&gt; cudnn_convolution_forward -&gt; raw_cudnn_convolution_forward_out -&gt; raw_cudnn_convolution_forward_out_32bit -&gt; cudnnConvolutionForward <a href="https://github1s.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/cudnn/ConvShared.cpp#L180" target="_blank" rel="noreferrer">https://github1s.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/cudnn/ConvShared.cpp#L180</a><a href="https://github1s.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/cudnn/Conv_v7.cpp#L629" target="_blank" rel="noreferrer">https://github1s.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/cudnn/Conv_v7.cpp#L629</a> 例如反向传播的实现 <a href="https://github1s.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/Convolution.cpp#L1974-L1978" target="_blank" rel="noreferrer">https://github1s.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/Convolution.cpp#L1974-L1978</a> 这里去根据后端选择对应实现，例如 cudnn_convolution_backward_stub，实现在 /aten/src/ATen/native/cudnn/ConvShared.cpp。</p><h2 id="aten-src-aten-native-cuda-cudaloops-cuh" tabindex="-1">/aten/src/ATen/native/cuda/CUDALoops.cuh <a class="header-anchor" href="#aten-src-aten-native-cuda-cudaloops-cuh" aria-hidden="true">#</a></h2><p>借助 cuda 遍历 Tensor 中的每个元素，上述工厂类中会通过 Tensor::fill_()方法调用到它，对张量进行赋值。</p><h2 id="aten-src-aten-native-native-functions-yaml" tabindex="-1">/aten/src/ATen/native/native_functions.yaml <a class="header-anchor" href="#aten-src-aten-native-native-functions-yaml" aria-hidden="true">#</a></h2><p>每个 native 算子有多个后端的 native 实现，该文件描述了这些变体。 例如 fft 变换，/aten/src/ATen/native/SpectralOps.cpp 中的 Tensor stft()调用了对应的 native 算子 _fft_r2c，这又对应了两类后端实现，_fft_r2c_cufft 和 _fft_r2c_mkl</p><ul><li>func: _fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -&gt; Tensor variants: function dispatch: CPU: _fft_r2c_mkl CUDA: _fft_r2c_cufft</li></ul>`,80),t=[l];function c(r,p,i,d,y,u){return n(),s("div",null,t)}const F=a(o,[["render",c]]);export{D as __pageData,F as default};
