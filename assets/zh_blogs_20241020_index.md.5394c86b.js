import{_ as s,c as e,o as a,e as n}from"./app.db17c96a.js";const h=JSON.parse('{"title":"HAMI 源码阅读","description":"第四范式开源的通用 GPU 虚拟化组件","frontmatter":{"title":"HAMI 源码阅读","description":"第四范式开源的通用 GPU 虚拟化组件","tags":["GPU","CUDA","虚拟化","k8s"]},"headers":[{"level":2,"title":"src/libvgpu.c","slug":"src-libvgpu-c","link":"#src-libvgpu-c","children":[]},{"level":2,"title":"src/nvml","slug":"src-nvml","link":"#src-nvml","children":[{"level":3,"title":"hook.c","slug":"hook-c","link":"#hook-c","children":[]},{"level":3,"title":"nvml_entry.c","slug":"nvml-entry-c","link":"#nvml-entry-c","children":[]}]},{"level":2,"title":"src/multiprocess","slug":"src-multiprocess","link":"#src-multiprocess","children":[{"level":3,"title":"shrreg_tool.c","slug":"shrreg-tool-c","link":"#shrreg-tool-c","children":[]},{"level":3,"title":"multiprocess_memory_limit.c","slug":"multiprocess-memory-limit-c","link":"#multiprocess-memory-limit-c","children":[]},{"level":3,"title":"multiprocess_utilization_watcher.c","slug":"multiprocess-utilization-watcher-c","link":"#multiprocess-utilization-watcher-c","children":[]}]},{"level":2,"title":"src/allocator/allocator.c","slug":"src-allocator-allocator-c","link":"#src-allocator-allocator-c","children":[]},{"level":2,"title":"src/cuda/","slug":"src-cuda","link":"#src-cuda","children":[{"level":3,"title":"memory.c","slug":"memory-c","link":"#memory-c","children":[]}]},{"level":2,"title":"src/utils.c","slug":"src-utils-c","link":"#src-utils-c","children":[]},{"level":2,"title":"include","slug":"include","link":"#include","children":[{"level":3,"title":"libnvml_hook.h","slug":"libnvml-hook-h","link":"#libnvml-hook-h","children":[]},{"level":3,"title":"libcuda_hook.h","slug":"libcuda-hook-h","link":"#libcuda-hook-h","children":[]}]},{"level":2,"title":"cmd/vGPUmonitor/","slug":"cmd-vgpumonitor","link":"#cmd-vgpumonitor","children":[{"level":3,"title":"metrics.go","slug":"metrics-go","link":"#metrics-go","children":[]},{"level":3,"title":"validation.go","slug":"validation-go","link":"#validation-go","children":[]},{"level":3,"title":"feedback.go","slug":"feedback-go","link":"#feedback-go","children":[]},{"level":3,"title":"noderpc/noderpc.proto","slug":"noderpc-noderpc-proto","link":"#noderpc-noderpc-proto","children":[]}]},{"level":2,"title":"pkg/monitor/","slug":"pkg-monitor","link":"#pkg-monitor","children":[{"level":3,"title":"nvidia/cudevshr.go","slug":"nvidia-cudevshr-go","link":"#nvidia-cudevshr-go","children":[]}]},{"level":2,"title":"cmd/scheduler/","slug":"cmd-scheduler","link":"#cmd-scheduler","children":[{"level":3,"title":"main.go","slug":"main-go","link":"#main-go","children":[]},{"level":3,"title":"metrics.go","slug":"metrics-go-1","link":"#metrics-go-1","children":[]}]},{"level":2,"title":"pkg/scheduler","slug":"pkg-scheduler","link":"#pkg-scheduler","children":[{"level":3,"title":"routes/route.go","slug":"routes-route-go","link":"#routes-route-go","children":[]},{"level":3,"title":"scheduler.go","slug":"scheduler-go","link":"#scheduler-go","children":[]},{"level":3,"title":"events.go","slug":"events-go","link":"#events-go","children":[]},{"level":3,"title":"nodes.go","slug":"nodes-go","link":"#nodes-go","children":[]},{"level":3,"title":"pods.go","slug":"pods-go","link":"#pods-go","children":[]},{"level":3,"title":"webhook.go","slug":"webhook-go","link":"#webhook-go","children":[]},{"level":3,"title":"score.go","slug":"score-go","link":"#score-go","children":[]},{"level":3,"title":"node_policy.go","slug":"node-policy-go","link":"#node-policy-go","children":[]},{"level":3,"title":"policy/gpu_policy.go","slug":"policy-gpu-policy-go","link":"#policy-gpu-policy-go","children":[]}]},{"level":2,"title":"cmd/device-plugin/nvidia","slug":"cmd-device-plugin-nvidia","link":"#cmd-device-plugin-nvidia","children":[{"level":3,"title":"main.go","slug":"main-go-1","link":"#main-go-1","children":[]},{"level":3,"title":"plugin-manager.go","slug":"plugin-manager-go","link":"#plugin-manager-go","children":[]},{"level":3,"title":"vgpucfg.go","slug":"vgpucfg-go","link":"#vgpucfg-go","children":[]}]},{"level":2,"title":"pkg/device-plugin/nvidiadevice/nvinternal","slug":"pkg-device-plugin-nvidiadevice-nvinternal","link":"#pkg-device-plugin-nvidiadevice-nvinternal","children":[{"level":3,"title":"plugin","slug":"plugin","link":"#plugin","children":[]},{"level":3,"title":"rm","slug":"rm","link":"#rm","children":[]},{"level":3,"title":"cdi","slug":"cdi","link":"#cdi","children":[]},{"level":3,"title":"mig/mig.go","slug":"mig-mig-go","link":"#mig-mig-go","children":[]}]}],"relativePath":"zh/blogs/20241020/index.md"}'),o={name:"zh/blogs/20241020/index.md"},l=n(`<nav class="table-of-contents"><ul><li><a href="#src-libvgpu-c">src/libvgpu.c</a></li><li><a href="#src-nvml">src/nvml</a><ul><li><a href="#hook-c">hook.c</a></li><li><a href="#nvml-entry-c">nvml_entry.c</a></li></ul></li><li><a href="#src-multiprocess">src/multiprocess</a><ul><li><a href="#shrreg-tool-c">shrreg_tool.c</a></li><li><a href="#multiprocess-memory-limit-c">multiprocess_memory_limit.c</a></li><li><a href="#multiprocess-utilization-watcher-c">multiprocess_utilization_watcher.c</a></li></ul></li><li><a href="#src-allocator-allocator-c">src/allocator/allocator.c</a></li><li><a href="#src-cuda">src/cuda/</a><ul><li><a href="#memory-c">memory.c</a></li></ul></li><li><a href="#src-utils-c">src/utils.c</a></li><li><a href="#include">include</a><ul><li><a href="#libnvml-hook-h">libnvml_hook.h</a></li><li><a href="#libcuda-hook-h">libcuda_hook.h</a></li></ul></li><li><a href="#cmd-vgpumonitor">cmd/vGPUmonitor/</a><ul><li><a href="#metrics-go">metrics.go</a></li><li><a href="#validation-go">validation.go</a></li><li><a href="#feedback-go">feedback.go</a></li><li><a href="#noderpc-noderpc-proto">noderpc/noderpc.proto</a></li></ul></li><li><a href="#pkg-monitor">pkg/monitor/</a><ul><li><a href="#nvidia-cudevshr-go">nvidia/cudevshr.go</a></li></ul></li><li><a href="#cmd-scheduler">cmd/scheduler/</a><ul><li><a href="#main-go">main.go</a></li><li><a href="#metrics-go-1">metrics.go</a></li></ul></li><li><a href="#pkg-scheduler">pkg/scheduler</a><ul><li><a href="#routes-route-go">routes/route.go</a></li><li><a href="#scheduler-go">scheduler.go</a></li><li><a href="#events-go">events.go</a></li><li><a href="#nodes-go">nodes.go</a></li><li><a href="#pods-go">pods.go</a></li><li><a href="#webhook-go">webhook.go</a></li><li><a href="#score-go">score.go</a></li><li><a href="#node-policy-go">node_policy.go</a></li><li><a href="#policy-gpu-policy-go">policy/gpu_policy.go</a></li></ul></li><li><a href="#cmd-device-plugin-nvidia">cmd/device-plugin/nvidia</a><ul><li><a href="#main-go-1">main.go</a></li><li><a href="#plugin-manager-go">plugin-manager.go</a></li><li><a href="#vgpucfg-go">vgpucfg.go</a></li></ul></li><li><a href="#pkg-device-plugin-nvidiadevice-nvinternal">pkg/device-plugin/nvidiadevice/nvinternal</a><ul><li><a href="#plugin">plugin</a></li><li><a href="#rm">rm</a></li><li><a href="#cdi">cdi</a></li><li><a href="#mig-mig-go">mig/mig.go</a></li></ul></li></ul></nav><p>第四范式开源的通用 GPU 虚拟化组件，支持多家 GPU 产品，目前只做了切分功能，已进 CNCF。比较像之前腾讯开源的 <a href="https://github.com/tkestack/gpu-manager" target="_blank" rel="noreferrer">https://github.com/tkestack/gpu-manager</a> 和对应的 <a href="https://github.com/tkestack/vcuda-controller%EF%BC%8C%E4%B9%8B%E5%89%8D%E4%B8%80%E7%9B%B4%E6%B2%A1%E4%BB%94%E7%BB%86%E8%AF%BB%EF%BC%8C%E5%88%9A%E5%A5%BD%E8%AF%BB%E4%B8%8B%E8%BF%99%E4%B8%AA%E8%BF%9B%E4%BA%86" target="_blank" rel="noreferrer">https://github.com/tkestack/vcuda-controller，之前一直没仔细读，刚好读下这个进了</a> CNCF 的吧。</p><h1 id="hami-core" tabindex="-1">HAMi-core <a class="header-anchor" href="#hami-core" aria-hidden="true">#</a></h1><p><a href="https://github.com/Project-HAMi/HAMi-core" target="_blank" rel="noreferrer">https://github.com/Project-HAMi/HAMi-core</a> 基于主线 6b2aed490910db1a33c6575ba81b1ecd96fce5f4</p><h2 id="src-libvgpu-c" tabindex="-1">src/libvgpu.c <a class="header-anchor" href="#src-libvgpu-c" aria-hidden="true">#</a></h2><p>劫持 <a href="http://libcuda.so" target="_blank" rel="noreferrer">libcuda.so</a> 和 <a href="http://libnvidia-ml.so" target="_blank" rel="noreferrer">libnvidia-ml.so</a> 的方法是通过劫持 dlsym 函数，如果用户查询的某个符号是 HAMI-core 可以拦截的，就返回对应的拦截函数。dlsym 则是通过暴露一个同名符号，通过 LD_PRELOAD 等方式对 libdl 做覆盖拦截。</p><h2 id="src-nvml" tabindex="-1">src/nvml <a class="header-anchor" href="#src-nvml" aria-hidden="true">#</a></h2><h3 id="hook-c" tabindex="-1">hook.c <a class="header-anchor" href="#hook-c" aria-hidden="true">#</a></h3><p>做了改动的一些 NVML API，查初始化的时候构造的真实函数指针表调用过去。最重要的就是 <code>_nvmlDeviceGetMemoryInfo</code> 这个实现。</p><p><code>_nvmlDeviceGetMemoryInfo</code> 里面 <code>nvmlDeviceGetIndex</code> 获取到设备的 id，通过 <code>nvml_to_cuda_map()</code> 转换成 <code>shared_region_info_t</code> 中用于标识虚拟设备的 id。根据 id 查询 <code>shared_region_info_t</code> 类型的全局单例 <code>region_info</code>，获取当前虚拟设备的显存限制和使用情况。</p><h3 id="nvml-entry-c" tabindex="-1">nvml_entry.c <a class="header-anchor" href="#nvml-entry-c" aria-hidden="true">#</a></h3><p>没有做改动直接调用下去的 NVML API。</p><h2 id="src-multiprocess" tabindex="-1">src/multiprocess <a class="header-anchor" href="#src-multiprocess" aria-hidden="true">#</a></h2><h3 id="shrreg-tool-c" tabindex="-1">shrreg_tool.c <a class="header-anchor" href="#shrreg-tool-c" aria-hidden="true">#</a></h3><p>一个命令行小工具，支持几个选项：</p><ul><li>create_new：创建了一个文件 <code>/tmp/cudevshr.cache</code>，后面会被用来做跨进程的共享区域，它只是保证这个文件存在。</li><li>Suspend/resume：对所有运行中的被监控到的进程执行 <code>SIGUSR1</code> 和 <code>SIGUSR2</code> 分别用于恢复和挂起这些任务。</li></ul><h3 id="multiprocess-memory-limit-c" tabindex="-1">multiprocess_memory_limit.c <a class="header-anchor" href="#multiprocess-memory-limit-c" aria-hidden="true">#</a></h3><p>这个文件里面比较杂，主要是 HAMI 的多进程资源使用情况的共享内存文件缓存、基于这个共享内存实现的显存管理、还有一些工具函数如host/container pid 转换、共享内存的加锁（lock_shrreg、unlock_shrreg ），虽然看文件名只是做显存限制的。vcuda-controller 里面实现的比较简单，就是在一个文件里面存了下每个进程的 pid，然后每个 API 调用都会调用 <code>nvmlDeviceGetComputeRunningProcesses</code> 去查然后去对 pid 做匹配，为了省时，搜索的时候对 pid 做了二分，总体上开销还是比较高的。HAMI 这里则是通过直接创建一个多进程共享的资源消耗统计文件，进行了缓存，减少 NVML API 调用次数。这个共享文件会被 mmap 到每个进程内，也就是 <code>shared_region_t</code> 类型的 <code>region_info.shared_region</code>。</p><p>初始化过程中主要做了两件事 <code>try_create_shrreg()</code> 和 <code>init_proc_slot_withlock()</code>。<code>try_create_shrreg</code> 就是创建、初始化上面提到的共享文件的过程。<code>init_proc_slot_withlock</code> 则是对共享内存中当前进程的 slot 做了初始化。为 <code>SIGUSR1</code> 和 <code>SIGUSR1</code> 分别注册信号处理函数 <code>sig_restore_stub</code> 和 <code>sig_swap_stub</code>，用来恢复/暂停显存分配。</p><p>那么有了上面的共享内存，HAMI 中就不是通过 NVML 去查询显存占用了，而是通过 <code>get_gpu_memory_usage</code> 直接查询共享内存缓存中的统计数据。那么它又是如何收集这个统计数据的呢？这就涉及到了另一个比较有趣的函数是 <code>add_gpu_device_memory_usage</code>，它在涉及到显存剧烈变化的 CUDA API 执行时被调用，用来修改共享内存中的显存消耗统计数据，同时它还支持对显存进行分类，区分了是 CUcontext 相关、CUmodule相关、数据相关三类。但是读完代码发现，其实只有通过 cuda API 分配的数据显存被准确统计到了，不知道是不是他们内部实现没有开源出来。CUmodule 实际上完全没统计，CUcontext 则是用了一下初始化后 primary context 的消耗去计算，不过除了 primary context，一般上层框架也基本没有自己去创建 CUcontext 的（其他厂商，AMD ROCm 甚至实际上没有区分 device 和 context）。</p><h3 id="multiprocess-utilization-watcher-c" tabindex="-1">multiprocess_utilization_watcher.c <a class="header-anchor" href="#multiprocess-utilization-watcher-c" aria-hidden="true">#</a></h3><p>对 cuda core 进行分配，与 vcuda-controller 中类似。</p><p><code>cuda_to_nvml_map</code> 变量定义在这里，在别的地方 extern 引用了。全局变量 <code>g_cycle</code> 为 10ms，<code>g_wait</code> 为 120ms。</p><p>几个重要的函数，<code>setspec()</code> 用于计算 cuda core 的总数。这里的 FACTOR 是 32，没太搞懂为啥，A100 每个 SM 是 64 个 cuda core，L20、H100 GPU 每个 SM 上是 128 个 cuda core，当然也可能这里表示的是 CUDA 一个 SM 上的 wrap size 是 32。over-subscription 的话有允许调度多个线程，所以是乘起来。感觉只是一个软性的估计，下面的限流器实现其实也可以看出来是允许超过限制的。</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">int setspec() {</span></span>
<span class="line"><span style="color:#A6ACCD;">    CHECK_CU_RESULT(cuDeviceGetAttribute(&amp;g_sm_num,CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT,0));</span></span>
<span class="line"><span style="color:#A6ACCD;">    CHECK_CU_RESULT(cuDeviceGetAttribute(&amp;g_max_thread_per_sm,CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_MULTIPROCESSOR,0));</span></span>
<span class="line"><span style="color:#A6ACCD;">    g_total_cuda_cores = g_max_thread_per_sm * g_sm_num * FACTOR;</span></span>
<span class="line"><span style="color:#A6ACCD;">    return 0;</span></span>
<span class="line"><span style="color:#A6ACCD;">}</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span></code></pre></div><p><code>rate_limiter()</code> 是对 utilization 做限制的核心实现（注意 nvidia-smi 看到 utilization 是一个时分的统计数据，这里从 cuda core 的角度去限制实际上是空分的），在 <code>cuLaunchKernel</code> 被调用的时候先触发它，用来限流。加载核函数的 grid 参数被用来计算需要占用的 cuda core 数量。这里在限流的时候，留了一个优先级的接口，<code>region_info.shared_region-&gt;priority</code> 目前看没啥实际用途。（NSDI 23 的一个类似的工作 <a href="https://github.com/pkusys/TGS%EF%BC%8C%E5%81%9A%E4%BA%86%E4%BC%98%E5%85%88%E7%BA%A7%E8%B0%83%E5%BA%A6%EF%BC%8C%E7%AE%80%E5%8D%95%E7%9C%8B%E4%BA%86%E4%B8%8B%E4%BB%A3%E7%A0%81%E9%83%BD%E6%98%AF%E5%90%8C%E6%A0%B9%E7%94%9F~%EF%BC%89%E3%80%82%E7%B2%BE%E7%AE%80%E5%90%8E%E6%A0%B8%E5%BF%83%E5%A6%82%E4%B8%8B" target="_blank" rel="noreferrer">https://github.com/pkusys/TGS，做了优先级调度，简单看了下代码都是同根生~）。精简后核心如下</a></p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#676E95;font-style:italic;">// 同样没有用 block，按 vcuda-controller 那边的解释，他们是认为 block 的影响没有 grid 大所以只用了 grid，所以只是留了一个参数给其他算法实现</span></span>
<span class="line"><span style="color:#C792EA;">void</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">rate_limiter</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">int</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">grids</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">int</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">blocks</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">  	</span><span style="color:#C792EA;">int</span><span style="color:#A6ACCD;"> before_cuda_cores </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#A6ACCD;">  	</span><span style="color:#C792EA;">int</span><span style="color:#A6ACCD;"> after_cuda_cores </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#A6ACCD;">	</span><span style="color:#C792EA;">int</span><span style="color:#A6ACCD;"> kernel_size </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> grids</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#A6ACCD;">    	</span><span style="color:#89DDFF;font-style:italic;">do</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">      		before_cuda_cores </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> g_cur_cuda_cores</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">		// cuda core 不够了，进入睡眠，每 10ms 检查一次 cuda core 资源，等待别的核函数跑完释放了 cuda core，再提交新的核函数。</span></span>
<span class="line"><span style="color:#F07178;">      		</span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">before_cuda_cores </span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        		</span><span style="color:#82AAFF;">nanosleep</span><span style="color:#89DDFF;">(&amp;</span><span style="color:#F07178;">g_cycle</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">NULL);</span></span>
<span class="line"><span style="color:#F07178;">			</span><span style="color:#89DDFF;font-style:italic;">continue</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">      		</span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">		// 更新如果加载了核函数，会剩余的 cuda core 数量。按这种实现，用户可以使用远超过限制的 cuda core？</span></span>
<span class="line"><span style="color:#F07178;">      		after_cuda_cores </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> before_cuda_cores </span><span style="color:#89DDFF;">-</span><span style="color:#F07178;"> kernel_size</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    	</span><span style="color:#89DDFF;">}</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;font-style:italic;">while</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(!</span><span style="color:#82AAFF;">CAS</span><span style="color:#89DDFF;">(&amp;</span><span style="color:#A6ACCD;">g_cur_cuda_cores</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> before_cuda_cores</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> after_cuda_cores</span><span style="color:#89DDFF;">));</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p><code>utilization_watcher</code> 是一个进程内的守护线程，在 <code>src/libvgpu.c</code> 中通过 <code>init_utilization_watcher()</code> 在初始化完成后创建，以 <code>g_wait</code> 为周期重新分配 cuda core 计算资源，较 vcuda-controller 中的 cuda core 分配策略做了很大的删减，代码简化后如下，</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">void*</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">utilization_watcher</span><span style="color:#89DDFF;">()</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C792EA;">int</span><span style="color:#A6ACCD;"> userutil</span><span style="color:#89DDFF;">[</span><span style="color:#A6ACCD;">CUDA_DEVICE_MAX_COUNT</span><span style="color:#89DDFF;">];</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C792EA;">int</span><span style="color:#A6ACCD;"> sysprocnum</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C792EA;">int</span><span style="color:#A6ACCD;"> share </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C792EA;">int</span><span style="color:#A6ACCD;"> upper_limit </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">get_current_device_sm_limit</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">while</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">){</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">	// 120ms 更新一次分配</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#82AAFF;">nanosleep</span><span style="color:#89DDFF;">(&amp;</span><span style="color:#F07178;">g_wait</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">NULL);</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">pidfound</span><span style="color:#89DDFF;">==</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">	  // 在 \`region_info.shared_region-&gt;procs\` 中注册自己，目前代码中写死了最多支持 1024 个进程。</span></span>
<span class="line"><span style="color:#F07178;">          </span><span style="color:#82AAFF;">update_host_pid</span><span style="color:#89DDFF;">();</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">	// 设置进程 sm 利用率为 0</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#82AAFF;">init_gpu_device_sm_utilization</span><span style="color:#89DDFF;">();</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">	// 这里实际上拿到了多卡的信息，而且和 vcuda-controller 不同的是它是会把查询结果写到一个共享文件里面做缓存。</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#82AAFF;">get_used_gpu_utilization</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">userutil</span><span style="color:#89DDFF;">,&amp;</span><span style="color:#F07178;">sysprocnum</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">((</span><span style="color:#F07178;">share</span><span style="color:#89DDFF;">==</span><span style="color:#F07178;">g_total_cuda_cores</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;&amp;</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">g_cur_cuda_cores</span><span style="color:#89DDFF;">&lt;</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">))</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">	  // 这里没看懂</span></span>
<span class="line"><span style="color:#F07178;">          g_total_cuda_cores </span><span style="color:#89DDFF;">*=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">          share </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> g_total_cuda_cores</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">	// 但是按这里的写法，当前是只支持了单卡，没有利用到上一步取出的多卡信息，</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">	// 根据利用率限制、当前的利用率、上次的变化值，计算这次分配额度的变化</span></span>
<span class="line"><span style="color:#F07178;">        share </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">delta</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">upper_limit</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">userutil</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">],</span><span style="color:#F07178;"> share</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">	// 应用计算出的额度变化，重新配置 \`g_cur_cuda_cores\`</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#82AAFF;">change_token</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">share</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>上面通过在 <code>get_used_gpu_utilization()</code> 内，调用 <code>nvmlDeviceGetComputeRunningProcesses</code> 获取所有正在使用某个 GPU 设备的进程，然后调用 <code>nvmlDeviceGetProcessUtilization</code> 获取每个进程的 GPU 利用率和显存占用，分别记录到了 <code>region_info.shared_region-&gt;procs[i].device_util[dev].sm_util</code> 和 <code>region_info.shared_region-&gt;procs[i].monitorused[dev]</code>。这个函数是考虑了多进程的。</p><h2 id="src-allocator-allocator-c" tabindex="-1">src/allocator/allocator.c <a class="header-anchor" href="#src-allocator-allocator-c" aria-hidden="true">#</a></h2><p><code>allocated_list</code> 是一个存了 <code>allocated_device_memory_struct</code> 的双向链表，实例化了两个全局变量 <code>device_overallocated</code>、<code>array_list</code>。成员定义如下</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">struct</span><span style="color:#F07178;"> </span><span style="color:#FFCB6B;">allocated_device_memory_struct</span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">    CUdeviceptr address</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">size_t</span><span style="color:#F07178;"> length</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    CUcontext ctx</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    CUmemGenericAllocationHandle </span><span style="color:#89DDFF;">*</span><span style="color:#F07178;">allocHandle</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#89DDFF;">};</span></span>
<span class="line"></span></code></pre></div><p><code>region_list</code> 是一个存了 <code>region_struct</code> 的双向链表，实例化成了一个全局变量 \`\`r_list。成员定义如下</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">struct</span><span style="color:#F07178;"> </span><span style="color:#FFCB6B;">region_struct</span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">size_t</span><span style="color:#F07178;"> start</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">size_t</span><span style="color:#F07178;"> freemark</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">size_t</span><span style="color:#F07178;"> freed_map</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">size_t</span><span style="color:#F07178;"> length</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    CUcontext ctx</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    allocated_list </span><span style="color:#89DDFF;">*</span><span style="color:#F07178;">region_allocs</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">char</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">*</span><span style="color:#F07178;">bitmap</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    CUmemGenericAllocationHandle </span><span style="color:#89DDFF;">*</span><span style="color:#F07178;">allocHandle</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#89DDFF;">};</span></span>
<span class="line"></span></code></pre></div><p>OVERSIZE 128M，IPCSIZE 2M，ALIGN 也是 2M。</p><p><code>oom_check()</code> 就是查询了上面提到的记录着进程信息的共享内存区域，获取当前设备的显存用量，加上请求分配的显存值，和设定的限制值做比较。如果超过限制，就尝试清理下已经结束的进程的显存记录，然后重新计算一遍。</p><p>剩下的接口就都是对 cuda 显存分配的一些抽象，把分配结果记录到上面创建的全局列表里面。他们底层又对应着 <code>add_chunk_async()</code></p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">int</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">allocate_raw</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">CUdeviceptr</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">*</span><span style="color:#A6ACCD;font-style:italic;">dptr</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">size_t</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">size</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#C792EA;">int</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">free_raw</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">CUdeviceptr</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">dptr</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#C792EA;">int</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">add_chunk_only</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">CUdeviceptr</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">address</span><span style="color:#89DDFF;">,</span><span style="color:#C792EA;">size_t</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">size</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#C792EA;">int</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">allocate_async_raw</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">CUdeviceptr</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">*</span><span style="color:#A6ACCD;font-style:italic;">dptr</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">size_t</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">size</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">CUstream</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">hStream</span><span style="color:#89DDFF;">);</span><span style="color:#676E95;font-style:italic;"> // 基于 add_chunk_async</span></span>
<span class="line"><span style="color:#C792EA;">int</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">free_raw_async</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">CUdeviceptr</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">dptr</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">CUstream</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">hStream</span><span style="color:#89DDFF;">);</span></span>
<span class="line"></span></code></pre></div><p><code>check_memory_type()</code> 就是在 <code>device_overallocated</code> 里面检查有没有查询的指针，判断是设备地址还是 host 侧地址。值得注意的是按这个实现，<code>cuMemAllocManaged</code> 分配出来的地址算到了设备地址里面。</p><h2 id="src-cuda" tabindex="-1">src/cuda/ <a class="header-anchor" href="#src-cuda" aria-hidden="true">#</a></h2><p><a href="http://libcuda.so" target="_blank" rel="noreferrer">libcuda.so</a> 库劫持逻辑，好多 API 其实没实现劫持方案，只是打印了一下日志，感觉是之后打算做。劫持的时候注意一下 <code>cuGetProcAddress</code> 即可。</p><h3 id="memory-c" tabindex="-1">memory.c <a class="header-anchor" href="#memory-c" aria-hidden="true">#</a></h3><p>劫持了显存分配 API，逻辑上就是先调用 <code>oom_check()</code> 检查一下，如果超过显存限制，就不分配了直接返回 OOM。</p><p>比较特殊的是 <code>cuMemHostAlloc</code>、<code>cuMemAllocHost_v2</code>、<code>cuMemHostRegister_v2</code>，这三个 API，则是先真实触发进行分配，再调用 <code>oom_check()</code> 检查显存，如果超了，就释放掉回滚刚刚到真实分配。这个逻辑感觉很迷惑，这三个 API 的分配并没有涉及到对 <code>src/allocator/allocator.c</code> 中的全局链表的修改，也就是说分配前后实际上检查的效果是一样的，为什么要先分配再回滚呢？</p><h2 id="src-utils-c" tabindex="-1">src/utils.c <a class="header-anchor" href="#src-utils-c" aria-hidden="true">#</a></h2><p>定义了一个跨进程的锁，<code>&quot;/tmp/vgpulock/lock”</code>，<code>try_lock_unified_lock</code> 通过标记 <code>O_EXCL</code> 互斥地打开该文件作为锁。</p><p><code>parse_cuda_visible_env</code> 查看环境变量中 <code>CUDA_VISIBLE_DEVICES</code>，尝试对卡的序号进行修正，存到 <code>cuda_to_nvml_map</code> 里面。那这里实际上只支持 nvidia-container-toolkit 对应的 runc 场景，其他厂商都还需要适配。比如昇腾，他们接入 HAMI，是也拿 <code>CUDA_VISIBLE_DEVICES</code> 去做自己的 runc 配置的环境变量了么。</p><p><code>mergepid</code> 接收两个 <code>nvmlDeviceGetComputeRunningProcesses</code> 采集到的进程组，合并到一个里面。实际上不要这个函数也行，反正现在只支持单卡，这个函数只是为了对多卡的<code>nvmlDeviceGetComputeRunningProcesses</code> 返回结果做聚合。</p><p><code>getextrapid</code> 比较两个<code>nvmlDeviceGetComputeRunningProcesses</code> 采集到的进程组，找到新增的那一个进程。</p><p><code>set_task_pid</code> 是为容器内 pid 和 host 侧 pid 建立关联，因为 <code>nvmlDeviceGetComputeRunningProcesses</code> 可以获取到占用 GPU 设备的 host 侧进程号。先调用一次<code>mergepid</code> 将所有占用 GPU 的进程记录到 <code>pre_pids_on_device</code>。<code>cuDevicePrimaryCtxRetain</code> 之后（看上去没激活 ctx 的话不会被 nvml 检测到？），再调用一次 <code>mergepid</code>把所有占用 GPU 的进程记录到 <code>pids_on_device</code>，然后通过<code>getextrapid</code>过滤出来一个新增的进程，就是当前进程在 host 侧的进程号，然后它通过 <code>set_host_pid</code> 把这个 hostpid 写入到 region_info 的共享内存当前进程的分区中。如果有恰好在查找过程中退出的进程，没有影响，因为我们只关心新增的进程，而别的新增进程还卡在 <code>try_lock_unified_lock</code> 那里。</p><h2 id="include" tabindex="-1">include <a class="header-anchor" href="#include" aria-hidden="true">#</a></h2><h3 id="libnvml-hook-h" tabindex="-1">libnvml_hook.h <a class="header-anchor" href="#libnvml-hook-h" aria-hidden="true">#</a></h3><p>定义了宏如 NVML_OVERRIDE_CALL，和用于标识 NVML API 的枚举 NVML_OVERRIDE_ENUM_t。实现上不够简洁很多地方可以 #include 同一个 API 列表去做替换。也没看到生成这些头文件的相关脚本，后面升级更新 API 列表很麻烦。</p><h3 id="libcuda-hook-h" tabindex="-1">libcuda_hook.h <a class="header-anchor" href="#libcuda-hook-h" aria-hidden="true">#</a></h3><p>类似 libnvml_hook.h</p><h1 id="hami" tabindex="-1">HAMi <a class="header-anchor" href="#hami" aria-hidden="true">#</a></h1><p><a href="https://github.com/Project-HAMi/HAMi" target="_blank" rel="noreferrer">https://github.com/Project-HAMi/HAMi</a></p><p>HAMI-core 中的劫持库，会被编译成 <a href="http://libvgpu.so" target="_blank" rel="noreferrer">libvgpu.so</a>，通过挂载 ld.so.preload 文件的方式注入到容器里面做 cuda/nvml 劫持。</p><h2 id="cmd-vgpumonitor" tabindex="-1">cmd/vGPUmonitor/ <a class="header-anchor" href="#cmd-vgpumonitor" aria-hidden="true">#</a></h2><p>看代码像是内部删了一些东西才开源的，好多没用到的符号……main.go 中开了两个协程分别运行 <code>initMetrics</code> 和 <code>watchAndFeedback</code>。</p><h3 id="metrics-go" tabindex="-1">metrics.go <a class="header-anchor" href="#metrics-go" aria-hidden="true">#</a></h3><p>将自定义的 vGPU 数据格式转换收集到 Promethus。</p><p><code>initMetrics</code> 监听了 <code>9394</code> 端口，<code>/metrics</code> 会被路由到 Prometheus 服务。由 <code>ClusterManagerCollector</code> 实现 Prometheus 的 <code>Collector</code> 接口，负责采集 metrics 并注册到 Prometheus，注册时定义了 metric 的标签 zone为 <code>vGPU</code>。接口 <code>Collect()</code> 实现时候就是通过 nvml 获取了 host 侧 GPU 指标，通过 <code>ContainerLister.ListContainers()</code> 获取了每个容器的 vGPU 指标。</p><h4 id="testcollector-main-go" tabindex="-1">testcollector/main.go <a class="header-anchor" href="#testcollector-main-go" aria-hidden="true">#</a></h4><p>验证 metrics.go 中的 Prometheus 数据采集。</p><h3 id="validation-go" tabindex="-1">validation.go <a class="header-anchor" href="#validation-go" aria-hidden="true">#</a></h3><p><code>ValidateEnvVars</code> 检查了一下 <code>HOOK_PATH</code> 环境变量是否配置了。</p><h3 id="feedback-go" tabindex="-1">feedback.go <a class="header-anchor" href="#feedback-go" aria-hidden="true">#</a></h3><p><code>watchAndFeedback</code> 中每五秒，通过 <code>pkg/monitor/nvidia.ContainerLister</code> 遍历一遍所有容器，记录容器配置的优先级，综合各个容器内的 <code>Priority</code>、<code>RecentKernel</code>、<code>UtilizationSwitch</code> 信息分别修改他们的配置。 这里没看懂修改这三个变量的逻辑，还得回到 HAMI-core 那边联合起来看下。看上去 <code>Priority</code> 是数值越小优先级越高。</p><h3 id="noderpc-noderpc-proto" tabindex="-1">noderpc/noderpc.proto <a class="header-anchor" href="#noderpc-noderpc-proto" aria-hidden="true">#</a></h3><p>定义了一个 gRPC 服务用来获取各个 POD 中的 vGPU 使用情况，<code>rpc GetNodeVGPU (GetNodeVGPURequest) returns (GetNodeVGPUReply) {}</code>。响应中的 <code>sharedRegionT</code>，也就是 HAMI-core 中存放资源切分数据的共享内存中的数据。</p><h2 id="pkg-monitor" tabindex="-1">pkg/monitor/ <a class="header-anchor" href="#pkg-monitor" aria-hidden="true">#</a></h2><h3 id="nvidia-cudevshr-go" tabindex="-1">nvidia/cudevshr.go <a class="header-anchor" href="#nvidia-cudevshr-go" aria-hidden="true">#</a></h3><p>为 HAMI-core 中共享内存区域内的数据，定义了 v0 和 v1 两个版本的统计信息，通过统一接口 <code>UsageInfo</code>。单个容器的统计信息如下</p><div class="language-go"><button title="Copy Code" class="copy"></button><span class="lang">go</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;">type</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">ContainerUsage</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">struct</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    PodUID        </span><span style="color:#C792EA;">string</span></span>
<span class="line"><span style="color:#A6ACCD;">    ContainerName </span><span style="color:#C792EA;">string</span></span>
<span class="line"><span style="color:#A6ACCD;">    data          </span><span style="color:#89DDFF;">[]</span><span style="color:#C792EA;">byte</span></span>
<span class="line"><span style="color:#A6ACCD;">    Info          UsageInfo</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p><code>ContainerUsage</code> 数据存储在 <code>ContainerLister</code> 类型中，<code>ContainerLister.ListContainers()</code>在上面看到过的 <code>vGPUmonitor</code> 中被用来获取容器内的统计信息。 <code>ContainerLister.Update()</code> 则是遍历各个，通过 <code>loadCache()</code> 函数获取容器的统计数据 <code>ContainerUsage</code>。如果容器内没有调用过 cuInit，那 <code>loadCache()</code> 不会统计到它。 函数<code>loadCache()</code> 实现的逻辑是，查询文件 <code>$HOOK_PATH/containers/$POD_NAME/.cache</code>（<a href="http://libvgpu.so" target="_blank" rel="noreferrer">libvgpu.so</a> 也在该目录内），然后直接 mmap 读取出来转换成符合 <code>UsageInfo</code> 接口的数据。 \`</p><h2 id="cmd-scheduler" tabindex="-1">cmd/scheduler/ <a class="header-anchor" href="#cmd-scheduler" aria-hidden="true">#</a></h2><p>节点/GPU 调度器，实现在 <code>pkg/scheduler/scheduler.go</code>。</p><h3 id="main-go" tabindex="-1">main.go <a class="header-anchor" href="#main-go" aria-hidden="true">#</a></h3><p>启动两个协程运行收集集群中的节点设备信息的 <code>Scheduler.RegisterFromNodeAnnotations()</code> 和采集上报 Prometheus metric 的 <code>initMetrics()</code>，然后默认监听 <code>8080</code> 端口，为 <code>pkg/scheduler/routes/route.go</code> 中定义的 http 服务提供扩展 k8s 调度器服务。</p><h3 id="metrics-go-1" tabindex="-1">metrics.go <a class="header-anchor" href="#metrics-go-1" aria-hidden="true">#</a></h3><p>和 vGPUmonitor 中的 <code>cmd/vGPUmonitor/metrics.go</code> 一致，不同的地方是容器内信息来自 <code>pkg/scheduler/scheduler.go</code> 中的 <code>Scheduler</code> 类型，例如这里获取 host 侧指标是通过 <code>Scheduler.InspectAllNodesUsage()</code>，每个容器的信息是通过 <code>podManager.GetScheduledPods()</code></p><h2 id="pkg-scheduler" tabindex="-1">pkg/scheduler <a class="header-anchor" href="#pkg-scheduler" aria-hidden="true">#</a></h2><p>通过 <code>k8s.io/kube-scheduler/extender/v1</code> API 拓展 k8s 的调度器。k8s 调度器负责将 Pod 分配到合适的节点，而扩展调度器可以让用户自定义调度逻辑，一般都会包含过滤、打分、绑定等机制。</p><h3 id="routes-route-go" tabindex="-1">routes/route.go <a class="header-anchor" href="#routes-route-go" aria-hidden="true">#</a></h3><p><code>cmd/scheduler/main.go</code> 中定义的路由为</p><ul><li><code>/filter</code> 对应 <code>PredicateRoute</code>。调用 <code>Scheduler.Filter()</code>，处理 Pod 调度过滤逻辑。</li><li><code>/bind</code> 对应 <code>Bind</code>，调用<code>Scheduler.Bind()</code>，处理 Pod 调度绑定逻辑。</li><li><code>/webhook</code> 对应 <code>WebHookRoute</code>，调用 <code>Scheduler.NewWebHook()</code> 创建 webhook，在 webhook 上调用 <code>ServeHTTP()</code>。</li><li><code>/healthz</code> 对应 <code>HealthzRoute</code>，心跳包。</li></ul><h3 id="scheduler-go" tabindex="-1">scheduler.go <a class="header-anchor" href="#scheduler-go" aria-hidden="true">#</a></h3><p>调度器类定义</p><div class="language-go"><button title="Copy Code" class="copy"></button><span class="lang">go</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;">type</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">Scheduler</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">struct</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    nodeManager</span></span>
<span class="line"><span style="color:#A6ACCD;">    podManager</span></span>
<span class="line"><span style="color:#A6ACCD;">    stopCh     </span><span style="color:#89DDFF;">chan</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">struct{}</span></span>
<span class="line"><span style="color:#A6ACCD;">    kubeClient kubernetes</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">Interface</span></span>
<span class="line"><span style="color:#A6ACCD;">    podLister  listerscorev1</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">PodLister</span></span>
<span class="line"><span style="color:#A6ACCD;">    nodeLister listerscorev1</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">NodeLister</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;">//Node status returned by filter</span></span>
<span class="line"><span style="color:#A6ACCD;">    cachedstatus </span><span style="color:#89DDFF;">map[</span><span style="color:#C792EA;">string</span><span style="color:#89DDFF;">]*</span><span style="color:#A6ACCD;">NodeUsage</span></span>
<span class="line"><span style="color:#A6ACCD;">    nodeNotify   </span><span style="color:#89DDFF;">chan</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">struct{}</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;">//Node Overview</span></span>
<span class="line"><span style="color:#A6ACCD;">    overviewstatus </span><span style="color:#89DDFF;">map[</span><span style="color:#C792EA;">string</span><span style="color:#89DDFF;">]*</span><span style="color:#A6ACCD;">NodeUsage</span></span>
<span class="line"><span style="color:#A6ACCD;">    eventRecorder record</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">EventRecorder</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>调度器类实现了接口 <code>onUpdateNode()</code>、<code>onDelNode()</code>、<code>onAddNode()</code>，当集群中的节点变更时，会通过回调写入事件到 <code>Scheduler.nodeNotify</code>。</p><p>它也实现了<code>onAddPod()</code>、<code>onUpdatePod()</code>、<code>onDelPod()</code>，其中 <code>onUpdatePod()</code> 算是 <code>onAddPod（）</code> 的一种特殊情况。这几个回调会直接调用 <code>Scheduler.addPod()</code> 和 <code>Scheduler.delPod()</code>。</p><p><code>Scheduler.RegisterFromNodeAnnotations()</code> 会启动一个无限循环，每隔一段时间或收到节点变更通知时，执行节点注册逻辑，直到<code>Scheduler.stopCh</code> 收到信号终止。它通过 <code>Scheduler.nodeLister</code> 获取所有节点，做一次健康检测，然后把每个正常的节点信息建模成 <code>util.NodeInfo</code> 类型，通过 <code>Scheduler.addNode()</code> 函数在调度器中存储节点。注册完毕后会调用一次 <code>Scheduler.getNodesUsage()</code> 获取所有节点/Pod 的设备和显存信息，更新到 <code>Scheduler.cachedstatus</code> 中，不过这个成员看上去还没有怎么用到，可能是之后想做缓存。</p><p><code>Scheduler.Filter()</code> 和 <code>Scheduler.Bind()</code> 就是之前 <code>routes/route.go</code> 部分提过的调度器做设备分配和过滤的 http 接口的实现。<code>Scheduler.Filter()</code> 首先尝试清理掉当前请求相关的 Pod 以避免对节点资源统计数据造成偏差，然后通过<code>Scheduler.getNodesUsage()</code>获取当前所有的节点及其资源信息，然后通过 <code>Scheduler.calcScore()</code> 计算一遍节点的分值和对应的节点信息、节点设备信息，按分值排序后获取最优的一个可分配节点，最后调整 Pod 的 annotation，调用 <code>Scheduler.addPod()</code>记录。<code>Scheduler.Bind()</code> 的实现比较简单，就是调用 k8s 的API 获取 node 和 pod，对 node 加锁（锁实现见 <code>pkg/util/nodelock/nodelock.go</code>），然后调用 k8s 的 Bind API 去把 pod 调度到请求的节点上。</p><h3 id="events-go" tabindex="-1">events.go <a class="header-anchor" href="#events-go" aria-hidden="true">#</a></h3><p>基于 <code>k8s.io/client-go/tools/record</code> 中的 <code>EventRecorder</code>，将Pod调度过程中的绑定/过滤事件记录到 k8s事件系统中，便于后续的故障排查和状态监控。</p><h3 id="nodes-go" tabindex="-1">nodes.go <a class="header-anchor" href="#nodes-go" aria-hidden="true">#</a></h3><p><code>nodeManager</code> 持有一个节点和设备的哈希表，可以通过 <code>nodeManager.addNode()</code> 和 <code>nodeManager.rmNodeDevice()</code> 添加、删除调度器自身维护的节点上的 GPU 设备信息。</p><h3 id="pods-go" tabindex="-1">pods.go <a class="header-anchor" href="#pods-go" aria-hidden="true">#</a></h3><p><code>podManager</code> 持有一个已被调度的 Pod 的哈希表，可以通过 <code>podManager.addPod()</code> 和 <code>podManager.delPod()</code> 添加、删除调度器自身维护的 Pod 信息。</p><h3 id="webhook-go" tabindex="-1">webhook.go <a class="header-anchor" href="#webhook-go" aria-hidden="true">#</a></h3><p>k8s 允许集群中存在多个调度器。默认情况下，Pod 使用的是 kube-scheduler 调度器。通过设置 SchedulerName 字段，我们可以指定哪个调度器来调度特定的 Pod。 这个 webhook 的 <code>Handle()</code> 就是用来为合法的 Pod 选择使用 HAMI 实现的调度器进行调度。它的实现逻辑是，先检查是否 Pod 内有容器，如果有，再去看是不是特权容器，如果不是特权容器，再调用 <code>pkg/device/devices.go</code> 中定义的 <code>Device</code> 公共接口，检查容器的资源限制、annotation 等是否符合对应设备的配置规范，如果合规就修改调度器。</p><h3 id="score-go" tabindex="-1">score.go <a class="header-anchor" href="#score-go" aria-hidden="true">#</a></h3><p>实现 k8s 中调度算法的核心，打分机制。该文件内的函数之间的调用链为 <code>Scheduler.calcScore()-&gt;fitInDevices()-&gt;fitInCertainDevice()</code>，对每个节点、每个请求进行遍历，又会对请求中的每种设备需求进行检查，最后返回一个包含节点、设备（包含了针对请求的分配情况）、分值的 <code>policy.NodeScore</code> 的列表。最外层的 <code>Scheduler.calcScore()</code> 是被 <code>Scheduler.Filter()</code> 用在了调度器的节点过滤逻辑中，用来计算节点、设备的分数选择节点设备。</p><h3 id="node-policy-go" tabindex="-1">node_policy.go <a class="header-anchor" href="#node-policy-go" aria-hidden="true">#</a></h3><p>节点调度策略，目前实现了两个，<code>binpack</code> 和 <code>spread</code>，默认为 <code>binpack</code>，优先占满节点，可以通过 POD 级别的 annotation <code>hami.io/node-scheduler-policy</code>进行修改。</p><p>为 <code>NodeScoreList</code> 定义了 <code>Less</code> 接口，按节点的分数进行排序。根据节点上的设备使用占比、设备核心使用占比，显存使用占比，三者求和计算分数。</p><h3 id="policy-gpu-policy-go" tabindex="-1">policy/gpu_policy.go <a class="header-anchor" href="#policy-gpu-policy-go" aria-hidden="true">#</a></h3><p>GPU 调度策略，目前实现了两个，<code>binpack</code> 和 <code>spread</code>，默认为 <code>spread</code>，优先均匀分布，可以通过 POD 级别的 annotation <code>hami.io/gpu-scheduler-policy</code> 进行修改。</p><p>为 <code>DeviceUsageList</code> 定义了 <code>Less</code> 接口，按节点的分数进行排序。根据设备上的设备使用占比、设备核心使用占比，显存使用占比，三者求和计算分数，这里注意是要加上申请的额度的。</p><h2 id="cmd-device-plugin-nvidia" tabindex="-1">cmd/device-plugin/nvidia <a class="header-anchor" href="#cmd-device-plugin-nvidia" aria-hidden="true">#</a></h2><p>为英伟达设备实现的 <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/" target="_blank" rel="noreferrer">k8s Device Plugin</a>，HAMI 一开始只支持 nvidia，放在这里也算是一个实现范例，其他厂商的 DP 在别的仓库。 也相当于是 nvidia 官方的 <a href="https://github.com/NVIDIA/k8s-device-plugin" target="_blank" rel="noreferrer">DP</a> 的一个拓展。</p><h3 id="main-go-1" tabindex="-1">main.go <a class="header-anchor" href="#main-go-1" aria-hidden="true">#</a></h3><p>通过 <code>startPlugins()</code> 启动 DP 的服务，它的主要逻辑如下</p><div class="language-go"><button title="Copy Code" class="copy"></button><span class="lang">go</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;">func</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">startPlugins</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">c </span><span style="color:#89DDFF;">*</span><span style="color:#A6ACCD;">cli</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">Context</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> flags </span><span style="color:#89DDFF;">[]</span><span style="color:#A6ACCD;">cli</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">Flag</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> restarting </span><span style="color:#C792EA;">bool</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">([]</span><span style="color:#A6ACCD;">plugin</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">Interface</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">bool</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">error</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    config</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> err </span><span style="color:#89DDFF;">:=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">loadConfig</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">c</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> flags</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">disableResourceRenamingInConfig</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">config</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    devConfig</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> err </span><span style="color:#89DDFF;">:=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">generateDeviceConfigFromNvidia</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">config</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> c</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> flags</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;">// Update the configuration file with default resources.</span></span>
<span class="line"><span style="color:#A6ACCD;">    err </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> rm</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">AddDefaultResourcesToConfig</span><span style="color:#89DDFF;">(&amp;</span><span style="color:#A6ACCD;">devConfig</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;">// Get the set of plugins.</span></span>
<span class="line"><span style="color:#A6ACCD;">    pluginManager</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> err </span><span style="color:#89DDFF;">:=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">NewPluginManager</span><span style="color:#89DDFF;">(&amp;</span><span style="color:#A6ACCD;">devConfig</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    plugins</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> err </span><span style="color:#89DDFF;">:=</span><span style="color:#A6ACCD;"> pluginManager</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">GetPlugins</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;">// Loop through all plugins, starting them if they have any devices to serve. </span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#A6ACCD;"> _</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> p </span><span style="color:#89DDFF;">:=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;font-style:italic;">range</span><span style="color:#A6ACCD;"> plugins </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">len</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">p</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Devices</span><span style="color:#89DDFF;">())</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">==</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">0</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">            </span><span style="color:#89DDFF;font-style:italic;">continue</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> err </span><span style="color:#89DDFF;">:=</span><span style="color:#A6ACCD;"> p</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Start</span><span style="color:#89DDFF;">();</span><span style="color:#A6ACCD;"> err </span><span style="color:#89DDFF;">!=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">nil</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">            </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> plugins</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">true,</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">nil</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> plugins</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">false,</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">nil</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p><code>disableResourceRenamingInConfig()</code> 中禁用了官方 DP 中对设备的重命名，之后会恢复回来，应该是受限于当前的 HAMI-core 的实现？</p><h3 id="plugin-manager-go" tabindex="-1">plugin-manager.go <a class="header-anchor" href="#plugin-manager-go" aria-hidden="true">#</a></h3><p><code>NewPluginManager()</code> 根据配置生成一个 DP 的工厂用来构建各种依赖的 DP，后面就会看到不只有一个，例如 nvml 也需要单独的 DP 配置。</p><h3 id="vgpucfg-go" tabindex="-1">vgpucfg.go <a class="header-anchor" href="#vgpucfg-go" aria-hidden="true">#</a></h3><p>实现了用来解析 HAMI 自定义的参数的工具函数 <code>generateDeviceConfigFromNvidia()</code>。</p><h2 id="pkg-device-plugin-nvidiadevice-nvinternal" tabindex="-1">pkg/device-plugin/nvidiadevice/nvinternal <a class="header-anchor" href="#pkg-device-plugin-nvidiadevice-nvinternal" aria-hidden="true">#</a></h2><p>也是基于英伟达官方的 <a href="https://github.com/NVIDIA/k8s-device-plugin/blob/main/internal" target="_blank" rel="noreferrer">https://github.com/NVIDIA/k8s-device-plugin/blob/main/internal</a> 中的 DP 实现进行了拓展。</p><h3 id="plugin" tabindex="-1">plugin <a class="header-anchor" href="#plugin" aria-hidden="true">#</a></h3><h4 id="api-go" tabindex="-1">api.go <a class="header-anchor" href="#api-go" aria-hidden="true">#</a></h4><p>定义接口</p><div class="language-go"><button title="Copy Code" class="copy"></button><span class="lang">go</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;">type</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">Interface</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">interface</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">Devices</span><span style="color:#89DDFF;">()</span><span style="color:#A6ACCD;"> rm</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">Devices</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">Start</span><span style="color:#89DDFF;">()</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">error</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">Stop</span><span style="color:#89DDFF;">()</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">error</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><h4 id="server-go" tabindex="-1">server.go <a class="header-anchor" href="#server-go" aria-hidden="true">#</a></h4><p>定义了类型 <code>NvidiaDevicePlugin</code>，实现 DP 的标准服务接口</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">service DevicePlugin {</span></span>
<span class="line"><span style="color:#A6ACCD;">      // GetDevicePluginOptions returns options to be communicated with Device Manager.</span></span>
<span class="line"><span style="color:#A6ACCD;">      rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;">      // ListAndWatch returns a stream of List of Devices</span></span>
<span class="line"><span style="color:#A6ACCD;">      // Whenever a Device state change or a Device disappears, ListAndWatch</span></span>
<span class="line"><span style="color:#A6ACCD;">      // returns the new list</span></span>
<span class="line"><span style="color:#A6ACCD;">      rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;">      // Allocate is called during container creation so that the Device</span></span>
<span class="line"><span style="color:#A6ACCD;">      // Plugin can run device specific operations and instruct Kubelet</span></span>
<span class="line"><span style="color:#A6ACCD;">      // of the steps to make the Device available in the container</span></span>
<span class="line"><span style="color:#A6ACCD;">      rpc Allocate(AllocateRequest) returns (AllocateResponse) {}</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;">      // GetPreferredAllocation returns a preferred set of devices to allocate</span></span>
<span class="line"><span style="color:#A6ACCD;">      // from a list of available ones. The resulting preferred allocation is not</span></span>
<span class="line"><span style="color:#A6ACCD;">      // guaranteed to be the allocation ultimately performed by the</span></span>
<span class="line"><span style="color:#A6ACCD;">      // devicemanager. It is only designed to help the devicemanager make a more</span></span>
<span class="line"><span style="color:#A6ACCD;">      // informed allocation decision when possible.</span></span>
<span class="line"><span style="color:#A6ACCD;">      rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {}</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;">      // PreStartContainer is called, if indicated by Device Plugin during registration phase,</span></span>
<span class="line"><span style="color:#A6ACCD;">      // before each container start. Device plugin can run device specific operations</span></span>
<span class="line"><span style="color:#A6ACCD;">      // such as resetting the device before making devices available to the container.</span></span>
<span class="line"><span style="color:#A6ACCD;">      rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {}</span></span>
<span class="line"><span style="color:#A6ACCD;">}</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span></code></pre></div><h4 id="register-go" tabindex="-1">register.go <a class="header-anchor" href="#register-go" aria-hidden="true">#</a></h4><h4 id="manager" tabindex="-1">manager <a class="header-anchor" href="#manager" aria-hidden="true">#</a></h4><p>不同平台下的 DP manager。</p><h5 id="api-go-1" tabindex="-1">api.go <a class="header-anchor" href="#api-go-1" aria-hidden="true">#</a></h5><p>定义接口</p><div class="language-go"><button title="Copy Code" class="copy"></button><span class="lang">go</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;">type</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">Interface</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">interface</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">GetPlugins</span><span style="color:#89DDFF;">()</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">([]</span><span style="color:#A6ACCD;">plugin</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">Interface</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">error</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">CreateCDISpecFile</span><span style="color:#89DDFF;">()</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">error</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><h5 id="factory-go" tabindex="-1">factory.go <a class="header-anchor" href="#factory-go" aria-hidden="true">#</a></h5><p><code>manager</code> 类型用来初始化 nvml、cdi，解析环境是 nvml 类型还是 tegra 类型，与后面的 manager 实现、<code>rm</code> 模块有关。</p><p><code>New()</code> 中 <code>manager</code> 被拓展成了 <code>nvmlmanager</code>（一般情况都是基于 nvml 来管理）、<code>tegramanager</code> （tegra 设备使用）和 <code>null</code>（错误情况下的 fallback） 三类 manager，他们均需要实现 <code>api.Interface</code>。</p><h5 id="null-go" tabindex="-1">null.go <a class="header-anchor" href="#null-go" aria-hidden="true">#</a></h5><p><code>null</code> manager 实现。</p><h5 id="nvml-go" tabindex="-1">nvml.go <a class="header-anchor" href="#nvml-go" aria-hidden="true">#</a></h5><p><code>nvmlmanager</code> 实现。<code>nvmlmanager.GetPlugins()</code> 接口，通过 <code>rm.NewNVMLResourceManagers()</code> 获取所有资源，对每个资源，通过 <code>plugin/server.go</code> 中 <code>plugin.NewNvidiaDevicePlugin()</code> 构建 DP。</p><h5 id="tegra-go" tabindex="-1">tegra.go <a class="header-anchor" href="#tegra-go" aria-hidden="true">#</a></h5><p><code>tegramanager</code> 实现。</p><h3 id="rm" tabindex="-1">rm <a class="header-anchor" href="#rm" aria-hidden="true">#</a></h3><p>分配、管理、监控每个资源对应的 GPU 设备。</p><h4 id="rm-go" tabindex="-1">rm.go <a class="header-anchor" href="#rm-go" aria-hidden="true">#</a></h4><p>实现 <code>resourceManager</code>，负责管理 GPU 设备。定义接口 <code>ResourceManager</code>。</p><div class="language-go"><button title="Copy Code" class="copy"></button><span class="lang">go</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;">type</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">ResourceManager</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">interface</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">Resource</span><span style="color:#89DDFF;">()</span><span style="color:#A6ACCD;"> spec</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">ResourceName</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">Devices</span><span style="color:#89DDFF;">()</span><span style="color:#A6ACCD;"> Devices</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">GetDevicePaths</span><span style="color:#89DDFF;">([]</span><span style="color:#C792EA;">string</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">[]</span><span style="color:#C792EA;">string</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">GetPreferredAllocation</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">available</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> required </span><span style="color:#89DDFF;">[]</span><span style="color:#C792EA;">string</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> size </span><span style="color:#C792EA;">int</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">([]</span><span style="color:#C792EA;">string</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">error</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">CheckHealth</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">stop </span><span style="color:#89DDFF;">&lt;-chan</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">interface{},</span><span style="color:#A6ACCD;"> unhealthy </span><span style="color:#89DDFF;">chan&lt;-</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">*</span><span style="color:#A6ACCD;">Device</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">error</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p><code>NewResourceManagers()</code> 为每个资源创建 <code>ResourceManager</code> 接口类型，一般来说使用的是 <code>NewNVMLResourceManagers()</code> （不需要考虑 tegra 设备）。</p><h4 id="allocate-go" tabindex="-1">allocate.go <a class="header-anchor" href="#allocate-go" aria-hidden="true">#</a></h4><p>实现了两个 GPU 分配算法，用户可以使用 <code>resourceManager.getPreferredAllocation()</code> 获取分配出的 GPU 设备。</p><p>其中一个集成了 <a href="https://github.com/NVIDIA/go-gpuallocator/" target="_blank" rel="noreferrer">https://github.com/NVIDIA/go-gpuallocator/</a> 中的 GPU 分配器，它会借助 nvml 识别拓扑关系，按预定的策略选择合适的 GPU 设备，<code>resourceManager.alignedAlloc()</code>。 另一个则是考虑了过往的分配情况，尽可能均匀地完成分配，<code>resourceManager.distributedAlloc()</code>。</p><h4 id="device-go" tabindex="-1">device.go <a class="header-anchor" href="#device-go" aria-hidden="true">#</a></h4><p><code>Device</code> 类包装 k8s 的 DP 中对设备的抽象，即 <a href="http://k8s.io/kubelet/pkg/apis/deviceplugin/v1beta1" target="_blank" rel="noreferrer">k8s.io/kubelet/pkg/apis/deviceplugin/v1beta1</a> 中的 <code>Device</code>。提供了一组公共的接口</p><div class="language-go"><button title="Copy Code" class="copy"></button><span class="lang">go</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;">type</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">deviceInfo</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">interface</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">GetUUID</span><span style="color:#89DDFF;">()</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">string</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">error</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">GetPaths</span><span style="color:#89DDFF;">()</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">([]</span><span style="color:#C792EA;">string</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">error</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">GetNumaNode</span><span style="color:#89DDFF;">()</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">bool</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">int</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">error</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><h4 id="device-map-go" tabindex="-1">device_map.go <a class="header-anchor" href="#device-map-go" aria-hidden="true">#</a></h4><p><code>DeviceMap</code> 基于给定的 libnvml、资源名、nvidia 官方 DP 的配置，构建资源名到 HAMI <code>resourceManager</code> 中的设备抽象的映射（device.go 中的 <code>Device</code> 类型）。</p><h4 id="health-go" tabindex="-1">health.go <a class="header-anchor" href="#health-go" aria-hidden="true">#</a></h4><p>检查 GPU 设备的监控状态，允许通过环境变量 <code>DP_DISABLE_HEALTHCHECKS</code> 指定一些可忽略的 xid 错误。目前默认忽略下面的 xid，因为他们只表明用户应用出错了但是设备可能仍然可用。</p><div class="language-go"><button title="Copy Code" class="copy"></button><span class="lang">go</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#676E95;font-style:italic;">// http://docs.nvidia.com/deploy/xid-errors/index.html#topic_4</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">// Application errors: the GPU should still be healthy</span></span>
<span class="line"><span style="color:#A6ACCD;">applicationErrorXids </span><span style="color:#89DDFF;">:=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">[]</span><span style="color:#C792EA;">uint64</span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#F78C6C;">13</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#676E95;font-style:italic;">// Graphics Engine Exception</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#F78C6C;">31</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#676E95;font-style:italic;">// GPU memory page fault</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#F78C6C;">43</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#676E95;font-style:italic;">// GPU stopped processing</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#F78C6C;">45</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#676E95;font-style:italic;">// Preemptive cleanup, due to previous errors</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#F78C6C;">68</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#676E95;font-style:italic;">// Video processor exception</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><h4 id="nvml-devices-go" tabindex="-1">nvml_devices.go <a class="header-anchor" href="#nvml-devices-go" aria-hidden="true">#</a></h4><p><code>nvmlDevice</code> 和 <code>nvmlMigDevice</code> 类型包装了 <a href="http://github.com/NVIDIA/go-nvlib/pkg/nvml" target="_blank" rel="noreferrer">github.com/NVIDIA/go-nvlib/pkg/nvml</a> 的 <code>Device</code>。同样是实现了 <code>deviceInfo</code> 接口。</p><h4 id="nvml-manager-go" tabindex="-1">nvml_manager.go <a class="header-anchor" href="#nvml-manager-go" aria-hidden="true">#</a></h4><p><code>nvmlResourceManager</code> 包装了 <code>resourceManager</code>，集成了 <a href="http://github.com/NVIDIA/go-nvlib/pkg/nvml" target="_blank" rel="noreferrer">github.com/NVIDIA/go-nvlib/pkg/nvml</a> 中的 nvml接口。</p><h4 id="wsl-go" tabindex="-1">wsl.go <a class="header-anchor" href="#wsl-go" aria-hidden="true">#</a></h4><p><code>wslDevice</code> 包装了一层 <code>nvmlDevice</code>。= = 还支持 wsl 的……</p><h4 id="tegra-devices-go、tegra-manager-go" tabindex="-1">tegra_devices.go、tegra_manager.go <a class="header-anchor" href="#tegra-devices-go、tegra-manager-go" aria-hidden="true">#</a></h4><p>Tegra 设备只支持 <code>resourceManager.distributedAlloc()</code> 分配策略。</p><p><code>tegraResourceManager</code> 包装了 <code>resourceManager</code>。</p><p>= = 还支持 tegra 的……</p><h3 id="cdi" tabindex="-1">cdi <a class="header-anchor" href="#cdi" aria-hidden="true">#</a></h3><p>借助官方实现 <code>github.com/NVIDIA/nvidia-container-toolkit/pkg/nvcdi</code>，为 DP 使用的 nvidia 设备创建 CDI specs</p><h4 id="api-go-2" tabindex="-1">api.go <a class="header-anchor" href="#api-go-2" aria-hidden="true">#</a></h4><p>定义实现 CDI 的接口</p><div class="language-go"><button title="Copy Code" class="copy"></button><span class="lang">go</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;">type</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">Interface</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">interface</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">CreateSpecFile</span><span style="color:#89DDFF;">()</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">error</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">QualifiedName</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">string</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">string</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">string</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><h4 id="factory-go-1" tabindex="-1">factory.go <a class="header-anchor" href="#factory-go-1" aria-hidden="true">#</a></h4><p>CDI <code>Interface</code> 的工厂函数，如果没有检测到 nvidia 设备，就创建一个空实现，也就无法生成 CDI specs。</p><h4 id="cdi-go" tabindex="-1">cdi.go <a class="header-anchor" href="#cdi-go" aria-hidden="true">#</a></h4><p>定义一个 <code>cdiHandler</code> 类型用于实现生成 CDI 的接口 <code>Interface</code></p><div class="language-go"><button title="Copy Code" class="copy"></button><span class="lang">go</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;">type</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">cdiHandler</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">struct</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    logger           </span><span style="color:#89DDFF;">*</span><span style="color:#A6ACCD;">logrus</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">Logger</span></span>
<span class="line"><span style="color:#A6ACCD;">    nvml             nvml</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">Interface </span><span style="color:#676E95;font-style:italic;">// github.com/NVIDIA/go-nvlib/pkg/nvml</span></span>
<span class="line"><span style="color:#A6ACCD;">    nvdevice         nvdevice</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">Interface </span><span style="color:#676E95;font-style:italic;">// github.com/NVIDIA/go-nvlib/pkg/nvlib/device</span></span>
<span class="line"><span style="color:#A6ACCD;">    driverRoot       </span><span style="color:#C792EA;">string</span></span>
<span class="line"><span style="color:#A6ACCD;">    targetDriverRoot </span><span style="color:#C792EA;">string</span></span>
<span class="line"><span style="color:#A6ACCD;">    nvidiaCTKPath    </span><span style="color:#C792EA;">string</span></span>
<span class="line"><span style="color:#A6ACCD;">    cdiRoot          </span><span style="color:#C792EA;">string</span></span>
<span class="line"><span style="color:#A6ACCD;">    vendor           </span><span style="color:#C792EA;">string</span></span>
<span class="line"><span style="color:#A6ACCD;">    deviceIDStrategy </span><span style="color:#C792EA;">string</span></span>
<span class="line"><span style="color:#A6ACCD;">    enabled      </span><span style="color:#C792EA;">bool</span></span>
<span class="line"><span style="color:#A6ACCD;">    gdsEnabled   </span><span style="color:#C792EA;">bool</span><span style="color:#A6ACCD;">  </span><span style="color:#676E95;font-style:italic;">// GPUDirect Storage</span></span>
<span class="line"><span style="color:#A6ACCD;">    mofedEnabled </span><span style="color:#C792EA;">bool</span><span style="color:#A6ACCD;"> </span><span style="color:#676E95;font-style:italic;">// Mellanox OpenFabrics Enterprise Distribution</span></span>
<span class="line"><span style="color:#A6ACCD;">    cdilibs </span><span style="color:#89DDFF;">map[</span><span style="color:#C792EA;">string</span><span style="color:#89DDFF;">]</span><span style="color:#A6ACCD;">nvcdi</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">Interface </span><span style="color:#676E95;font-style:italic;">// github.com/NVIDIA/nvidia-container-toolkit/pkg/nvcdi</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><h3 id="mig-mig-go" tabindex="-1">mig/mig.go <a class="header-anchor" href="#mig-mig-go" aria-hidden="true">#</a></h3><p><code>GetMigCapabilityDevicePaths</code> 获取 nvidia MIG 切分模式下的设备文件。</p>`,184),c=[l];function p(r,t,i,d,y,D){return a(),e("div",null,c)}const C=s(o,[["render",p]]);export{h as __pageData,C as default};
