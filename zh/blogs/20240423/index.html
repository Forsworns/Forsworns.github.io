<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Let's build the GPT Tokenizer 笔记 | Sharlayan</title>
    <meta name="description" content="学习了一下 Andrej Karpathy 大神的 Tokenizer 视频课程">
    <link rel="preload stylesheet" href="/assets/style.3ef9b918.css" as="style">
    <link rel="modulepreload" href="/assets/chunks/VPAlgoliaSearchBox.df3ef109.js">
    <link rel="modulepreload" href="/assets/app.0f5a0ae1.js">
    <link rel="modulepreload" href="/assets/zh_blogs_20240423_index.md.e39dc142.lean.js">
    
    <script src="https://cdn.jsdelivr.net/gh/stevenjoezhang/live2d-widget@latest/autoload.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/4.0.0/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.staticfile.org/ionicons/2.0.1/css/ionicons.min.css">
  <link rel="icon" type="image/png" href="/logo.png">
  <meta name="author" content="Peihao Yang">
  <meta property="og:title" content="Home">
  <meta property="og:description" content="Home of Peihao Yang">
  <script id="check-dark-light">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
  </head>
  <body>
    <div id="app"><!--[--><div class="Layout" data-v-b617430f><!--[--><!--]--><!--[--><span tabindex="-1" data-v-d4120332></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-d4120332> Skip to content </a><!--]--><!----><header class="VPNav no-sidebar" data-v-b617430f data-v-aa1cde23><div class="VPNavBar" data-v-aa1cde23 data-v-cbdd8588><div class="container" data-v-cbdd8588><div class="title" data-v-cbdd8588><div class="VPNavBarTitle" data-v-cbdd8588 data-v-730d6dd1><a class="title" href="/zh/" data-v-730d6dd1><!--[--><!--]--><!--[--><img class="VPImage logo" src="/assets/logo.png" alt data-v-0f13a436><!--]--><!----><!--[--><!--]--></a></div></div><div class="content" data-v-cbdd8588><div class="curtain" data-v-cbdd8588></div><!--[--><!--]--><div class="VPNavBarSearch search" data-v-cbdd8588 style="--699c4559:&#39;Meta&#39;;"><div id="docsearch"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg class="DocSearch-Search-Icon" width="20" height="20" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-cbdd8588 data-v-2d3a777e><span id="main-nav-aria-label" class="visually-hidden" data-v-2d3a777e>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/" data-v-2d3a777e data-v-f559a019 data-v-1a0f9836><!--[-->🏡主页<!--]--><!----></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/about-me/" data-v-2d3a777e data-v-f559a019 data-v-1a0f9836><!--[-->🦹‍♂️关于我<!--]--><!----></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-2d3a777e data-v-fc33d832><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-fc33d832><span class="text" data-v-fc33d832><!----> 📓博客 <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-fc33d832><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-fc33d832><div class="VPMenu" data-v-fc33d832 data-v-ecf4e7d9><div class="items" data-v-ecf4e7d9><!--[--><!--[--><div class="VPMenuLink" data-v-ecf4e7d9 data-v-c7da634f><a class="VPLink link" href="/zh/blogs/" data-v-c7da634f data-v-1a0f9836><!--[-->📃所有博客<!--]--><!----></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-ecf4e7d9 data-v-c7da634f><a class="VPLink link" href="/zh/blogs/tags/" data-v-c7da634f data-v-1a0f9836><!--[-->🔖标签分类<!--]--><!----></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="https://forsworns.github.io/feed.xml" target="_blank" rel="noreferrer" data-v-2d3a777e data-v-f559a019 data-v-1a0f9836><!--[-->🔥RSS<!--]--><!----></a><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-cbdd8588 data-v-7cdc304e data-v-fc33d832><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-fc33d832><span class="text" data-v-fc33d832><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="option-icon" data-v-fc33d832><path d="M0 0h24v24H0z" fill="none"></path><path d=" M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z " class="css-c4d79v"></path></svg>  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-fc33d832><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-fc33d832><div class="VPMenu" data-v-fc33d832 data-v-ecf4e7d9><!----><!--[--><!--[--><div class="items" data-v-7cdc304e><p class="title" data-v-7cdc304e>中文</p><!--[--><div class="VPMenuLink" data-v-7cdc304e data-v-c7da634f><a class="VPLink link" href="/" data-v-c7da634f data-v-1a0f9836><!--[-->English<!--]--><!----></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-cbdd8588 data-v-7f24c201><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-7f24c201 data-v-2b897f09 data-v-ab7cdc04><span class="check" data-v-ab7cdc04><span class="icon" data-v-ab7cdc04><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-2b897f09><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-2b897f09><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-cbdd8588 data-v-a5afa74d data-v-80c99471><!--[--><a class="VPSocialLink" href="https://github.com/forsworns/blog-vitepress" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><a class="VPSocialLink" href="mailto:peihao.young@gmail.com" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="20">
            <path d="M874.666667 375.189333V746.666667a64 64 0 0 1-64 64H213.333333a64 64 0 0 1-64-64V375.189333l266.090667 225.6a149.333333 149.333333 0 0 0 193.152 0L874.666667 375.189333zM810.666667 213.333333a64.789333 64.789333 0 0 1 22.826666 4.181334 63.616 63.616 0 0 1 26.794667 19.413333 64.32 64.32 0 0 1 9.344 15.466667c2.773333 6.570667 4.48 13.696 4.906667 21.184L874.666667 277.333333v21.333334L553.536 572.586667a64 64 0 0 1-79.893333 2.538666l-3.178667-2.56L149.333333 298.666667v-21.333334a63.786667 63.786667 0 0 1 35.136-57.130666A63.872 63.872 0 0 1 213.333333 213.333333h597.333334z" ></path>
            </svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-cbdd8588 data-v-e459e5dc data-v-fc33d832><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-fc33d832><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-fc33d832><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-fc33d832><div class="VPMenu" data-v-fc33d832 data-v-ecf4e7d9><!----><!--[--><!--[--><div class="group" data-v-e459e5dc><p class="trans-title" data-v-e459e5dc>中文</p><!--[--><div class="VPMenuLink" data-v-e459e5dc data-v-c7da634f><a class="VPLink link" href="/" data-v-c7da634f data-v-1a0f9836><!--[-->English<!--]--><!----></a></div><!--]--></div><div class="group" data-v-e459e5dc><div class="item appearance" data-v-e459e5dc><p class="label" data-v-e459e5dc>Appearance</p><div class="appearance-action" data-v-e459e5dc><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-e459e5dc data-v-2b897f09 data-v-ab7cdc04><span class="check" data-v-ab7cdc04><span class="icon" data-v-ab7cdc04><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-2b897f09><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-2b897f09><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="group" data-v-e459e5dc><div class="item social-links" data-v-e459e5dc><div class="VPSocialLinks social-links-list" data-v-e459e5dc data-v-80c99471><!--[--><a class="VPSocialLink" href="https://github.com/forsworns/blog-vitepress" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><a class="VPSocialLink" href="mailto:peihao.young@gmail.com" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="20">
            <path d="M874.666667 375.189333V746.666667a64 64 0 0 1-64 64H213.333333a64 64 0 0 1-64-64V375.189333l266.090667 225.6a149.333333 149.333333 0 0 0 193.152 0L874.666667 375.189333zM810.666667 213.333333a64.789333 64.789333 0 0 1 22.826666 4.181334 63.616 63.616 0 0 1 26.794667 19.413333 64.32 64.32 0 0 1 9.344 15.466667c2.773333 6.570667 4.48 13.696 4.906667 21.184L874.666667 277.333333v21.333334L553.536 572.586667a64 64 0 0 1-79.893333 2.538666l-3.178667-2.56L149.333333 298.666667v-21.333334a63.786667 63.786667 0 0 1 35.136-57.130666A63.872 63.872 0 0 1 213.333333 213.333333h597.333334z" ></path>
            </svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-cbdd8588 data-v-d6834c14><span class="container" data-v-d6834c14><span class="top" data-v-d6834c14></span><span class="middle" data-v-d6834c14></span><span class="bottom" data-v-d6834c14></span></span></button></div></div></div><!----></header><!----><!----><div class="VPContent" id="VPContent" data-v-b617430f data-v-f32377af><div class="VPDoc has-aside" data-v-f32377af data-v-1e970af1><div class="container" data-v-1e970af1><div class="aside" data-v-1e970af1><div class="aside-curtain" data-v-1e970af1></div><div class="aside-container" data-v-1e970af1><div class="aside-content" data-v-1e970af1><div class="VPDocAside" data-v-1e970af1 data-v-b1723386><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" data-v-b1723386 data-v-0980ba1d><div class="content" data-v-0980ba1d><div class="outline-marker" data-v-0980ba1d></div><div class="outline-title" data-v-0980ba1d>On this page</div><nav aria-labelledby="doc-outline-aria-label" data-v-0980ba1d><span class="visually-hidden" id="doc-outline-aria-label" data-v-0980ba1d> Table of Contents for current page </span><ul class="root" data-v-0980ba1d data-v-6f4caaf4><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-b1723386></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-1e970af1><div class="content-container" data-v-1e970af1><!--[--><!--]--><main class="main" data-v-1e970af1><div style="position:relative;" class="vp-doc _zh_blogs_20240423_index" data-v-1e970af1><div><nav class="table-of-contents"><ul></ul></nav><p>前段时间 Andrej Karpathy 大神发了视频手把手带着大家了解 LLM 使用的 Tokenizer，记录一下。</p><p><a href="https://github.com/karpathy/minbpe" target="_blank" rel="noreferrer">教学仓库地址</a></p><p><a href="https://www.youtube.com/watch?v=zduSFxRajkE4" target="_blank" rel="noreferrer">视频地址</a></p><p>视频中演示的应用，对不同的 tokenizer 的分词结果做了<a href="https://tiktokenizer.vercel.app" target="_blank" rel="noreferrer">可视化</a></p><h1 id="引入" tabindex="-1">引入 <a class="header-anchor" href="#引入" aria-hidden="true">#</a></h1><p>首先来看几个问题，实际上它们都和 tokenizer 有关：</p><ul><li>为什么 LLM 不能拼写单词</li><li>为什么 LLM 不能做一些很简单的字符串操作，比如反转字符串</li><li>为什么 LLM 在非英语任务上表现差</li><li>为什么在简单数学任务上 LLM 的表现差</li><li>为什么 GPT-2 在编写 Python 代码方面有困难</li><li>为什么 LLM 看到 <code>&lt;end-of-text&gt;</code> 就终止了</li><li>Open AI 的 &quot;Trailing space&quot; 提示是什么用意</li><li>为什么 LLM 看到 &quot;SolidGoldMagikarp&quot; 就出错了</li><li>为什么在使用 LLM 时，我应该使用 YAML 而不是 JSON</li></ul><p>接着 Andrej 在演示应用中测试了几个不同的 Tokenizer 的效果，发现 GPT-2 在处理 python 代码时，倾向于将每个空格划分为独立的 token，导致缩进中出现了大量冗余 token；而 GPT-4 使用的 cl100k_base 在处理 python 代码时，对缩进的处理更加智能。同时，GPT-4 划分出的 token 数量远小于 GPT-2。当然 token 也不是越少越好，相当于要找到一个平衡点，信息足够密集但是又可以被划分开。</p><h1 id="基础的字节对编码" tabindex="-1">基础的字节对编码 <a class="header-anchor" href="#基础的字节对编码" aria-hidden="true">#</a></h1><p>python 中字符串以 unicode 编码，借助 <code>ord</code> 方法可以查看单个字符的码点。能不能简单地使用这些数字作为输入呢？ 因为这会导致我们模型的词汇量特别大，unicode 有 15 万码点，并且还在活跃地扩充中。（以 transformer 为例，导致最终用于预测的 softmax 层节点过多，要在 15 万个可能中预测一个；过大的 token 数，也意味着 embedding 要足够大，导致 transformer 模型本身的规模也变大了，上下文也会显著增加），因此我们要找到一个好的编码。</p><p>unicode 下的三种编码类型，UTF-8, UTF-16, UTF-32，将 unicode 转换为二进制字节流。我们更偏向于使用 UTF-8，因为后两者都会让我们的字节流中出现大量冗余的 0。直接拿字节流显然也是不好的，单个字节 256 个可能，完全是在预测字节序列。而且所有的输入文本都会被拉伸为很长的字节流表示，transformer 的注意力 context 会被浪费掉，特别是很多模型的 context 是很有限的。Andrej 在这里实际上展示了一篇论文，MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers。</p><p>于是出现了字节对编码算法，Byte Pair Encoding（BPE），Andrej 再次引用了维基百科来介绍它。实际上是一种无损压缩算法，每一轮会将当前数据转中最常见的一对编码，替换为一个新的编码，写入到字典中，循环这个过程直到达到设定的词汇量上限。（和 LZ77 等无损压缩算法不同，BPE 保证了编码后还是之前的编码数据类型，并保留了相邻 token 的相邻关系。）在大量的文本上完成训练后，就可以得到一个通用的编码字典，这里使用的训练文本和 LLM 的训练是独立的，可能不一致。训练集中的不同语言的密度，将会影响学习到的字典对它是否进行有效的压缩，决定了它最终在 token 空间中的密度。如果训练集中有更多中文，那显然更多中文的用词习惯将会被学习到，编码后输入给 LLM 的中文序列也会更短。</p><p>Andrej 在这里手写了一个基础的的 BPE encoder/decoder 用作 tokenizer。</p><h1 id="openai-更为先进的-tokenizer" tabindex="-1">OpenAI 更为先进的 tokenizer <a class="header-anchor" href="#openai-更为先进的-tokenizer" aria-hidden="true">#</a></h1><p>GPT-2 的论文，Language Models are Unsurpervised Multitask Learners，介绍了基础的 BPE 算法面临的问题：如果文本中包含了词语 dog，它在文章中以 &quot;dog,&quot;，&quot;dog!&quot; 等形式大量出现，那 BPE 显然会讲这些相邻字符合并到字典中进行压缩，显然这很低效。于是他们使用了正则表达式，添加了人工先验。</p><p>在 OpenAI 的 github 上查看 GPT-2 的代码仓库，会发现一个 <a href="https://github.com/openai/gpt-2/blob/master/src/encoder.py" target="_blank" rel="noreferrer"><code>gpt-2/src/encoder.py</code></a>，里面有他们使用的正则：</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">regex</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">compile</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">r</span><span style="color:#89DDFF;">&quot;&quot;&quot;</span><span style="color:#C3E88D;">&#39;s</span><span style="color:#89DDFF;">|</span><span style="color:#C3E88D;">&#39;t</span><span style="color:#89DDFF;">|</span><span style="color:#C3E88D;">&#39;re</span><span style="color:#89DDFF;">|</span><span style="color:#C3E88D;">&#39;ve</span><span style="color:#89DDFF;">|</span><span style="color:#C3E88D;">&#39;m</span><span style="color:#89DDFF;">|</span><span style="color:#C3E88D;">&#39;ll</span><span style="color:#89DDFF;">|</span><span style="color:#C3E88D;">&#39;d</span><span style="color:#89DDFF;">|</span><span style="color:#C3E88D;"> </span><span style="color:#89DDFF;">?</span><span style="color:#A6ACCD;">\p</span><span style="color:#C3E88D;">{L}</span><span style="color:#89DDFF;">+|</span><span style="color:#C3E88D;"> </span><span style="color:#89DDFF;">?</span><span style="color:#A6ACCD;">\p</span><span style="color:#C3E88D;">{N}</span><span style="color:#89DDFF;">+|</span><span style="color:#C3E88D;"> </span><span style="color:#89DDFF;">?[^</span><span style="color:#C3E88D;">\s</span><span style="color:#A6ACCD;">\p</span><span style="color:#C3E88D;">{L}</span><span style="color:#A6ACCD;">\p</span><span style="color:#C3E88D;">{N}</span><span style="color:#89DDFF;">]+|</span><span style="color:#C3E88D;">\s</span><span style="color:#89DDFF;">+(?!</span><span style="color:#C3E88D;">\S</span><span style="color:#89DDFF;">)|</span><span style="color:#C3E88D;">\s</span><span style="color:#89DDFF;">+&quot;&quot;&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><p>这里主要做的就是把字符串拆分开，数字和字母拆分开，保留 <code>&#39;s</code>,<code>&#39;ve</code> 这样的 token（源码上有段注释，表明了这里实际上有缺陷，即忽略了大写的情况，只匹配了小写，我们在后面将会看到 cl100k 修复了它）。值得注意的是，它还会保留单词前的单个空格。比如 <code>----you</code> 将会被分割成 <code>---</code> 和 <code>-you</code>，这里 <code>-</code> 代表空格。</p><p>遗憾的是 GPT-2 开源的只是推理代码，而非训练代码，因此我们没法给定文本训练一个自己的 tokenizer。</p><p>OpenAI 有另一个公开的 tokenizer 库，<a href="https://github.com/openai/tiktoken" target="_blank" rel="noreferrer">tiktoken</a> 库，它是由 rust 写的！但是同样它只是一个推理库，无法进行训练。</p><p>GPT-4 的 cl100k 中使用的正则表达式可以在 tiktoken 的<a href="https://github.com/openai/tiktoken/blob/1b9faf2779855124f05174adf1383e53689ed94b/tiktoken_ext/openai_public.py#L85" target="_blank" rel="noreferrer">源码中找到</a>，</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;font-style:italic;">r</span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span><span style="color:#676E95;font-style:italic;">&#39;(?i:[sdmt]|ll|ve|re)|[^\r\n\p{L}\p{N}]?+\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+</span><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span></code></pre></div><p>这个新正则表达式中，值得关注的点除了 GPT-2 中的大小写问题被修复了，还有就是现在只会匹配长度为 1-3 的数字，不会合并超过三位数字，以防 token 中出现很长的数字。</p><p>回到 GPT-2 的 <a href="http://encoder.py" target="_blank" rel="noreferrer">encoder.py</a>，会发现 OpenAI 的 encoder、decoder 确实是类似 Andrej 前面手写的 BPE tokenizer。 但是这里奇怪的一点是他们同时还有一对 byte encoder 和 byte decoder。用于 BPE encoder/decoder 前后。</p><h1 id="特殊-token" tabindex="-1">特殊 token <a class="header-anchor" href="#特殊-token" aria-hidden="true">#</a></h1><p>除了 BPE 算法中组合出的 token，我们也可以加入一些人工设置的特殊 token，区分数据的不同部分。</p><p>例如，GPT-2 的 token 字典的规模是 50257，这个规模是怎么来的呢？单个字节是 256，然后它们在训练时做了 50000 次合并，最后剩余一个是 <code>&#39;&lt;|endoftext|&gt;&#39;</code>。打印下字典，也会发现事实如此。类似的，微调过的对话模型 gpt-3.5-turbo 使用了 <code>&lt;im_start&gt;</code>、<code>&lt;im_end&gt;</code> 用作特殊的 token 标记对话的开始和结束。</p><p>tiktoken 也允许我们加载某个基础的 tokenizer，然后创建自己的特殊 token 去拓展它。</p><h1 id="sentencepiece" tabindex="-1">sentencepiece <a class="header-anchor" href="#sentencepiece" aria-hidden="true">#</a></h1><p><a href="https://github.com/google/sentencepiece" target="_blank" rel="noreferrer">sentencepiece</a> 是一个常用的 tokenizer 库，它不仅可用于推理，还允许你训练自己的 tokenizer，Llama 和 Mistral 系列的模型都是用了它。</p><p>它和 Tiktoken 不同。Tiktoken 会首先获取 unicode 码点，然后用 UTF-8 编码成为字节流，再运行 BPE 合并字节。而 sentencepiece 会直接在 unicode 码点的层级上运转。因此会有一些罕见的码点不会经常出现，可以用超参数 <code>character_coverage</code> 来控制，稀有码点会被直接转换成 <code>UNK</code> token。如果开启了 <code>byte_fallback</code> 选项，则会编码到 utf-8 并对这些字节进行编码。</p><p>sentencepiece 有很多历史包袱，可配置的参数特别多，Andrej 从 Llama-2 的配置中抄了一段过来。值得注意的是里面的 normalization 相关的选项完全可以关掉，这是之前 nlp 基础任务做正则化用的，但是显然 LLM 可以接收原始的字符串。另外，sentencepiece 有 sentence 的概念，这是因为当年曾有一种想法是在一堆独立的句子上训练 tokenizer，但是现在大家都在完整的文章上训练了，而且定义到底什么是一个句子，是很困难的。</p><h1 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-hidden="true">#</a></h1><p>回到最初的几个问题：</p><ul><li>为什么 LLM 不能拼写单词</li></ul><p>因为字符被组合成了 tokens，其中一些 token 很长，因此模型对字符的理解会较弱。在视频中 Andrej 让 ChatGPT 去数一个词中的特定的字符个数，得到了错误的答案。我在 Poe 上用不同模型进行了尝试，如下图，只有 Llama-3 答对了。</p><p><img src="/assets/char-count-challenge.6353bcee.png" alt="char-count-challenge"></p><ul><li>为什么 LLM 不能做一些很简单的字符串操作，比如反转字符串</li></ul><p>同样由于缺乏对字符串的理解能力，LLM 也不擅长做这件事。给它一个算法，然后让它模拟倒是可行。</p><ul><li>为什么 LLM 在非英语任务上表现差</li></ul><p>LLM 训练时候小语种数据不足，tokenizer 本身也没有经过良好训练。</p><ul><li>为什么在简单数学任务上 LLM 的表现差</li></ul><p>加法类似一种字符级别的算法，但是基于 BPE，数字会被合并成 token，导致难以处理。视频中推荐阅读博客 <a href="https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/" target="_blank" rel="noreferrer">Integer tokenization is insane</a>。</p><ul><li>为什么 GPT-2 在编写 Python 代码方面有困难</li></ul><p>如全文所述，GPT-2 处理 python 的缩进时，把每个空格都当做了一个 token，这也影响到了上下文和 attention 的计算。在 GPT-4 中进行了改进。</p><ul><li>为什么 LLM 看到 <code>&lt;end-of-text&gt;</code> 就终止了</li></ul><p>因为在 tokenizer 中使用它作为了特殊标记，可能是在训练时用来分割语料。特殊标记可能成为 LLM 的攻击点。</p><ul><li>OpenAI 的 &quot;Trailing space&quot; 提示是什么用意</li></ul><p>使用 OpenAI 的生成模型时，输入 Prompt 让它填充，例如 &quot;Here is a tag lien for the ice cream shop:&quot;。但是如果换用&quot;Here is a tag lien for the ice cream shop: &quot;，即句尾多加一个空格，就会收到 &quot;Trailing space&quot; 警告。</p><p>这也是前文提过的，OpenAI 的 tokenizer 使用的正则，会保留单词前的单个空格，在 BPE 中会被合并到 token 中。sentencepiece 中甚至有一个选项可以在句子中添加一个空格前缀，以防止这种现象出现。</p><p>多加了一个空格，就导致当前的 prompt 在转换成 token 后，出现了一个远离正常的 token 分布的奇异 token，会影响模型预测出的概率分布。这些 LLM 是构建于 token 之上的，而非我们常规认知里的的字符。</p><ul><li>为什么 LLM 看到 &quot;SolidGoldMagikarp&quot; 就会输出一些混乱的结果</li></ul><p>视频中推荐了文章 <a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation" target="_blank" rel="noreferrer">SolidGoldMagikarp (plus, prompt generation)</a>。文中对 token 的嵌入做了聚类，然后发现了一些奇怪的 token 被聚集在了一起。这些 token 是从哪里来的，字面上看，它们毫无意义。有趣的是，如果你向模型输入包含这些 token 的 prompt，会得到混乱的回复。</p><p>深入调查后，发现这个 SolidGoldMagikarp 其实是一名 reddit 用户。于是，可能是因为训练 tokenizer 的数据和训练语言模型的数据不同，而碰巧 tokenizer 的训练数据中包含了大量来自 reddit 的文本，然后 SolidGoldMagikarp 他发了很多帖子，或者被多次回复引用，然后被 BPE 算法学到了，合并成了一个 token。但是当训练语言模型的时候，并没有那部分 reddit 数据，于是这个 token 就不会被激活，它会被随机初始化，但是永远不会被采样到，也不会被更新，有点像未分配的内存。然后在推理的时候，你触发到了这个 token，那么就是采样到了未经初始化的嵌入，导致了未定义的行为。</p><p>无端联想：前两天看了个很乐的文章，百度贴吧弱智吧的语料是很好的训练来源哈哈</p><ul><li>为什么在使用 LLM 时，我应该使用 YAML 而不是 JSON</li></ul><p>YAML 这种结构化文本在 GPT-4 上会消耗更少的 token，更加高效，更加经济。</p></div></div></main><!--[--><!--]--><!----><!--[--><!--[--><!--[--><!----><!--]--><!--]--><!--]--></div></div></div></div></div><!----><!--[--><!--]--></div><!----><footer data-v-4f0db67d> Powered by <a href="https://github.com/forsworns/" target="_blank" title="Author" data-v-4f0db67d>Peihao Yang</a> | Copyright © 2019-2024 | MIT License </footer><!--]--></div>
    <script>__VP_HASH_MAP__ = JSON.parse("{\"about-me_index.md\":\"ae155f90\",\"zh_blogs_20190721_index.md\":\"4f1bd34e\",\"index.md\":\"431bcd57\",\"zh_about-me_index.md\":\"cb485e9f\",\"zh_blogs_20190901_index.md\":\"0f58967a\",\"zh_blogs_20190824_index.md\":\"c9e31211\",\"zh_blogs_20190919_index.md\":\"660a84aa\",\"zh_blogs_20190908_index.md\":\"b837c695\",\"zh_blogs_20191109_index.md\":\"f3f202fd\",\"zh_blogs_20191112_index.md\":\"20a903ef\",\"zh_blogs_20210204_index.md\":\"97569ac5\",\"zh_blogs_20201023_index.md\":\"848aa516\",\"zh_blogs_20210120_index.md\":\"87b674e6\",\"zh_blogs_20210203_index.md\":\"6662f4c7\",\"zh_blogs_20210123_index.md\":\"1fe092d1\",\"zh_blogs_20200816_index.md\":\"f570af2a\",\"zh_blogs_20200818_index.md\":\"e18d8843\",\"zh_blogs_20200616_index.md\":\"1909c34c\",\"zh_blogs_20200817_index.md\":\"e7f1158c\",\"zh_blogs_20191102_index.md\":\"e1722464\",\"zh_blogs_20191103_index.md\":\"9b710856\",\"zh_blogs_20210310_index.md\":\"bb00b880\",\"zh_blogs_20210226_index.md\":\"043359b1\",\"zh_blogs_20210311_index.md\":\"2bdad88d\",\"zh_blogs_20210223_index.md\":\"a04bbf06\",\"zh_blogs_20210224_index.md\":\"8bf1d3c9\",\"zh_blogs_20210430_index.md\":\"7d887266\",\"zh_blogs_20210506_index.md\":\"9a4cc383\",\"zh_blogs_20230125_index.md\":\"0e4ab5d9\",\"zh_blogs_20230126_index.md\":\"961f0979\",\"zh_blogs_20230121_index.md\":\"1e80c99d\",\"zh_blogs_20230201_repost.md\":\"16684978\",\"zh_blogs_20230322_index.md\":\"8ba4df8b\",\"zh_blogs_20230209_index.md\":\"d6571f6a\",\"zh_blogs_20230201_index.md\":\"5a07f4b5\",\"zh_blogs_20230601_index.md\":\"cbd6f55d\",\"zh_blogs_20240220_index.md\":\"6f23a2ab\",\"zh_blogs_20240413_index.md\":\"6cfc1faa\",\"zh_blogs_20240427_index.md\":\"92f8531f\",\"zh_blogs_20240423_index.md\":\"e39dc142\",\"zh_blogs_20240513_index.md\":\"5054ded0\",\"zh_blogs_20210706_index.md\":\"e46538a1\",\"zh_blogs_20210312_index.md\":\"a97c91e4\",\"zh_blogs_20240215_index.md\":\"29b27448\",\"zh_blogs_20240626_index.md\":\"6d490338\",\"zh_blogs_20240909_index.md\":\"9e9ac428\",\"zh_blogs_20240924_index.md\":\"c16775d2\",\"zh_blogs_20240916_index.md\":\"34a833d6\",\"zh_blogs_20240526_index.md\":\"5cc8945c\",\"zh_blogs_index.md\":\"284bf08e\",\"zh_blogs_20210409_index.md\":\"3374f5af\",\"zh_blogs_20220105_index.md\":\"057ef58c\",\"zh_blogs_20220101_index.md\":\"28fca584\",\"zh_blogs_20211210_index.md\":\"5cf5ec3b\",\"zh_blogs_20220224_index.md\":\"cdd187f4\",\"zh_blogs_20240622_index.md\":\"4921f51d\",\"zh_blogs_20210329_index.md\":\"48cb84e7\",\"zh_blogs_20220316_index.md\":\"09c75bb7\",\"zh_blogs_20241008_index.md\":\"cd84a035\",\"zh_blogs_20240623_index.md\":\"02de25e1\",\"zh_blogs_20221108_index.md\":\"e5b7be76\",\"zh_blogs_20221024_index.md\":\"8ebba006\",\"zh_blogs_20210315_index.md\":\"13799d08\",\"zh_blogs_20240128_index.md\":\"9236e79c\",\"zh_blogs_20230101_index.md\":\"e779c786\",\"zh_blogs_20210728_index.md\":\"e67450f5\",\"zh_blogs_20211120_index.md\":\"8e740225\",\"zh_blogs_20210627_index.md\":\"1767f8a9\",\"zh_blogs_20210822_index.md\":\"6aba4591\",\"zh_blogs_20220611_index.md\":\"48a2162f\",\"zh_blogs_20240519_index.md\":\"917234d5\",\"zh_blogs_20240518_index.md\":\"cd515693\",\"zh_blogs_20210715_index.md\":\"16ce33cc\",\"zh_blogs_20210412_index.md\":\"cb1d48be\",\"zh_blogs_20240620_index.md\":\"cadc4632\",\"zh_index.md\":\"bd0115db\",\"zh_blogs_tags_index.md\":\"08460bbb\",\"zh_blogs_20211130_index.md\":\"e06fc018\",\"zh_blogs_20210801_index.md\":\"640475cd\",\"zh_blogs_20221228_index.md\":\"9dc40de8\",\"zh_blogs_20241020_index.md\":\"8fc01acc\",\"zh_blogs_20211002_index.md\":\"c2dacf15\"}")</script>
    <script type="module" async src="/assets/app.0f5a0ae1.js"></script>
    
  </body>
</html>