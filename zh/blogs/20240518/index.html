<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Pytorch2 Tensor Parallelism | Sharlayan</title>
    <meta name="description" content="Pytorch2 Recap">
    <link rel="preload stylesheet" href="/assets/style.3ef9b918.css" as="style">
    <link rel="modulepreload" href="/assets/chunks/VPAlgoliaSearchBox.b4b38c48.js">
    <link rel="modulepreload" href="/assets/app.7e53e423.js">
    <link rel="modulepreload" href="/assets/zh_blogs_20240518_index.md.dd571dd5.lean.js">
    
    <script src="https://cdn.jsdelivr.net/gh/stevenjoezhang/live2d-widget@latest/autoload.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/4.0.0/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.staticfile.org/ionicons/2.0.1/css/ionicons.min.css">
  <link rel="icon" type="image/png" href="/logo.png">
  <meta name="author" content="Peihao Yang">
  <meta property="og:title" content="Home">
  <meta property="og:description" content="Home of Peihao Yang">
  <script id="check-dark-light">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
  </head>
  <body>
    <div id="app"><!--[--><div class="Layout" data-v-b617430f><!--[--><!--]--><!--[--><span tabindex="-1" data-v-d4120332></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-d4120332> Skip to content </a><!--]--><!----><header class="VPNav no-sidebar" data-v-b617430f data-v-aa1cde23><div class="VPNavBar" data-v-aa1cde23 data-v-cbdd8588><div class="container" data-v-cbdd8588><div class="title" data-v-cbdd8588><div class="VPNavBarTitle" data-v-cbdd8588 data-v-730d6dd1><a class="title" href="/zh/" data-v-730d6dd1><!--[--><!--]--><!--[--><img class="VPImage logo" src="/assets/logo.png" alt data-v-0f13a436><!--]--><!----><!--[--><!--]--></a></div></div><div class="content" data-v-cbdd8588><div class="curtain" data-v-cbdd8588></div><!--[--><!--]--><div class="VPNavBarSearch search" data-v-cbdd8588 style="--699c4559:&#39;Meta&#39;;"><div id="docsearch"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg class="DocSearch-Search-Icon" width="20" height="20" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-cbdd8588 data-v-2d3a777e><span id="main-nav-aria-label" class="visually-hidden" data-v-2d3a777e>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/" data-v-2d3a777e data-v-f559a019 data-v-1a0f9836><!--[-->ğŸ¡ä¸»é¡µ<!--]--><!----></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/about-me/" data-v-2d3a777e data-v-f559a019 data-v-1a0f9836><!--[-->ğŸ¦¹â€â™‚ï¸å…³äºæˆ‘<!--]--><!----></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-2d3a777e data-v-fc33d832><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-fc33d832><span class="text" data-v-fc33d832><!----> ğŸ““åšå®¢ <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-fc33d832><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-fc33d832><div class="VPMenu" data-v-fc33d832 data-v-ecf4e7d9><div class="items" data-v-ecf4e7d9><!--[--><!--[--><div class="VPMenuLink" data-v-ecf4e7d9 data-v-c7da634f><a class="VPLink link" href="/zh/blogs/" data-v-c7da634f data-v-1a0f9836><!--[-->ğŸ“ƒæ‰€æœ‰åšå®¢<!--]--><!----></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-ecf4e7d9 data-v-c7da634f><a class="VPLink link" href="/zh/blogs/tags/" data-v-c7da634f data-v-1a0f9836><!--[-->ğŸ”–æ ‡ç­¾åˆ†ç±»<!--]--><!----></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="https://forsworns.github.io/feed.xml" target="_blank" rel="noreferrer" data-v-2d3a777e data-v-f559a019 data-v-1a0f9836><!--[-->ğŸ”¥RSS<!--]--><!----></a><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-cbdd8588 data-v-7cdc304e data-v-fc33d832><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-fc33d832><span class="text" data-v-fc33d832><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="option-icon" data-v-fc33d832><path d="M0 0h24v24H0z" fill="none"></path><path d=" M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z " class="css-c4d79v"></path></svg>  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-fc33d832><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-fc33d832><div class="VPMenu" data-v-fc33d832 data-v-ecf4e7d9><!----><!--[--><!--[--><div class="items" data-v-7cdc304e><p class="title" data-v-7cdc304e>ä¸­æ–‡</p><!--[--><div class="VPMenuLink" data-v-7cdc304e data-v-c7da634f><a class="VPLink link" href="/" data-v-c7da634f data-v-1a0f9836><!--[-->English<!--]--><!----></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-cbdd8588 data-v-7f24c201><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-7f24c201 data-v-2b897f09 data-v-ab7cdc04><span class="check" data-v-ab7cdc04><span class="icon" data-v-ab7cdc04><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-2b897f09><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-2b897f09><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-cbdd8588 data-v-a5afa74d data-v-80c99471><!--[--><a class="VPSocialLink" href="https://github.com/forsworns/blog-vitepress" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><a class="VPSocialLink" href="mailto:peihao.young@gmail.com" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="20">
            <path d="M874.666667 375.189333V746.666667a64 64 0 0 1-64 64H213.333333a64 64 0 0 1-64-64V375.189333l266.090667 225.6a149.333333 149.333333 0 0 0 193.152 0L874.666667 375.189333zM810.666667 213.333333a64.789333 64.789333 0 0 1 22.826666 4.181334 63.616 63.616 0 0 1 26.794667 19.413333 64.32 64.32 0 0 1 9.344 15.466667c2.773333 6.570667 4.48 13.696 4.906667 21.184L874.666667 277.333333v21.333334L553.536 572.586667a64 64 0 0 1-79.893333 2.538666l-3.178667-2.56L149.333333 298.666667v-21.333334a63.786667 63.786667 0 0 1 35.136-57.130666A63.872 63.872 0 0 1 213.333333 213.333333h597.333334z" ></path>
            </svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-cbdd8588 data-v-e459e5dc data-v-fc33d832><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-fc33d832><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-fc33d832><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-fc33d832><div class="VPMenu" data-v-fc33d832 data-v-ecf4e7d9><!----><!--[--><!--[--><div class="group" data-v-e459e5dc><p class="trans-title" data-v-e459e5dc>ä¸­æ–‡</p><!--[--><div class="VPMenuLink" data-v-e459e5dc data-v-c7da634f><a class="VPLink link" href="/" data-v-c7da634f data-v-1a0f9836><!--[-->English<!--]--><!----></a></div><!--]--></div><div class="group" data-v-e459e5dc><div class="item appearance" data-v-e459e5dc><p class="label" data-v-e459e5dc>Appearance</p><div class="appearance-action" data-v-e459e5dc><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-e459e5dc data-v-2b897f09 data-v-ab7cdc04><span class="check" data-v-ab7cdc04><span class="icon" data-v-ab7cdc04><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-2b897f09><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-2b897f09><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="group" data-v-e459e5dc><div class="item social-links" data-v-e459e5dc><div class="VPSocialLinks social-links-list" data-v-e459e5dc data-v-80c99471><!--[--><a class="VPSocialLink" href="https://github.com/forsworns/blog-vitepress" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><a class="VPSocialLink" href="mailto:peihao.young@gmail.com" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="20">
            <path d="M874.666667 375.189333V746.666667a64 64 0 0 1-64 64H213.333333a64 64 0 0 1-64-64V375.189333l266.090667 225.6a149.333333 149.333333 0 0 0 193.152 0L874.666667 375.189333zM810.666667 213.333333a64.789333 64.789333 0 0 1 22.826666 4.181334 63.616 63.616 0 0 1 26.794667 19.413333 64.32 64.32 0 0 1 9.344 15.466667c2.773333 6.570667 4.48 13.696 4.906667 21.184L874.666667 277.333333v21.333334L553.536 572.586667a64 64 0 0 1-79.893333 2.538666l-3.178667-2.56L149.333333 298.666667v-21.333334a63.786667 63.786667 0 0 1 35.136-57.130666A63.872 63.872 0 0 1 213.333333 213.333333h597.333334z" ></path>
            </svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-cbdd8588 data-v-d6834c14><span class="container" data-v-d6834c14><span class="top" data-v-d6834c14></span><span class="middle" data-v-d6834c14></span><span class="bottom" data-v-d6834c14></span></span></button></div></div></div><!----></header><!----><!----><div class="VPContent" id="VPContent" data-v-b617430f data-v-f32377af><div class="VPDoc has-aside" data-v-f32377af data-v-1e970af1><div class="container" data-v-1e970af1><div class="aside" data-v-1e970af1><div class="aside-curtain" data-v-1e970af1></div><div class="aside-container" data-v-1e970af1><div class="aside-content" data-v-1e970af1><div class="VPDocAside" data-v-1e970af1 data-v-b1723386><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" data-v-b1723386 data-v-0980ba1d><div class="content" data-v-0980ba1d><div class="outline-marker" data-v-0980ba1d></div><div class="outline-title" data-v-0980ba1d>On this page</div><nav aria-labelledby="doc-outline-aria-label" data-v-0980ba1d><span class="visually-hidden" id="doc-outline-aria-label" data-v-0980ba1d> Table of Contents for current page </span><ul class="root" data-v-0980ba1d data-v-6f4caaf4><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-b1723386></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-1e970af1><div class="content-container" data-v-1e970af1><!--[--><!--]--><main class="main" data-v-1e970af1><div style="position:relative;" class="vp-doc _zh_blogs_20240518_index" data-v-1e970af1><div><nav class="table-of-contents"><ul><li><a href="#pytorch-å¼ é‡å¹¶è¡Œ-tutorial">Pytorch å¼ é‡å¹¶è¡Œ Tutorial</a><ul><li><a href="#tp-æ˜¯æ€ä¹ˆèµ·æ•ˆçš„">TP æ˜¯æ€ä¹ˆèµ·æ•ˆçš„</a></li><li><a href="#ä»€ä¹ˆæ—¶å€™åº”è¯¥ä½¿ç”¨-tp">ä»€ä¹ˆæ—¶å€™åº”è¯¥ä½¿ç”¨ TP</a></li><li><a href="#å¦‚ä½•å®æ–½-tp">å¦‚ä½•å®æ–½ TP</a></li><li><a href="#sp-ä¾‹å­">SP ä¾‹å­</a></li><li><a href="#lp">LP</a></li><li><a href="#tp-å’Œ-fsdp-ç»“åˆ">TP å’Œ FSDP ç»“åˆ</a></li></ul></li><li><a href="#å¼ é‡å¹¶è¡Œ-api">å¼ é‡å¹¶è¡Œ API</a></li><li><a href="#device-mesh">Device Mesh</a><ul><li><a href="#æ¡ˆä¾‹">æ¡ˆä¾‹</a></li></ul></li><li><a href="#dtensor">DTensor</a></li></ul></nav><h2 id="pytorch-å¼ é‡å¹¶è¡Œ-tutorial" tabindex="-1">Pytorch å¼ é‡å¹¶è¡Œ Tutorial <a class="header-anchor" href="#pytorch-å¼ é‡å¹¶è¡Œ-tutorial" aria-hidden="true">#</a></h2><p><a href="https://pytorch.org/tutorials/intermediate/TP_tutorial.html" target="_blank" rel="noreferrer">https://pytorch.org/tutorials/intermediate/TP_tutorial.html</a></p><p>è¯¥æ–‡æ¡£ä»‹ç»çš„æ˜¯å¦‚ä½•åº”ç”¨ FSDP (Fully Sharded Data Parallel) å’Œ TP (Tensor Parallel) è®­ç»ƒ Transformer ç±»æ¨¡å‹ã€‚</p><h3 id="tp-æ˜¯æ€ä¹ˆèµ·æ•ˆçš„" tabindex="-1">TP æ˜¯æ€ä¹ˆèµ·æ•ˆçš„ <a class="header-anchor" href="#tp-æ˜¯æ€ä¹ˆèµ·æ•ˆçš„" aria-hidden="true">#</a></h3><p><a href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noreferrer">Tensor Parallelï¼ˆTPï¼‰</a> æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æ¨¡å‹å¹¶è¡Œæ–¹æ³•ï¼Œæœ¬æ–‡æåˆ°çš„ <a href="https://arxiv.org/abs/2205.05198" target="_blank" rel="noreferrer">Sequence Parallel (SP)</a> æ˜¯ä¸€ç§ç‰¹æ®Šçš„ TPï¼Œå®ƒåœ¨ <code>nn.LayerNorm</code> æˆ– <code>RMSNorm</code> çš„ sequence ç»´åº¦ä¸Šåšåˆ†ç‰‡ï¼ŒèŠ‚çœè®­ç»ƒæœŸé—´æ¿€æ´»éƒ¨åˆ†çš„æ˜¾å­˜å ç”¨ã€‚å½“æ¨¡å‹å˜å¤§ï¼Œè¿™éƒ¨åˆ†å ç”¨ä¼šå¾ˆé«˜ï¼Œæ‰€ä»¥ä¸€èˆ¬ TP éƒ½æ˜¯ä»¥ SP çš„å½¢å¼å®æ–½ã€‚</p><p>ä¸‹å›¾å±•ç¤ºäº† Transformer æ¨¡å‹çš„ MLP å±‚å’Œ Self-Attention å±‚æ˜¯æ€ä¹ˆé‡‡ç”¨ TP çš„æ–¹å¼è¿›è¡Œåˆ†ç‰‡çš„ã€‚åŸå§‹çš„çŸ©é˜µä¹˜æ³•éƒ½å¯ä»¥è¢«åˆ†ç‰‡å¤„ç†ã€‚</p><p><img src="/assets/shard.57043b71.png" alt=""></p><p>Pytorch TP çš„å·¥ä½œæµç¨‹å¤§è‡´å¦‚ä¸‹ï¼š</p><h4 id="åˆ†ç‰‡åˆå§‹åŒ–" tabindex="-1">åˆ†ç‰‡åˆå§‹åŒ–ï¼š <a class="header-anchor" href="#åˆ†ç‰‡åˆå§‹åŒ–" aria-hidden="true">#</a></h4><ul><li>å†³å®šæ¨¡å‹å„å±‚çš„å¹¶è¡Œç­–ç•¥ <code>ParallelStyle</code>ï¼Œè°ƒç”¨ <code>parallelize_module</code>ã€‚</li><li>å¹¶è¡ŒåŒ–çš„æ¨¡å‹å‚æ•°è¢«è½¬æ¢ä¸º DTensorï¼ŒDTensor è´Ÿè´£ä½¿ç”¨åˆ†ç‰‡çš„è®¡ç®—æ–¹æ³•å…è®¸å¹¶è¡ŒåŒ–çš„æ¨¡å‹ã€‚</li></ul><h4 id="è¿è¡Œæ—¶çš„å‰å‘åå‘æ›´æ–°" tabindex="-1">è¿è¡Œæ—¶çš„å‰å‘åå‘æ›´æ–° <a class="header-anchor" href="#è¿è¡Œæ—¶çš„å‰å‘åå‘æ›´æ–°" aria-hidden="true">#</a></h4><ul><li>æ ¹æ®ç”¨æˆ·ç»™ <code>ParallelStyle</code> æŒ‡å®šçš„è¾“å…¥è¾“å‡º DTensor çš„è§„æ¨¡ï¼Œä½¿ç”¨é›†åˆé€šä¿¡æ–¹æ³•è¿›è¡Œè½¬æ¢ã€‚</li><li>åœ¨å¹¶è¡Œè¹­ä¸Šè¿›è¡Œåˆ†ç‰‡åçš„è®¡ç®—ã€‚</li></ul><h3 id="ä»€ä¹ˆæ—¶å€™åº”è¯¥ä½¿ç”¨-tp" tabindex="-1">ä»€ä¹ˆæ—¶å€™åº”è¯¥ä½¿ç”¨ TP <a class="header-anchor" href="#ä»€ä¹ˆæ—¶å€™åº”è¯¥ä½¿ç”¨-tp" aria-hidden="true">#</a></h3><p>Pytorch çš„ FSDP å·²ç»å¯ä»¥å®ç°å¤šå¡ä¸Šåˆ†ç‰‡è®¡ç®—ï¼Œä½†æ˜¯å¡æ•°å¢å¤šåï¼ŒFSDP å°±ä¼šæœ‰åˆ«çš„é—®é¢˜ï¼š</p><ol><li>GPU æ•°é‡è¿‡å¤§æ—¶ï¼ˆ128/256ï¼‰ï¼ŒFSDP ä»¥æ¥çš„ <code>allgather</code> ç­‰é›†åˆé€šä¿¡ä¼šè¢«é€šä¿¡è€—æ—¶å½±å“ã€‚é€šè¿‡åœ¨ FSDP ä¹‹ä¸Šéƒ¨ç½² TP/SPï¼ŒFSDP çš„å°ºåº¦å¯ä»¥è¢«å¤§å¹…å‰Šå‡ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨èŠ‚ç‚¹å†…éƒ¨ç½² TP/SPï¼ŒèŠ‚ç‚¹é—´ä½¿ç”¨ FSDPã€‚</li><li>ç”±äºæ”¶æ•›æ€§å’Œ GPU æ˜¾å­˜é™åˆ¶ï¼Œå…¨å±€çš„ batch size è¾¾åˆ°äº†æ•°æ®å¹¶è¡Œçš„ä¸Šé™ï¼Œæ— æ³•ç»§ç»­æå‡äº†ã€‚TP/SP æˆä¸ºäº†å”¯ä¸€çš„æ–¹æ³•å»ç»§ç»­æ‹“å±•åˆ°æ›´å¤šçš„ GPUï¼Œè¿™ä¹Ÿæ„å‘³ç€ GPU æ•°é‡å’Œæ¨¡å‹è§„æ¨¡å¯ä»¥ç»§ç»­æ‰©å¼ ã€‚</li><li>ç‰¹å®šçš„æ¨¡å‹ï¼Œå½“æœ¬åœ°çš„ batch size å˜å°ä»¥åï¼ŒTP/SP å¯ä»¥è¿”å›å¯¹ FLOPS è°ƒä¼˜çš„çŸ©é˜µä¹˜æ³•ã€‚</li></ol><p>å½“é¢„è®­ç»ƒçš„æ—¶å€™ï¼Œå¾ˆå®¹æ˜“è¾¾åˆ°ä¸Šé™ã€‚é¢„è®­ç»ƒä¸€ä¸ªæ•°äº¿å¤§æ¨¡å‹çš„æ¨¡å‹ä¼šèŠ±å¾ˆå¤šæœˆï¼Œå³ä½¿ä½¿ç”¨å‡ åƒä¸ª GPUã€‚</p><ul><li>é—®é¢˜ 1 å¾ˆå®¹æ˜“ç¢°åˆ°ï¼Œä¾‹å¦‚ Llama 2 70B ä½¿ç”¨ 2000 å¼  GPU è®­ç»ƒäº† 35 å¤©ï¼Œå¤šç»´å¹¶è¡Œçš„è§„æ¨¡ä¸º 2000ã€‚</li><li>å½“ transformer æ¨¡å‹å˜å¤§ï¼Œå¾ˆå¿«å°±ä¼šç¢°åˆ°é—®é¢˜ 2ã€‚ä¾‹å¦‚åœ¨ Llama 2 çš„å…¨å±€ batch size ä¸º 1000 æ—¶ï¼Œå³ä½¿è®¾ç½® local batch size ä¸º 1ï¼Œä¹Ÿä¸èƒ½ä»…å®æ–½ FSDPï¼Œå› ä¸ºæœ‰ 2000 å¼ å¡ã€‚</li></ul><h3 id="å¦‚ä½•å®æ–½-tp" tabindex="-1">å¦‚ä½•å®æ–½ TP <a class="header-anchor" href="#å¦‚ä½•å®æ–½-tp" aria-hidden="true">#</a></h3><p>Pytorch TP API æä¾›çš„æ˜¯å¯¹æ¨¡å‹å„å±‚è¿›è¡Œåˆ†ç‰‡ç­–ç•¥çš„æ¥å£ã€‚</p><ul><li><p><code>ColwiseParallel</code> å’Œ <code>RowwiseParallel</code>ï¼šä»¥è¡Œ/åˆ—çš„æ–¹å¼åˆ†ç‰‡ <code>nn.Linear</code> å’Œ <code>nn.Embedding</code></p></li><li><p><code>SequenceParallel</code>: å¯¹ <code>nn.LayerNorm</code>, <code>nn.Dropout</code>, <code>RMSNormPython</code> ç­‰è¿›è¡Œåˆ†ç‰‡</p></li><li><p><code>PrepareModuleInput</code> å’Œ <code>PrepareModuleOutput</code>: è®¾å®šæ¨¡å—çš„è¾“å…¥è¾“å‡ºåˆ†ç‰‡è®¾ç½®ï¼Œä»¥ä¾¿é‡‡ç”¨åˆé€‚çš„é›†åˆé€šä¿¡æ“ä½œ</p></li></ul><p>ä»¥ <a href="https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/llama2_model.py" target="_blank" rel="noreferrer">Llama2</a> ä¸ºä¾‹ï¼Œé¦–å…ˆå€ŸåŠ© <code>DeviceMesh</code> ç®€æ´åœ°åˆå§‹åŒ– NCCLï¼ŒTP ä¸€èˆ¬åœ¨èŠ‚ç‚¹å†…ä½¿ç”¨ï¼Œå¦‚ä¸‹é¢è¿™ä¸ªå…«å¡çš„ç”¨ä¾‹ï¼š</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#676E95;font-style:italic;"># run this via torchrun: torchrun --standalone --nproc_per_node=8 ./tp_tutorial.py</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">device_mesh </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> init_device_mesh</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">tp_mesh </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">init_device_mesh</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">8</span><span style="color:#89DDFF;">,))</span></span>
<span class="line"></span></code></pre></div><p><a href="https://github.com/pytorch/examples/blob/main/distributed/tensor_parallelism/llama2_model.py" target="_blank" rel="noreferrer">Llama2</a> çš„å®ç°ä¸­ï¼Œæ ¸å¿ƒçš„ <code>TransformerBlock </code> ç”± <code>Attention</code> å’Œ <code>FeedForward</code> å±‚ç»„æˆï¼Œ<code>FeedForward</code> å¦‚ä¸‹</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#676E95;font-style:italic;"># forward in the FeedForward layer</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">w2</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">F</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">silu</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">w1</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">))</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">*</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">w3</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">))</span></span>
<span class="line"></span></code></pre></div><p>æ˜¾ç„¶å¯ä»¥å¯¹ <code>w1/w3</code> æŒ‰åˆ—è¿›è¡Œåˆ†ç‰‡ï¼Œ<code>w2</code> æŒ‰è¡Œè¿›è¡Œåˆ†ç‰‡ï¼Œä»£ç å¦‚ä¸‹ï¼Œä¸éœ€è¦å…³å¿ƒåº•å±‚çš„é›†åˆé€šä¿¡ï¼Œä¼šè¢«è‡ªåŠ¨æ‰§è¡Œ</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">tensor</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">parallel </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> ColwiseParallel</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> RowwiseParallel</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> parallelize_module</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">layer_tp_plan </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># by default ColwiseParallel input layouts is replicated</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># and RowwiseParallel output layouts is replicated</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_foward.w1</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward.w2</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">RowwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward.w3</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>ç±»ä¼¼åœ°ï¼Œå¯¹ <code>Attention</code> æ¨¡å—ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰åˆ—åˆ†ç‰‡ <code>q/k/v</code> çš„æŠ•å½±è¿‡ç¨‹ï¼Œç„¶åæŒ‰è¡Œå¯¹ <code>wo</code> çš„çº¿æ€§æŠ•å½±è¿›è¡Œåˆ†ç‰‡ã€‚</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">layer_tp_plan </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># by default ColwiseParallel input layouts is replicated</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># and RowwiseParallel output layouts is replicated</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention.wq</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention.wk</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention.wv</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention.wo</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">RowwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward.w1</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward.w2</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">RowwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward.w3</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>åŸºæœ¬ä¸Šæ¨¡å‹çš„åˆ†ç‰‡å°±å®Œæˆäº†ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯äººæ ¼å¯¹çº¿å½¢å±‚è¿›è¡ŒæŒ‰åˆ—åˆ†ç‰‡ï¼Œè¾“å‡ºå°±ä¼šè¢«æŒ‰å¼ é‡çš„æœ€åä¸€ä¸ªç»´åº¦å»åˆ†ç‰‡ï¼Œè€ŒæŒ‰è¡Œåˆ†ç‰‡çš„çº¿å½¢å±‚åˆ™æ¥æ”¶åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šè¿›è¡Œåˆ†ç‰‡çš„è¾“å…¥ã€‚å¦‚æœåœ¨æŒ‰åˆ—/æŒ‰è¡Œåˆ†ç‰‡çš„çº¿æ€§å±‚é—´è¿˜æœ‰åˆ«çš„æ“ä½œï¼ˆå¦‚ <code>view()</code> æ“ä½œï¼‰ï¼Œéœ€è¦å»è°ƒæ•´å¼ é‡çš„å½¢çŠ¶ã€‚</p><p>Llama2 çš„ attention å±‚ä¸­å°±æœ‰ <code>view</code> æ“ä½œã€‚<code>wq/ wk/ wv</code> çš„çº¿æ€§å±‚æ˜¯æŒ‰åˆ—åˆ†ç‰‡çš„ï¼Œæ¿€æ´»å¼ é‡æ˜¯åœ¨ <code>num_heads</code> ç»´åº¦ä¸Šè¿›è¡Œåˆ†ç‰‡ï¼Œæ‰€ä»¥éœ€è¦è°ƒæ•´ <code>num_heads</code> ä¸ºå±€éƒ¨çš„ <code>num_heads</code>ã€‚</p><p>æœ€åè°ƒç”¨ <code>parallelize_module</code> å°†æ¨¡å‹è½¬æ¢åˆ° DTensor ä¸Šï¼Œé›†åˆé€šä¿¡å°±ä¼šè¢«è‡ªåŠ¨æ³¨å†Œåˆ°å„ä¸ªæ¨¡å—çš„è¾“å…¥å’Œè¾“å‡ºä¸Šã€‚</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#A6ACCD;"> layer_id</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> transformer_block </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">enumerate</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">model</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">layers</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    layer_tp_plan </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span><span style="color:#A6ACCD;">...</span><span style="color:#89DDFF;">}</span><span style="color:#A6ACCD;">  </span><span style="color:#676E95;font-style:italic;"># i.e. the plan we just generated</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># Adjust attention module to use the local number of heads</span></span>
<span class="line"><span style="color:#A6ACCD;">    attn_layer </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> transformer_block</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">attention</span></span>
<span class="line"><span style="color:#A6ACCD;">    attn_layer</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">n_heads</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> attn_layer</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">n_heads</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">//</span><span style="color:#A6ACCD;"> tp_mesh</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">    attn_layer</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">n_kv_heads</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> attn_layer</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">n_kv_heads</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">//</span><span style="color:#A6ACCD;"> tp_mesh</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">parallelize_module</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">module</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">transformer_block</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">device_mesh</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">tp_mesh</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">parallelize_plan</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">layer_tp_plan</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><p>ç°åœ¨æˆ‘ä»¬å·²ç»è¯¦ç»†é˜è¿°äº†æ¯ä¸ª <code>TransformerBlock</code> çš„åˆ†ç‰‡è®¡åˆ’ï¼Œé€šå¸¸åœ¨ç¬¬ä¸€å±‚ä¸­æœ‰ä¸€ä¸ª <code>nn.Embedding</code>ï¼Œæœ€åä¸€å±‚æœ‰ä¸€ä¸ª <code>nn.Linear</code>æŠ•å½±å±‚ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©å¯¹ç¬¬ä¸€ä¸ª <code>nn.Embedding</code> è¿›è¡Œé€è¡Œæˆ–é€åˆ—åˆ†ç‰‡ï¼Œå¹¶å¯¹æœ€åä¸€ä¸ª <code>nn.Linear</code> æŠ•å½±å±‚è¿›è¡Œé€åˆ—åˆ†ç‰‡ï¼ŒåŒæ—¶æŒ‡å®šé€‚å½“çš„è¾“å…¥å’Œè¾“å‡ºå¸ƒå±€ã€‚</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">model </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">parallelize_module</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">    model</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    tp_mesh</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">tok_embeddings</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> RowwiseParallel</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">input_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Replicate</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">output</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> ColwiseParallel</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">output_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Replicate</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><h3 id="sp-ä¾‹å­" tabindex="-1">SP ä¾‹å­ <a class="header-anchor" href="#sp-ä¾‹å­" aria-hidden="true">#</a></h3><p>SP æ˜¯åœ¨ä¸Šé¢æ‰€ç¤ºçš„ TP çš„åŸºç¡€ä¸Šè¿›è¡Œçš„ã€‚åŸºæœ¬çš„ TP åªåœ¨æ³¨æ„åŠ›æ¨¡å—å’Œå‰é¦ˆæ¨¡å—ä¸­åˆ†ç‰‡å¼ é‡ï¼Œä¼šå¤åˆ¶å®ƒä»¬çš„æ¨¡å—è¾“å…¥å’Œè¾“å‡ºï¼ˆå³å‰å‘ä¼ é€’ä¸­çš„æ¿€æ´»å’Œåå‘ä¼ é€’ä¸­çš„æ¢¯åº¦ï¼‰ã€‚åºåˆ—å¹¶è¡Œåˆ™åœ¨åºåˆ—ç»´åº¦ä¸Šè¿›è¡Œåˆ†ç‰‡ã€‚</p><p>åœ¨å…¸å‹çš„ <code>TransformerBlock</code> ä¸­ï¼Œ<code>forward()</code> å‡½æ•°ç»“åˆäº†ç”¨äºæ­£åˆ™åŒ–çš„å±‚ï¼ˆ<code>LayerNorm</code> æˆ– <code>RMSNorm</code>ï¼‰ã€æ³¨æ„åŠ›å±‚ã€å‰é¦ˆå±‚å’Œæ®‹å·®è¿æ¥ã€‚ä¾‹å¦‚ï¼š</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#676E95;font-style:italic;"># forward in a TransformerBlock</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    h </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> x </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">attention</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">attention_norm</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#A6ACCD;">    out </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> h </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">feed_forward</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">ffn_norm</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">h</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> out</span></span>
<span class="line"></span></code></pre></div><p>åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ³¨æ„åŠ›å’Œå‰é¦ˆæ¨¡å—ä¹‹å¤–çš„æ¿€æ´»ï¼ˆå’Œæ¢¯åº¦ï¼‰çš„å½¢çŠ¶ä¸º [æ‰¹æ¬¡å¤§å°ï¼Œåºåˆ—é•¿åº¦ï¼Œéšè—ç»´åº¦]ã€‚åœ¨ DTensor çš„æœ¯è¯­ä¸­ï¼ŒSP ä½¿ç”¨ Shard(1) å¸ƒå±€æ¥æ‰§è¡Œæ¨¡å—çš„å‰å‘ä¼ é€’å’Œåå‘ä¼ é€’çš„æ¿€æ´»è®¡ç®—ã€‚ä»¥ä¸‹ä»£ç ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•å°†åºåˆ—å¹¶è¡Œåº”ç”¨äº<code>TransformerBlock</code> ä¸­çš„æ­£åˆ™åŒ–å±‚ï¼š</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">tensor</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">parallel </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#A6ACCD;">    PrepareModuleInput</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    SequenceParallel</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">layer_tp_plan </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># Now the input and output of SequenceParallel has Shard(1) layouts,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># to represent the input/output tensors sharded on the sequence dimension</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention_norm</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">SequenceParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">PrepareModuleInput</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">input_layouts</span><span style="color:#89DDFF;">=(</span><span style="color:#82AAFF;">Shard</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">desired_input_layouts</span><span style="color:#89DDFF;">=(</span><span style="color:#82AAFF;">Replicate</span><span style="color:#89DDFF;">(),),</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention.wq</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention.wk</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention.wv</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">attention.wo</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">RowwiseParallel</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">output_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Shard</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">ffn_norm</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">SequenceParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">PrepareModuleInput</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">input_layouts</span><span style="color:#89DDFF;">=(</span><span style="color:#82AAFF;">Shard</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#A6ACCD;font-style:italic;">desired_input_layouts</span><span style="color:#89DDFF;">=(</span><span style="color:#82AAFF;">Replicate</span><span style="color:#89DDFF;">(),),</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward.w1</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward.w2</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">RowwiseParallel</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">output_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Shard</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)),</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">feed_forward.w3</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ColwiseParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬ç°åœ¨ä½¿ç”¨ <code>PrepareModuleInput</code> å°†æ³¨æ„åŠ›å’Œå‰é¦ˆå±‚çš„æ¨¡å—è¾“å…¥å¸ƒå±€ä» <code>Shard(1)</code> ä¿®æ”¹ä¸º <code>Replicate()</code>ï¼Œå¹¶å°†å®ƒä»¬çš„è¾“å‡ºå¸ƒå±€æ ‡è®°ä¸º <code>Shard(1)</code>ã€‚å°±åƒ TP ä¸­å‘ç”Ÿçš„é‚£æ ·ï¼Œæˆ‘ä»¬åªéœ€è¦æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºçš„å¼ é‡åˆ†ç‰‡å¸ƒå±€ï¼Œå±‚ä¹‹é—´çš„é€šä¿¡å°†è‡ªåŠ¨å‘ç”Ÿã€‚</p><p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨ SP ä¸­ï¼Œæˆ‘ä»¬å‡è®¾ <code>TransformerBlock</code> çš„è¾“å…¥å’Œè¾“å‡ºå§‹ç»ˆåœ¨åºåˆ—ç»´åº¦ä¸Šè¿›è¡Œåˆ†ç‰‡ï¼Œä»¥ä¾¿å¤šä¸ª <code>TransformerBlock</code> å¯ä»¥æ— ç¼è¿æ¥ã€‚è¿™å¯ä»¥é€šè¿‡æ˜¾å¼æŒ‡å®šèµ·å§‹çš„ <code>nn.Embedding</code> å±‚çš„è¾“å‡ºå’Œæœ€ç»ˆ <code>nn.Linear</code> æŠ•å½±å±‚çš„è¾“å…¥ä¸º <code>Shard(1)</code> æ¥å®ç°ï¼š</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">model </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">parallelize_module</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">    model</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    tp_mesh</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">tok_embeddings</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> RowwiseParallel</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">input_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Replicate</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">output_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Shard</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">norm</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> SequenceParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">output</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> ColwiseParallel</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">input_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Shard</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">output_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Replicate</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><h3 id="lp" tabindex="-1">LP <a class="header-anchor" href="#lp" aria-hidden="true">#</a></h3><p>æŸå¤±å¹¶è¡Œ LPï¼ˆLoss Parallelï¼‰ç”¨äºåœ¨è®¡ç®—æŸå¤±å‡½æ•°æ—¶èŠ‚çœå†…å­˜å’Œé€šä¿¡ï¼Œå› ä¸ºæ¨¡å‹è¾“å‡ºé€šå¸¸éå¸¸å¤§ã€‚åœ¨ LP ä¸­ï¼Œå½“æ¨¡å‹è¾“å‡ºåœ¨å·¨å¤§çš„è¯æ±‡ç»´åº¦ä¸Šè¿›è¡Œåˆ†ç‰‡æ—¶ï¼ˆæ¯”å¦‚ GPT4 ä½¿ç”¨çš„ cl100k_baseï¼Œå…·æœ‰ 10ä¸‡ç§ tokenï¼‰ï¼Œå¯ä»¥é«˜æ•ˆåœ°è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼Œè€Œæ— éœ€å°†æ‰€æœ‰æ¨¡å‹è¾“å‡ºèšé›†åˆ°æ¯ä¸ª GPU ä¸Šã€‚è¿™ä¸ä»…æ˜¾è‘—å‡å°‘äº†å†…å­˜æ¶ˆè€—ï¼Œè¿˜é€šè¿‡å‡å°‘é€šä¿¡å¼€é”€å’Œå¹¶è¡Œè¿›è¡Œåˆ†ç‰‡è®¡ç®—æ¥æé«˜è®­ç»ƒé€Ÿåº¦ã€‚ä¸‹å›¾ç®€è¦è¯´æ˜äº† LP å¦‚ä½•é€šè¿‡è¿›è¡Œåˆ†ç‰‡è®¡ç®—æ¥é¿å…å°†æ‰€æœ‰æ¨¡å‹è¾“å‡ºèšé›†åˆ°æ¯ä¸ª GPU ä¸Šã€‚</p><p>ä¸‹å›¾åœ¨ä½¿ç”¨ LP è¿›è¡Œäº¤å‰ç†µæŸå¤±å‰å‘è®¡ç®—ã€‚è“è‰²ä»£è¡¨åˆ†ç‰‡å¼ é‡ï¼Œç»¿è‰²ä»£è¡¨å¤åˆ¶å¼ é‡ï¼Œé»„è‰²ä»£è¡¨å…·æœ‰éƒ¨åˆ†å€¼çš„å¼ é‡ï¼ˆå°†è¿›è¡Œå…¨å±€çš„ reduction é›†åˆé€šä¿¡ï¼‰ã€‚é»‘è‰²ç®­å¤´è¡¨ç¤ºæœ¬åœ°è®¡ç®—ï¼Œçº¢è‰²ç®­å¤´è¡¨ç¤º GPU ä¹‹é—´çš„é›†åˆé€šä¿¡è¡Œä¸ºã€‚å›¾é‡Œåˆ†ä¸¤æ­¥æ˜¯å› ä¸ºåœ¨é’ˆå¯¹ç±»åˆ«è®¡ç®—æŸå¤±çš„æ—¶å€™ï¼Œ<code>CrossEntropyLoss</code> å¯ä»¥æ‹†è§£æˆ <code>LogSoftmax</code> å’Œ <code>NLLLoss</code>ã€‚</p><p><img src="/assets/loss.24c62066.png" alt=""></p><p>åœ¨ PyTorch çš„ TP APIä¸­ï¼Œå¯ä»¥é€šè¿‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨ <code>loss_parallel</code> å¯ç”¨ LPã€‚ä½¿ç”¨ <code>loss_parallel</code>ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ <code>torch.nn.functional.cross_entropy</code> æˆ– <code>torch.nn.CrossEntropyLoss</code>ï¼Œè€Œæ— éœ€ä¿®æ”¹ä»£ç ä¸­çš„å…¶ä»–éƒ¨åˆ†ã€‚</p><p>è¦åº”ç”¨ LPï¼Œæ¨¡å‹çš„é¢„æµ‹ç»“æœé€šå¸¸æ˜¯å½¢çŠ¶ä¸º[æ‰¹æ¬¡å¤§å°ï¼Œåºåˆ—é•¿åº¦ï¼Œè¯æ±‡å¤§å°]çš„å¼ é‡ï¼Œåº”åœ¨è¯æ±‡å¤§å°çš„ç»´åº¦ä¸Šè¿›è¡Œåˆ†ç‰‡ã€‚å¯ä»¥é€šè¿‡æ ‡è®°æœ€åä¸€ä¸ªçº¿æ€§æŠ•å½±å±‚è¾“å‡ºçš„è¾“å‡ºå¸ƒå±€æ¥è½»æ¾å®ç°è¿™ä¸€ç‚¹ã€‚</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">model </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">parallelize_module</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">    model</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    tp_mesh</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">tok_embeddings</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> RowwiseParallel</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">input_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Replicate</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">output_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Shard</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">norm</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> SequenceParallel</span><span style="color:#89DDFF;">(),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">output</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> ColwiseParallel</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">input_layouts</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">Shard</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#676E95;font-style:italic;"># use DTensor as the output</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;font-style:italic;">use_local_output</span><span style="color:#89DDFF;">=False,</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">},</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><p>åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œæˆ‘ä»¬è¿˜åœ¨è¾“å‡ºä¹‹å‰å¯¹è§„èŒƒåŒ–å±‚åº”ç”¨äº† SPã€‚æˆ‘ä»¬ä½¿ç”¨<code>use_local_output=False</code>ï¼Œä½¿è¾“å‡ºä¿æŒä¸ºä¸€ä¸ª DTensorï¼Œä»¥ä¾¿åœ¨<code>loss_parallel</code> çš„ä¸Šä¸‹æ–‡ç§ä½¿ç”¨ã€‚ä¹‹åï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚è¯·æ³¨æ„ï¼Œåå‘è®¡ç®—ä¹Ÿéœ€è¦åœ¨è¯¥ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œã€‚</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">functional</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#A6ACCD;"> F</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">tensor</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">parallel </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> loss_parallel</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">pred </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">model</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">input_ids</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">with</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">loss_parallel</span><span style="color:#89DDFF;">():</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># assuming pred and labels are of the shape [batch, seq, vocab]</span></span>
<span class="line"><span style="color:#A6ACCD;">    loss </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> F</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">cross_entropy</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">pred</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">flatten</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> labels</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">flatten</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#A6ACCD;">    loss</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">backward</span><span style="color:#89DDFF;">()</span></span>
<span class="line"></span></code></pre></div><h3 id="tp-å’Œ-fsdp-ç»“åˆ" tabindex="-1">TP å’Œ FSDP ç»“åˆ <a class="header-anchor" href="#tp-å’Œ-fsdp-ç»“åˆ" aria-hidden="true">#</a></h3><p>ç°åœ¨æˆ‘ä»¬å·²ç»å±•ç¤ºäº†å¦‚ä½•å°† TP/SP åº”ç”¨äºæ¨¡å‹ï¼Œè®©æˆ‘ä»¬ä¹Ÿçœ‹ä¸€ä¸‹å¦‚ä½•å°† TP å’Œ FSDP ç»“åˆèµ·æ¥ä½¿ç”¨ã€‚ç”±äº TP ä¼šé˜»å¡è®¡ç®—çš„é€šä¿¡ï¼Œæˆ‘ä»¬å¸Œæœ›ç¡®ä¿å®ƒåœ¨æœ¬åœ°çš„ NVLink é—´ä¼ è¾“ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬é€šå¸¸åœ¨ host å†…åº”ç”¨ TPï¼Œå¹¶åœ¨ host é—´åº”ç”¨FSDPã€‚</p><p><img src="/assets/tp_fsdp.89afb392.png" alt=""></p><p>å›¾3. FSDP å’Œ TPåœ¨ä¸åŒçš„è®¾å¤‡ç»´åº¦ä¸Šå·¥ä½œï¼ŒFSDP é€šä¿¡åœ¨ host é—´è¿›è¡Œï¼ŒTP é€šä¿¡åœ¨ host å†…è¿›è¡Œã€‚</p><p>é€šè¿‡ 2D DeviceMesh å¯ä»¥è½»æ¾è¡¨ç¤ºè¿™ç§ 2D å¹¶è¡Œæ¨¡å¼ï¼Œæˆ‘ä»¬åªéœ€è¦å°†æ¯ä¸ªâ€œå­â€ DeviceMesh ä¼ é€’ç»™å„ä¸ªå¹¶è¡ŒAPIï¼š</p><p>è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥è½»æ¾åœ°åœ¨æ¯ä¸ª host å†…åº”ç”¨ TP å¹¶åœ¨ host é—´åº”ç”¨ FSDPï¼Œè€Œä¸éœ€è¦å¯¹ Llama æ¨¡å‹è¿›è¡Œä»»ä½•ä»£ç æ›´æ”¹ã€‚å¹¶å¯ä»¥ç»§ç»­å¢åŠ æ¨¡å‹è§„æ¨¡ï¼Œä½¿ç”¨å¤§é‡GPUè¿›è¡Œé«˜æ•ˆè®­ç»ƒã€‚</p><h2 id="å¼ é‡å¹¶è¡Œ-api" tabindex="-1">å¼ é‡å¹¶è¡Œ API <a class="header-anchor" href="#å¼ é‡å¹¶è¡Œ-api" aria-hidden="true">#</a></h2><p><code>torch.distributed.tensor.parallel.parallelize_module(module, device_mesh, parallelize_plan)</code> æ¥å£ç”¨äºå¯¹ä»»æ„ <code>nn.Module</code> å®è¡Œå¼ é‡å¹¶è¡Œã€‚å®ƒå°†è¿”å›ä¸€ä¸ªå¹¶è¡ŒåŒ–çš„åçš„ <code>nn.Module</code>ã€‚ç”¨æ³•å¦‚ä¸‹ï¼ŒæŒ‡å®šæ¨¡å‹ã€Device Mesh å’Œå…·ä½“å‚æ•°çš„å¹¶è¡Œç­–ç•¥ã€‚</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">tensor</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">parallel </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> parallelize_module</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> ColwiseParallel</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">device_mesh </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> init_device_mesh</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Define the module.</span></span>
<span class="line"><span style="color:#A6ACCD;">m </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">Model</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">...</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">tp_mesh </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">init_device_mesh</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">8</span><span style="color:#89DDFF;">,))</span></span>
<span class="line"><span style="color:#A6ACCD;">m </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">parallelize_module</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">m</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> tp_mesh</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">{</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">w1</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> ColwiseParallel</span><span style="color:#89DDFF;">(),</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">w2</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> RowwiseParallel</span><span style="color:#89DDFF;">()})</span></span>
<span class="line"></span></code></pre></div><p>è¯¦ç»†å†…å®¹é˜…è¯»<a href="https://pytorch.org/docs/stable/distributed.tensor.parallel.html" target="_blank" rel="noreferrer">æ–‡æ¡£</a>ã€‚</p><h2 id="device-mesh" tabindex="-1">Device Mesh <a class="header-anchor" href="#device-mesh" aria-hidden="true">#</a></h2><p>å…³äº DeviceMeshï¼Œå¯ä»¥å‚è€ƒ <a href="https://pytorch.org/tutorials/recipes/distributed_device_mesh.html" target="_blank" rel="noreferrer">ç›¸å…³ æ–‡æ¡£</a>ï¼Œå®ƒç”¨äºç®€åŒ–å¤šç»´å¹¶è¡Œæƒ…å†µä¸‹ï¼ŒNCCL é›†åˆé€šä¿¡åˆ†ç»„çš„é…ç½®ã€‚</p><p>åœ¨æ²¡æœ‰ Device Mesh çš„æ—¶å€™ï¼Œå•æœºå…«å¡æƒ³è¦åˆ†æˆä¸¤ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹å››å¼ å¡ï¼Œåšé›†åˆé€šä¿¡çš„è¯ï¼Œåˆå§‹åŒ–ä»£ç å¦‚ä¸‹</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> os</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> torch</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">distributed</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#A6ACCD;"> dist</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Understand world topology</span></span>
<span class="line"><span style="color:#A6ACCD;">rank </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">int</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">os</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">environ</span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">RANK</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">])</span></span>
<span class="line"><span style="color:#A6ACCD;">world_size </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">int</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">os</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">environ</span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">WORLD_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">])</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;Running example on </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">rank</span><span style="color:#C792EA;">=</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;"> in a world with </span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">world_size</span><span style="color:#C792EA;">=</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Create process groups to manage 2-D like parallel pattern</span></span>
<span class="line"><span style="color:#A6ACCD;">dist</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">init_process_group</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">nccl</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">cuda</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">set_device</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">rank</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Create shard groups (e.g. (0, 1, 2, 3), (4, 5, 6, 7))</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># and assign the correct shard group to each rank</span></span>
<span class="line"><span style="color:#A6ACCD;">num_node_devices </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">cuda</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">device_count</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">shard_rank_lists </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">list</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">range</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> num_node_devices </span><span style="color:#89DDFF;">//</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">)),</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">list</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">range</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">num_node_devices </span><span style="color:#89DDFF;">//</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> num_node_devices</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#A6ACCD;">shard_groups </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#A6ACCD;">    dist</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">new_group</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">shard_rank_lists</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">]),</span></span>
<span class="line"><span style="color:#A6ACCD;">    dist</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">new_group</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">shard_rank_lists</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">]),</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">current_shard_group </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#A6ACCD;">    shard_groups</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">]</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> rank </span><span style="color:#89DDFF;">in</span><span style="color:#A6ACCD;"> shard_rank_lists</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">]</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;font-style:italic;">else</span><span style="color:#A6ACCD;"> shard_groups</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">]</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Create replicate groups (for example, (0, 4), (1, 5), (2, 6), (3, 7))</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># and assign the correct replicate group to each rank</span></span>
<span class="line"><span style="color:#A6ACCD;">current_replicate_group </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">None</span></span>
<span class="line"><span style="color:#A6ACCD;">shard_factor </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">len</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">shard_rank_lists</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">])</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#A6ACCD;"> i </span><span style="color:#89DDFF;font-style:italic;">in</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">range</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">num_node_devices </span><span style="color:#89DDFF;">//</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    replicate_group_ranks </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">list</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">range</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">i</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> num_node_devices</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> shard_factor</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#A6ACCD;">    replicate_group </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> dist</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">new_group</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">replicate_group_ranks</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> rank </span><span style="color:#89DDFF;">in</span><span style="color:#A6ACCD;"> replicate_group_ranks</span><span style="color:#89DDFF;">:</span></span>
<span class="line"><span style="color:#A6ACCD;">        current_replicate_group </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> replicate_group</span></span>
<span class="line"></span></code></pre></div><p>å¯ä»¥ç®€åŒ–æˆ</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">device_mesh </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> init_device_mesh</span></span>
<span class="line"><span style="color:#A6ACCD;">mesh_2d </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">init_device_mesh</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">mesh_dim_names</span><span style="color:#89DDFF;">=(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">replicate</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">shard</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># Users can access the underlying process group thru `get_group` API.</span></span>
<span class="line"><span style="color:#A6ACCD;">replicate_group </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> mesh_2d</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">get_group</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">mesh_dim</span><span style="color:#89DDFF;">=</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">replicate</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">shard_group </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> mesh_2d</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">get_group</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">mesh_dim</span><span style="color:#89DDFF;">=</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">shard</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><h3 id="æ¡ˆä¾‹" tabindex="-1">æ¡ˆä¾‹ <a class="header-anchor" href="#æ¡ˆä¾‹" aria-hidden="true">#</a></h3><p>Hybrid Sharding Data Parallel(HSDP) æ˜¯ä¸€ä¸ª 2D ç­–ç•¥ï¼Œåœ¨å•ä¸ªèŠ‚ç‚¹å†…å®æ–½ FSDPï¼ŒèŠ‚ç‚¹é—´å®æ–½ DDPã€‚</p><p>ä¸‹é¢è¿™æ®µä»£ç å’Œä¸Šé¢çš„åˆ†ç»„æ‹“æ‰‘ä¸€è‡´ï¼Œåˆ†æˆä¸¤ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹å››å¼ å¡ï¼›ä¸¤ä¸ªèŠ‚ç‚¹é—´ DDPï¼Œæ˜¯è¦ replicateï¼›èŠ‚ç‚¹å†… FSDPï¼Œæ˜¯åšäº† shardã€‚</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> torch</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">nn</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#A6ACCD;"> nn</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">device_mesh </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> init_device_mesh</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">distributed</span><span style="color:#89DDFF;">.</span><span style="color:#A6ACCD;">fsdp </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> FullyShardedDataParallel </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#A6ACCD;"> FSDP</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> ShardingStrategy</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#C792EA;">class</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">ToyModel</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">ToyModel</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">).</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">net1</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">relu</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">ReLU</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">net2</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">5</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">net2</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">relu</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">net1</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)))</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># HSDP: MeshShape(2, 4)</span></span>
<span class="line"><span style="color:#A6ACCD;">mesh_2d </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">init_device_mesh</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">))</span></span>
<span class="line"><span style="color:#A6ACCD;">model </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">FSDP</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">    ToyModel</span><span style="color:#89DDFF;">(),</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">device_mesh</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">mesh_2d</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">sharding_strategy</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">ShardingStrategy</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">HYBRID_SHARD</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><h2 id="dtensor" tabindex="-1">DTensor <a class="header-anchor" href="#dtensor" aria-hidden="true">#</a></h2><p><a href="https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md" target="_blank" rel="noreferrer">https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md</a></p><p>Pytorch çš„ Tensor Parallel ä¾èµ–äºåº•å±‚çš„ DistributedTensor (DTCensor) æ•°æ®ç»“æ„ï¼Œå®ç° SPMDï¼ˆSingle Program Multiple Devicesï¼‰ï¼Œæ”¯æŒ sharding å’Œ replicationã€‚</p><p>éœ€è¦æ³¨æ„çš„æ˜¯å½“ DTensor ç›´æ¥ç”¨äº Data Parallel çš„æ—¶å€™ï¼Œå¯èƒ½ä¼šæ¯” pytorch ä¸­å®ç°çš„ DDP å’Œ FSDP è¦æ…¢ï¼Œå› ä¸ºä»–ä»¬æœ‰å…¨å±€çš„ä¿¡æ¯ï¼Œè€Œ DTensor åªæ˜¯å¼ é‡çº§åˆ«çš„ã€‚</p></div></div></main><!--[--><!--]--><!----><!--[--><!--[--><!--[--><!----><!--]--><!--]--><!--]--></div></div></div></div></div><!----><!--[--><!--]--></div><!----><footer data-v-4f0db67d> Powered by <a href="https://github.com/forsworns/" target="_blank" title="Author" data-v-4f0db67d>Peihao Yang</a> | Copyright Â© 2019-2024 | MIT License </footer><!--]--></div>
    <script>__VP_HASH_MAP__ = JSON.parse("{\"about-me_index.md\":\"aac85ab1\",\"index.md\":\"904c1faf\",\"zh_about-me_index.md\":\"7f3d6c59\",\"zh_blogs_20190721_index.md\":\"1f32ebce\",\"zh_blogs_20190919_index.md\":\"1db8f0f0\",\"zh_blogs_20190901_index.md\":\"367a45f0\",\"zh_blogs_20190824_index.md\":\"e7fae747\",\"zh_blogs_20190908_index.md\":\"6ae37cc5\",\"zh_blogs_20191102_index.md\":\"7be0a891\",\"zh_blogs_20191112_index.md\":\"e32acb02\",\"zh_blogs_20210120_index.md\":\"911eaf62\",\"zh_blogs_20200818_index.md\":\"8e097e80\",\"zh_blogs_20210203_index.md\":\"687380b2\",\"zh_blogs_20200616_index.md\":\"ae3433e1\",\"zh_blogs_20191109_index.md\":\"c638b4f7\",\"zh_blogs_20200816_index.md\":\"c6ddffa1\",\"zh_blogs_20200817_index.md\":\"288c87ef\",\"zh_blogs_20201023_index.md\":\"6fc987e2\",\"zh_blogs_20210204_index.md\":\"386f1179\",\"zh_blogs_20210123_index.md\":\"9c0d696b\",\"zh_blogs_20210223_index.md\":\"d3de7905\",\"zh_blogs_20210226_index.md\":\"b9dcbe95\",\"zh_blogs_20210311_index.md\":\"5b55f438\",\"zh_blogs_20210312_index.md\":\"763ac0c4\",\"zh_blogs_20210310_index.md\":\"66a3e7f3\",\"zh_blogs_20210224_index.md\":\"2adec6f7\",\"zh_blogs_20210315_index.md\":\"04f8e317\",\"zh_blogs_20191103_index.md\":\"59eb7951\",\"zh_blogs_20210430_index.md\":\"9db60c78\",\"zh_blogs_20210506_index.md\":\"3999d27e\",\"zh_blogs_20210329_index.md\":\"937369d7\",\"zh_blogs_20210801_index.md\":\"fd5bfe37\",\"zh_blogs_20210706_index.md\":\"2d7d2255\",\"zh_blogs_20210822_index.md\":\"5b9cfcec\",\"zh_blogs_20211130_index.md\":\"7c686de3\",\"zh_blogs_20220101_index.md\":\"1d37974c\",\"zh_blogs_20220611_index.md\":\"ede22e3f\",\"zh_blogs_20220224_index.md\":\"edfc1449\",\"zh_blogs_20221024_index.md\":\"d5f809aa\",\"zh_blogs_20211210_index.md\":\"9bc56a72\",\"zh_blogs_20220105_index.md\":\"438c05d9\",\"zh_blogs_20230101_index.md\":\"8b5bc229\",\"zh_blogs_20230125_index.md\":\"12e383e5\",\"zh_blogs_20221108_index.md\":\"5176bef5\",\"zh_blogs_20220316_index.md\":\"8f7ddd8f\",\"zh_blogs_20230201_repost.md\":\"9b8d567a\",\"zh_blogs_20210412_index.md\":\"72a83e78\",\"zh_blogs_20230209_index.md\":\"959471da\",\"zh_blogs_20210627_index.md\":\"ce4a3688\",\"zh_blogs_20211002_index.md\":\"0b6fa73c\",\"zh_blogs_20210728_index.md\":\"dba72473\",\"zh_blogs_20240518_index.md\":\"dd571dd5\",\"zh_blogs_20230322_index.md\":\"caad3bde\",\"zh_blogs_20240215_index.md\":\"619c2d85\",\"zh_blogs_20240128_index.md\":\"fc6b4359\",\"zh_blogs_20240220_index.md\":\"de838bfe\",\"zh_blogs_20240413_index.md\":\"74d611fe\",\"zh_blogs_20240423_index.md\":\"e118303d\",\"zh_blogs_20210715_index.md\":\"e5ea5b37\",\"zh_blogs_20240622_index.md\":\"541014de\",\"zh_index.md\":\"42216b06\",\"zh_blogs_index.md\":\"53e9e723\",\"zh_blogs_20210409_index.md\":\"530abc68\",\"zh_blogs_20240620_index.md\":\"9f3654f6\",\"zh_blogs_tags_index.md\":\"aece7df4\",\"zh_blogs_20230121_index.md\":\"9054d712\",\"zh_blogs_20230201_index.md\":\"a44bae4d\",\"zh_blogs_20230126_index.md\":\"1ed6070a\",\"zh_blogs_20211120_index.md\":\"90831aca\",\"zh_blogs_20240526_index.md\":\"876fde63\",\"zh_blogs_20221228_index.md\":\"0e86a4cc\",\"zh_blogs_20230601_index.md\":\"4be04903\",\"zh_blogs_20240623_index.md\":\"d740b350\",\"zh_blogs_20240519_index.md\":\"037f8dc5\",\"zh_blogs_20240427_index.md\":\"76f6ea87\",\"zh_blogs_20240513_index.md\":\"ba200ba4\"}")</script>
    <script type="module" async src="/assets/app.7e53e423.js"></script>
    
  </body>
</html>