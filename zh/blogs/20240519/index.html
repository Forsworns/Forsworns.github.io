<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>PyTorch2 论文笔记 | Sharlayan</title>
    <meta name="description" content="Pytorch2 Recap">
    <link rel="preload stylesheet" href="/assets/style.3ef9b918.css" as="style">
    <link rel="modulepreload" href="/assets/chunks/VPAlgoliaSearchBox.960fd572.js">
    <link rel="modulepreload" href="/assets/app.73f81c81.js">
    <link rel="modulepreload" href="/assets/zh_blogs_20240519_index.md.3935f5d4.lean.js">
    
    <script src="https://cdn.jsdelivr.net/gh/stevenjoezhang/live2d-widget@latest/autoload.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/4.0.0/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.staticfile.org/ionicons/2.0.1/css/ionicons.min.css">
  <link rel="icon" type="image/png" href="/logo.png">
  <meta name="author" content="Peihao Yang">
  <meta property="og:title" content="Home">
  <meta property="og:description" content="Home of Peihao Yang">
  <script id="check-dark-light">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
  </head>
  <body>
    <div id="app"><!--[--><div class="Layout" data-v-b617430f><!--[--><!--]--><!--[--><span tabindex="-1" data-v-d4120332></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-d4120332> Skip to content </a><!--]--><!----><header class="VPNav no-sidebar" data-v-b617430f data-v-aa1cde23><div class="VPNavBar" data-v-aa1cde23 data-v-cbdd8588><div class="container" data-v-cbdd8588><div class="title" data-v-cbdd8588><div class="VPNavBarTitle" data-v-cbdd8588 data-v-730d6dd1><a class="title" href="/zh/" data-v-730d6dd1><!--[--><!--]--><!--[--><img class="VPImage logo" src="/assets/logo.png" alt data-v-0f13a436><!--]--><!----><!--[--><!--]--></a></div></div><div class="content" data-v-cbdd8588><div class="curtain" data-v-cbdd8588></div><!--[--><!--]--><div class="VPNavBarSearch search" data-v-cbdd8588 style="--699c4559:&#39;Meta&#39;;"><div id="docsearch"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg class="DocSearch-Search-Icon" width="20" height="20" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-cbdd8588 data-v-2d3a777e><span id="main-nav-aria-label" class="visually-hidden" data-v-2d3a777e>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/" data-v-2d3a777e data-v-f559a019 data-v-1a0f9836><!--[-->🏡主页<!--]--><!----></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/about-me/" data-v-2d3a777e data-v-f559a019 data-v-1a0f9836><!--[-->🦹‍♂️关于我<!--]--><!----></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-2d3a777e data-v-fc33d832><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-fc33d832><span class="text" data-v-fc33d832><!----> 📓博客 <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-fc33d832><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-fc33d832><div class="VPMenu" data-v-fc33d832 data-v-ecf4e7d9><div class="items" data-v-ecf4e7d9><!--[--><!--[--><div class="VPMenuLink" data-v-ecf4e7d9 data-v-c7da634f><a class="VPLink link" href="/zh/blogs/" data-v-c7da634f data-v-1a0f9836><!--[-->📃所有博客<!--]--><!----></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-ecf4e7d9 data-v-c7da634f><a class="VPLink link" href="/zh/blogs/tags/" data-v-c7da634f data-v-1a0f9836><!--[-->🔖标签分类<!--]--><!----></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="https://forsworns.github.io/feed.xml" target="_blank" rel="noreferrer" data-v-2d3a777e data-v-f559a019 data-v-1a0f9836><!--[-->🔥RSS<!--]--><!----></a><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-cbdd8588 data-v-7cdc304e data-v-fc33d832><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-fc33d832><span class="text" data-v-fc33d832><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="option-icon" data-v-fc33d832><path d="M0 0h24v24H0z" fill="none"></path><path d=" M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z " class="css-c4d79v"></path></svg>  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-fc33d832><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-fc33d832><div class="VPMenu" data-v-fc33d832 data-v-ecf4e7d9><!----><!--[--><!--[--><div class="items" data-v-7cdc304e><p class="title" data-v-7cdc304e>中文</p><!--[--><div class="VPMenuLink" data-v-7cdc304e data-v-c7da634f><a class="VPLink link" href="/" data-v-c7da634f data-v-1a0f9836><!--[-->English<!--]--><!----></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-cbdd8588 data-v-7f24c201><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-7f24c201 data-v-2b897f09 data-v-ab7cdc04><span class="check" data-v-ab7cdc04><span class="icon" data-v-ab7cdc04><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-2b897f09><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-2b897f09><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-cbdd8588 data-v-a5afa74d data-v-80c99471><!--[--><a class="VPSocialLink" href="https://github.com/forsworns/blog-vitepress" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><a class="VPSocialLink" href="mailto:peihao.young@gmail.com" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="20">
            <path d="M874.666667 375.189333V746.666667a64 64 0 0 1-64 64H213.333333a64 64 0 0 1-64-64V375.189333l266.090667 225.6a149.333333 149.333333 0 0 0 193.152 0L874.666667 375.189333zM810.666667 213.333333a64.789333 64.789333 0 0 1 22.826666 4.181334 63.616 63.616 0 0 1 26.794667 19.413333 64.32 64.32 0 0 1 9.344 15.466667c2.773333 6.570667 4.48 13.696 4.906667 21.184L874.666667 277.333333v21.333334L553.536 572.586667a64 64 0 0 1-79.893333 2.538666l-3.178667-2.56L149.333333 298.666667v-21.333334a63.786667 63.786667 0 0 1 35.136-57.130666A63.872 63.872 0 0 1 213.333333 213.333333h597.333334z" ></path>
            </svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-cbdd8588 data-v-e459e5dc data-v-fc33d832><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-fc33d832><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-fc33d832><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-fc33d832><div class="VPMenu" data-v-fc33d832 data-v-ecf4e7d9><!----><!--[--><!--[--><div class="group" data-v-e459e5dc><p class="trans-title" data-v-e459e5dc>中文</p><!--[--><div class="VPMenuLink" data-v-e459e5dc data-v-c7da634f><a class="VPLink link" href="/" data-v-c7da634f data-v-1a0f9836><!--[-->English<!--]--><!----></a></div><!--]--></div><div class="group" data-v-e459e5dc><div class="item appearance" data-v-e459e5dc><p class="label" data-v-e459e5dc>Appearance</p><div class="appearance-action" data-v-e459e5dc><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-e459e5dc data-v-2b897f09 data-v-ab7cdc04><span class="check" data-v-ab7cdc04><span class="icon" data-v-ab7cdc04><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-2b897f09><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-2b897f09><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="group" data-v-e459e5dc><div class="item social-links" data-v-e459e5dc><div class="VPSocialLinks social-links-list" data-v-e459e5dc data-v-80c99471><!--[--><a class="VPSocialLink" href="https://github.com/forsworns/blog-vitepress" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><a class="VPSocialLink" href="mailto:peihao.young@gmail.com" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="20">
            <path d="M874.666667 375.189333V746.666667a64 64 0 0 1-64 64H213.333333a64 64 0 0 1-64-64V375.189333l266.090667 225.6a149.333333 149.333333 0 0 0 193.152 0L874.666667 375.189333zM810.666667 213.333333a64.789333 64.789333 0 0 1 22.826666 4.181334 63.616 63.616 0 0 1 26.794667 19.413333 64.32 64.32 0 0 1 9.344 15.466667c2.773333 6.570667 4.48 13.696 4.906667 21.184L874.666667 277.333333v21.333334L553.536 572.586667a64 64 0 0 1-79.893333 2.538666l-3.178667-2.56L149.333333 298.666667v-21.333334a63.786667 63.786667 0 0 1 35.136-57.130666A63.872 63.872 0 0 1 213.333333 213.333333h597.333334z" ></path>
            </svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-cbdd8588 data-v-d6834c14><span class="container" data-v-d6834c14><span class="top" data-v-d6834c14></span><span class="middle" data-v-d6834c14></span><span class="bottom" data-v-d6834c14></span></span></button></div></div></div><!----></header><!----><!----><div class="VPContent" id="VPContent" data-v-b617430f data-v-f32377af><div class="VPDoc has-aside" data-v-f32377af data-v-1e970af1><div class="container" data-v-1e970af1><div class="aside" data-v-1e970af1><div class="aside-curtain" data-v-1e970af1></div><div class="aside-container" data-v-1e970af1><div class="aside-content" data-v-1e970af1><div class="VPDocAside" data-v-1e970af1 data-v-b1723386><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" data-v-b1723386 data-v-0980ba1d><div class="content" data-v-0980ba1d><div class="outline-marker" data-v-0980ba1d></div><div class="outline-title" data-v-0980ba1d>On this page</div><nav aria-labelledby="doc-outline-aria-label" data-v-0980ba1d><span class="visually-hidden" id="doc-outline-aria-label" data-v-0980ba1d> Table of Contents for current page </span><ul class="root" data-v-0980ba1d data-v-6f4caaf4><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-b1723386></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-1e970af1><div class="content-container" data-v-1e970af1><!--[--><!--]--><main class="main" data-v-1e970af1><div style="position:relative;" class="vp-doc _zh_blogs_20240519_index" data-v-1e970af1><div><nav class="table-of-contents"><ul><li><a href="#_1-摘要-导言">1 摘要/导言</a></li><li><a href="#_2-过去的-pytorch-图捕获方法">2 过去的 PyTorch 图捕获方法</a><ul><li><a href="#_2-1-torch-jit-trace">2.1 torch.jit.trace</a></li><li><a href="#_2-2-torch-jit-script">2.2 torch.jit.script</a></li><li><a href="#_2-3-lazy-tensor">2.3 Lazy Tensor</a></li><li><a href="#_2-4-torch-fx-symbolic-trace">2.4 torch.fx.symbolic_trace</a></li><li><a href="#_2-6-和-jax-中的图捕获的对比">2.6 和 JAX 中的图捕获的对比</a></li></ul></li><li><a href="#_3-torchdynamo-设计实现">3 TorchDynamo 设计实现</a><ul><li><a href="#_3-1-api">3.1 API</a></li><li><a href="#_3-2-cpython-frame-evaluation-hook">3.2 CPython Frame Evaluation Hook</a></li><li><a href="#_3-3-guards">3.3 Guards</a></li><li><a href="#_3-4-symbolic-evaluation">3.4 Symbolic Evaluation</a></li><li><a href="#_3-5-modeling-python-data-structures">3.5 Modeling Python Data Structures</a></li><li><a href="#_3-6-inlining-control-flow-and-closures">3.6 Inlining, Control Flow, and Closures</a></li><li><a href="#_3-7-mutation-and-side-effects">3.7 Mutation and Side Effects</a></li></ul></li><li><a href="#_4-torchinductor-设计实现">4 TorchInductor 设计实现</a><ul><li><a href="#_4-5-triton-代码生成">4.5 Triton 代码生成</a></li><li><a href="#_4-6-c-代码生成">4.6 C++ 代码生成</a></li><li><a href="#_4-7-wrapper-codegen">4.7 Wrapper Codegen</a></li><li><a href="#_4-8-相关的深度学习编译器">4.8 相关的深度学习编译器</a></li></ul></li></ul></nav><p>论文 &quot;PyTorch2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation&quot; 介绍了 PyTorch2 中引入的新 API，<a href="https://pytorch.org/docs/stable/torch.compiler.html#torch-compiler" target="_blank" rel="noreferrer">torch.compile</a> 背后的实现原理。它旨在解决中在 Pytorch 中构建计算图的问题，并最终通过编译技术加速代码执行。</p><p>torch.compile 基于以下底层技术：</p><ul><li><p>TorchDynamo（<code>torch._dynamo</code>）：内部API，基于 CPython 的特性，<a href="https://peps.python.org/pep-0523/" target="_blank" rel="noreferrer">PEP523 Frame Evaluation API</a>，以安全地捕获 PyTorch 计算图。</p></li><li><p>TorchInductor：默认的深度学习编译器，可以为多种加速器和后端生成快速的代码。要通过torch.compile实现加速，需要使用后端编译器。对于NVIDIA和AMD GPU，它利用OpenAI Triton作为关键构建块。</p></li><li><p>AOT （Ahead-Of-Time）Autograd：不仅捕获用户级代码，还捕获反向传播。这使得使用 TorchInductor 能够加速前向传播和反向传播。</p></li></ul><p>借助 torch.compile，Pytorch2 的算子数量也显著减少了，参见 <a href="https://pytorch.org/docs/stable/torch.compiler_ir.html" target="_blank" rel="noreferrer">https://pytorch.org/docs/stable/torch.compiler_ir.html</a></p><p><code>torch.compile</code> 的使用很简单，可以通过装饰器加在函数上</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;">@</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">compile</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">opt_foo2</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">y</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    a </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">sin</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    b </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">cos</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">y</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> a </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> b</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">opt_foo2</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">),</span><span style="color:#82AAFF;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">)))</span></span>
<span class="line"></span></code></pre></div><p>可以直接将 <code>torch.nn.Module</code> 作为参数</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">class</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">MyModule</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#FFCB6B;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#FFCB6B;">super</span><span style="color:#89DDFF;">().</span><span style="color:#82AAFF;">__init__</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">        self</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">lin</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Linear</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">100</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">forward</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">nn</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">functional</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">relu</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">lin</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">mod </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">MyModule</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">opt_mod </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">compile</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">mod</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">opt_mod</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">100</span><span style="color:#89DDFF;">)))</span></span>
<span class="line"></span></code></pre></div><p>或者直接写 <a href="https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html" target="_blank" rel="noreferrer">triton</a> 算子</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> triton</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">from</span><span style="color:#A6ACCD;"> triton </span><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> language </span><span style="color:#89DDFF;font-style:italic;">as</span><span style="color:#A6ACCD;"> tl</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;">@</span><span style="color:#82AAFF;">triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">autotune</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">configs</span><span style="color:#89DDFF;">=[</span></span>
<span class="line"><span style="color:#82AAFF;">        triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Config</span><span style="color:#89DDFF;">({</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">BLOCK_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">},</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_stages</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">3</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_warps</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">8</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Config</span><span style="color:#89DDFF;">({</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">BLOCK_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">},</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_stages</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_warps</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Config</span><span style="color:#89DDFF;">({</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">BLOCK_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">},</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_stages</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">3</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_warps</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">8</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">Config</span><span style="color:#89DDFF;">({</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">BLOCK_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">:</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">2</span><span style="color:#89DDFF;">},</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_stages</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">num_warps</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#89DDFF;">],</span></span>
<span class="line"><span style="color:#82AAFF;">    </span><span style="color:#A6ACCD;font-style:italic;">key</span><span style="color:#89DDFF;">=[],</span></span>
<span class="line"><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#89DDFF;">@</span><span style="color:#82AAFF;">triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">jit</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">add_kernel_autotuned</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#A6ACCD;font-style:italic;">in_ptr0</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#A6ACCD;font-style:italic;">in_ptr1</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#A6ACCD;font-style:italic;">out_ptr</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#A6ACCD;font-style:italic;">n_elements</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#A6ACCD;font-style:italic;">BLOCK_SIZE</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">tl.constexpr</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    pid </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> tl</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">program_id</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">axis</span><span style="color:#89DDFF;">=</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    block_start </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> pid </span><span style="color:#89DDFF;">*</span><span style="color:#A6ACCD;"> BLOCK_SIZE</span></span>
<span class="line"><span style="color:#A6ACCD;">    offsets </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> block_start </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> tl</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">arange</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> BLOCK_SIZE</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    mask </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> offsets </span><span style="color:#89DDFF;">&lt;</span><span style="color:#A6ACCD;"> n_elements</span></span>
<span class="line"><span style="color:#A6ACCD;">    x </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> tl</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">load</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">in_ptr0 </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> offsets</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">mask</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">mask</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    y </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> tl</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">load</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">in_ptr1 </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> offsets</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">mask</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">mask</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    output </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> x </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> y</span></span>
<span class="line"><span style="color:#A6ACCD;">    tl</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">store</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">out_ptr </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> offsets</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> output</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">mask</span><span style="color:#89DDFF;">=</span><span style="color:#82AAFF;">mask</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;">@</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">compile</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">fullgraph</span><span style="color:#89DDFF;">=True)</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">add_fn</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">y</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    output </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">zeros_like</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    n_elements </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> output</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">numel</span><span style="color:#89DDFF;">()</span></span>
<span class="line"><span style="color:#A6ACCD;">    grid </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">lambda</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">meta</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">triton</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">cdiv</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">n_elements</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> meta</span><span style="color:#89DDFF;">[</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">BLOCK_SIZE</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">]),)</span></span>
<span class="line"><span style="color:#A6ACCD;">    add_kernel_autotuned</span><span style="color:#89DDFF;">[</span><span style="color:#A6ACCD;">grid</span><span style="color:#89DDFF;">](</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> y</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> output</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> n_elements</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> output</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">x </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">device</span><span style="color:#89DDFF;">=</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">y </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#A6ACCD;font-style:italic;">device</span><span style="color:#89DDFF;">=</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">out </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">add_fn</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> y</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">f</span><span style="color:#C3E88D;">&quot;Vector addition of</span><span style="color:#A6ACCD;">\n</span><span style="color:#C3E88D;">X:</span><span style="color:#A6ACCD;">\t</span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">x</span><span style="color:#F78C6C;">}</span><span style="color:#A6ACCD;">\n</span><span style="color:#C3E88D;">Y:</span><span style="color:#A6ACCD;">\t</span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">y</span><span style="color:#F78C6C;">}</span><span style="color:#A6ACCD;">\n</span><span style="color:#C3E88D;">is equal to</span><span style="color:#A6ACCD;">\n</span><span style="color:#F78C6C;">{</span><span style="color:#82AAFF;">out</span><span style="color:#F78C6C;">}</span><span style="color:#C3E88D;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><h1 id="论文笔记" tabindex="-1">论文笔记 <a class="header-anchor" href="#论文笔记" aria-hidden="true">#</a></h1><h2 id="_1-摘要-导言" tabindex="-1">1 摘要/导言 <a class="header-anchor" href="#_1-摘要-导言" aria-hidden="true">#</a></h2><p>作为 eager 模式的框架，pytorch 易于学习和调试，但是难以借助编译器实现图级别的优化。框架只能看到局部的算子信息，无法做算子融合和调度。已有的一些尝试，基于记录/重放、python 解析、懒执行等方法，会影响 pytorch 本身的易用性。记录/重放可能导致错误结果，python 解析无法处理复杂的 python 程序，懒执行的运行时开销太大，因此都不实际。</p><p>本文介绍了两个 PyTorch 的扩展，TorchDynamo 和 TorchInductor，它们实现了在 PyTorch2 中发布的 <code>torch.compile</code> 功能。TorchDynamo 是一个基于Python的即时编译器（JIT），它在不损失 Python 灵活性的情况下，使得 PyTorch 程序能够进行图编译。它挂载在 python 的 frame evaluation API 之上，通过在执行之前动态修改 Python 字节码，并将一系列 PyTorch 操作提取到一个 FX 图中来实现这一目标，然后使用可扩展的后端对其进行 JIT 编译。TorchInductor 是 TorchDynamo 的默认编译器后端，它将PyTorch 程序转换为 OpenAI 的 Triton（用于GPU）和 C++（用于CPU）。这些扩展为在 PyTorch 等 eager-mode 框架中通过编译器应用优化提供了一种新途径。</p><h2 id="_2-过去的-pytorch-图捕获方法" tabindex="-1">2 过去的 PyTorch 图捕获方法 <a class="header-anchor" href="#_2-过去的-pytorch-图捕获方法" aria-hidden="true">#</a></h2><p>eager 模式的框架，难点不仅在于执行时仅有局部信息，还在于它的代码中可以混淆任意 python 代码、第三方库，无法像基于图的框架，限制用户的行为。下面介绍下在实现 TorchDynamo 之前，pytorch 社区的尝试。</p><h3 id="_2-1-torch-jit-trace" tabindex="-1">2.1 torch.jit.trace <a class="header-anchor" href="#_2-1-torch-jit-trace" aria-hidden="true">#</a></h3><p><code>torch.jit.trace</code> 使用记录/重放的方法来构建 TorchScript 图。在 Pytorch dispatcher 的层面进行记录，dispatcher 是用来把算子转换成特定设备的核函数，以及用于自动微分的，用 C++ 实现。因为记录是实现在 C++ 层面，因此它无法捕获到 python 中的控制流。比如下面这个例子</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">example1</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">len</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">nonzero</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">))</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">&gt;</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">:</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> x </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">1</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> x </span><span style="color:#89DDFF;">-</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">1</span></span>
<span class="line"></span></code></pre></div><p>假设输入是 <code>tensor([0,0])</code>，它会捕获到一个等价于下面代码的图</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">example1_incorrect_capture</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">nonzero</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> x </span><span style="color:#89DDFF;">-</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">1</span></span>
<span class="line"></span></code></pre></div><p>显然输入换成 <code>tensor([1,1])</code>，捕获到的图就非法了。同时，python 代码中任意非 pytorch 的部分，也是无法捕获的，如三方库、日志打印、程序执行的副作用。</p><h3 id="_2-2-torch-jit-script" tabindex="-1">2.2 torch.jit.script <a class="header-anchor" href="#_2-2-torch-jit-script" aria-hidden="true">#</a></h3><p><code>torch.jit.script</code> 也用来构建 TorchScript 图，但是是通过解析 python 语法树进行静态分析，它能够正确捕获上面的 example1。但是问题是它在尝试将 python 整个实现成一个静态语言，遇到未实现的 python 组件将会导致它无法工作。它只支持部分模型，而且支持大些模型的工作量很大。</p><h3 id="_2-3-lazy-tensor" tabindex="-1">2.3 Lazy Tensor <a class="header-anchor" href="#_2-3-lazy-tensor" aria-hidden="true">#</a></h3><p>Pytorch/XLA 使用这种方法，它是一个 C++ 层面的方法，每轮迭代它都会延后算子执行，累积成一个图，然后将图喂给 XLA 编译器。它会对图做哈希以避免重复编译。它有效而且通用性强，但是运行时开销大（额外维护图结构）、延迟高（不必要的 device/host 串行）、有时可能频繁触发重新编译。</p><p>目前 Pytorch/XLA 已经集成了 TorchDynamo，它不会在每轮迭代都采用 Lazy Tensor，同时借助 TorchDynamo 来判断是否需要重新捕获图。</p><h3 id="_2-4-torch-fx-symbolic-trace" tabindex="-1">2.4 torch.fx.symbolic_trace <a class="header-anchor" href="#_2-4-torch-fx-symbolic-trace" aria-hidden="true">#</a></h3><p><code>torch.fx.symbolic_trace</code> 也是一个基于记录/重放的图捕获技术，但是它工作在 python 层面，因此可以捕获到 python 中的条件判断。它借助一个代理对象来执行用户的代码，因此可以捕获到关于规模/值的读取行为，而不是像 <code>torch.jit.trace</code> 一样直接代入真实的张量值。它的图表示，<code>FX graph</code> 格式，也被 TorchDynamo 所采纳。</p><p>但是它无法处理下面的例子，</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">example3</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#C792EA;">global</span><span style="color:#A6ACCD;"> call_count</span></span>
<span class="line"><span style="color:#A6ACCD;">    call_count </span><span style="color:#89DDFF;">+=</span><span style="color:#A6ACCD;"> </span><span style="color:#F78C6C;">1</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">rand</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> x</span></span>
<span class="line"></span></code></pre></div><p>它会生成类似下面代码的图，随机数和全局副作用都丢失了，因为它们不会和 <code>x</code> 的代理变量交互。而且即使全局副作用被捕获了，下游的编译器也大多不支持它，因为机器学习的图结构中几乎都没有 python 中的全局变量的概念。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">example3_incorrect_capture</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> _tensor_constant0 </span><span style="color:#89DDFF;">+</span><span style="color:#A6ACCD;"> x</span></span>
<span class="line"></span></code></pre></div><h3 id="_2-6-和-jax-中的图捕获的对比" tabindex="-1">2.6 和 JAX 中的图捕获的对比 <a class="header-anchor" href="#_2-6-和-jax-中的图捕获的对比" aria-hidden="true">#</a></h3><p>JAX 天然是和 XLA 联合设计的，而且对用户程序也有限制，<code>jax.jit</code> 不支持依赖数据的 python 控制流，并且要求用户函数是纯函数（无副作用），因此它可以采用一种类似 <code>torch.fx.symbolic_trace</code> 的图捕获技术，并且还更简单。</p><p>与之相对，torch 一开始设计的时候没有编译器的概念，是一个仅支持 eager 模式的框架，已经有大量的模型基于它构建，换条路不现实了。</p><h2 id="_3-torchdynamo-设计实现" tabindex="-1">3 TorchDynamo 设计实现 <a class="header-anchor" href="#_3-torchdynamo-设计实现" aria-hidden="true">#</a></h2><p>TorchDynamo 是一个工作在 CPython 层面的 python 字节码 JIT 编译器。它将 python 字节码翻译到 python 字节码，只是会将原始的字节码中的 pytorch 运算替换成编译后的产物，从而实现 pytorch 的算子融合。下图是它的原理。</p><p><img src="/assets/fig1.caca1932.png" alt=""></p><h3 id="_3-1-api" tabindex="-1">3.1 API <a class="header-anchor" href="#_3-1-api" aria-hidden="true">#</a></h3><p>当使用 <code>torch.compile</code> 运行 pytorch <code>Module</code> 时，自定义的 CPython Frame Evaluation Hook （参见 3.2）将重写正在执行的每个Python 函数的字节码，以提取和编译 PyTorch 算子。这个字节码重写可以被缓存，因为它可能依赖于程序的某些动态属性，需要使用 guards（参见 3.3）来在后续调用中进行检查。</p><h3 id="_3-2-cpython-frame-evaluation-hook" tabindex="-1">3.2 CPython Frame Evaluation Hook <a class="header-anchor" href="#_3-2-cpython-frame-evaluation-hook" aria-hidden="true">#</a></h3><h3 id="_3-3-guards" tabindex="-1">3.3 Guards <a class="header-anchor" href="#_3-3-guards" aria-hidden="true">#</a></h3><h3 id="_3-4-symbolic-evaluation" tabindex="-1">3.4 Symbolic Evaluation <a class="header-anchor" href="#_3-4-symbolic-evaluation" aria-hidden="true">#</a></h3><h3 id="_3-5-modeling-python-data-structures" tabindex="-1">3.5 Modeling Python Data Structures <a class="header-anchor" href="#_3-5-modeling-python-data-structures" aria-hidden="true">#</a></h3><h3 id="_3-6-inlining-control-flow-and-closures" tabindex="-1">3.6 Inlining, Control Flow, and Closures <a class="header-anchor" href="#_3-6-inlining-control-flow-and-closures" aria-hidden="true">#</a></h3><h3 id="_3-7-mutation-and-side-effects" tabindex="-1">3.7 Mutation and Side Effects <a class="header-anchor" href="#_3-7-mutation-and-side-effects" aria-hidden="true">#</a></h3><h2 id="_4-torchinductor-设计实现" tabindex="-1">4 TorchInductor 设计实现 <a class="header-anchor" href="#_4-torchinductor-设计实现" aria-hidden="true">#</a></h2><h3 id="_4-5-triton-代码生成" tabindex="-1">4.5 Triton 代码生成 <a class="header-anchor" href="#_4-5-triton-代码生成" aria-hidden="true">#</a></h3><p>Triton codegen 负责将 TorchInductor 的 IR 映射到 Triton 核函数。下图显示了上述 log2 示例生成的代码。</p><p><img src="/assets/fig3.234de2e1.png" alt=""></p><p>该核函数一次处理 <code>XBLOCK</code> 个元素的块。如果元素的数量不是<code>XBLOCK</code> 的倍数，则末尾可能会有一些元素被屏蔽。在代码生成过程中，我们简化了索引。例如，在 IR 中，2D strided 加载被转换为连续加载。代码生成还负责公共子表达式消除（CSE），通过生成代码时使用缓存，并分配以 tmp 开头的临时变量名。pointwise修饰符用于简化启发式块大小、自动调优和预先编译核函数的样板代码。修饰符是正在生成的核函数类型（pointwise、reduction或template），其参数是核函数的必需元数据，例如数据对齐方式。</p><p>在生成 reduction 核函数时，TorchInductor有两种代码生成模式。对于较小的 reduction 操作，它将生成持久 reduction，整个reduction加载到单个块中并在寄存器/共享内存中保留；在这种情况下，reduction 直接映射到 Triton 的 reduction 操作符。对于较大的 reduction，TorchInductor 生成一个循环，将整个块用作累加器，并在循环结束时调用 Triton reduction。</p><p>对于更复杂的操作（矩阵乘法和卷积），TorchInductor 有自己的模板系统，用于生成混合手写 Triton 和生成的 Triton 代码。模板使用 Jinja 编写，与 TorchInductor 的代码生成系统进行交互。</p><h3 id="_4-6-c-代码生成" tabindex="-1">4.6 C++ 代码生成 <a class="header-anchor" href="#_4-6-c-代码生成" aria-hidden="true">#</a></h3><p>CPU 后端会对应到 C++ 代码和 OpenMP。C++ 代码会有向量化和非向量化的实现。向量化的实现会讲大多数运算映射到 pytorch 的<code>at::vec::Vectorized</code> 类。这个类一次会处理 16 个元素，也是标准的 pytorch 核函数调用 SIMD 指令集的方式。非向量化的实现就是转换到 STL 标准库。两个实现都会尝试展开部分循环以实现并行。</p><h3 id="_4-7-wrapper-codegen" tabindex="-1">4.7 Wrapper Codegen <a class="header-anchor" href="#_4-7-wrapper-codegen" aria-hidden="true">#</a></h3><p>Wrapper Codegen 指的是生成调用核函数的代码，它负责计算张量规模、管理显存分配。有两个实现，一个生成 python 代码，另一个生成 C++ 代码，python 后延更加灵活，支持一些边界情况，C++ 代码开销更小。</p><p>当 <code>torch.compile</code> 指定 <code>mode=&quot;reduce-overhead&quot;</code> 时，TrochInductor 会尝试使用 CUDA Graph 来消除 wrapper 代码的开销，基于 API <code>cuStreamBeginCapture</code> 和 <code>cuStreamEndCapture</code>，属于 CUDA 自动图捕获。如果是张量是动态规模，或者不是 CUDA 张量的话，就放弃借助 CUDA Graph。</p><p>注意这里是 CUDA API 层面的图，和上面讲的算子层面的图不同。</p><h3 id="_4-8-相关的深度学习编译器" tabindex="-1">4.8 相关的深度学习编译器 <a class="header-anchor" href="#_4-8-相关的深度学习编译器" aria-hidden="true">#</a></h3><p>pytorch 选用了 OpenAI 的 Triton，因为它可以生成比手写库还快的核函数，大多编译器甚至没有考虑过这方面的工作，面对复杂算子会直接调用手写库。</p><h1 id="cuda-graph-相关缺陷" tabindex="-1">CUDA Graph 相关缺陷 <a class="header-anchor" href="#cuda-graph-相关缺陷" aria-hidden="true">#</a></h1><p><a href="https://pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html" target="_blank" rel="noreferrer">https://pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html</a></p><p>由于 <a href="https://developer.nvidia.com/blog/cuda-graphs/" target="_blank" rel="noreferrer">CUDA Graph</a> 使用了确定性的显存地址，所以前面的执行结果会被后续的执行结果覆盖。也就是说下面的用例会出错</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">import</span><span style="color:#A6ACCD;"> torch</span></span>
<span class="line"></span>
<span class="line"><span style="color:#89DDFF;">@</span><span style="color:#82AAFF;">torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">compile</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">mode</span><span style="color:#89DDFF;">=</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">reduce-overhead</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">my_model</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">x</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    y </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">matmul</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> y</span></span>
<span class="line"></span>
<span class="line"><span style="color:#A6ACCD;">x </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> torch</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">randn</span><span style="color:#89DDFF;">(</span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">10</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">y1 </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">my_model</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">y2 </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">my_model</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">x</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#82AAFF;">print</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">y1</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;"># RuntimeError: Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run.</span></span>
<span class="line"></span></code></pre></div><p>Pytorch 在实现 <code>torch.compile</code> 的时候，采用了一种启发式的方法来避免这个问题。在推理过程中会在每次调用 <code>torch.compile</code> 时开始新的迭代；在训练过程中也是如此，只要没有未调用的待处理反向传播。如果这些启发式算法不正确，可以使用 <code>torch.compiler.mark_step_begin()</code> 标记开始新的迭代，或在开始下一次运行之前（在 <code>torch.compile</code> 之外）克隆上一次迭代的张量。</p></div></div></main><!--[--><!--]--><!----><!--[--><!--[--><!--[--><!----><!--]--><!--]--><!--]--></div></div></div></div></div><!----><!--[--><!--]--></div><!----><footer data-v-4f0db67d> Powered by <a href="https://github.com/forsworns/" target="_blank" title="Author" data-v-4f0db67d>Peihao Yang</a> | Copyright © 2019-2024 | MIT License </footer><!--]--></div>
    <script>__VP_HASH_MAP__ = JSON.parse("{\"about-me_index.md\":\"846ea80d\",\"index.md\":\"e0ba6ad2\",\"zh_about-me_index.md\":\"3ebfac3a\",\"zh_blogs_20190721_index.md\":\"9a1992f7\",\"zh_blogs_20190824_index.md\":\"c631fdf0\",\"zh_blogs_20190901_index.md\":\"506c415e\",\"zh_blogs_20190919_index.md\":\"a9ef4728\",\"zh_blogs_20190908_index.md\":\"0ae17c24\",\"zh_blogs_20191112_index.md\":\"52603db5\",\"zh_blogs_20200816_index.md\":\"634fb620\",\"zh_blogs_20191109_index.md\":\"80d0c840\",\"zh_blogs_20200616_index.md\":\"24a6c73e\",\"zh_blogs_20200817_index.md\":\"b22a8d7c\",\"zh_blogs_20210506_index.md\":\"8c999b21\",\"zh_blogs_20210310_index.md\":\"b02adbe1\",\"zh_blogs_20210224_index.md\":\"c500882b\",\"zh_blogs_20210226_index.md\":\"417a3ac5\",\"zh_blogs_20210312_index.md\":\"c84a09fb\",\"zh_blogs_20210315_index.md\":\"4e8d51fd\",\"zh_blogs_20210204_index.md\":\"922002e3\",\"zh_blogs_20210311_index.md\":\"7de1b4a4\",\"zh_blogs_20210329_index.md\":\"810ef071\",\"zh_blogs_20210430_index.md\":\"9c389873\",\"zh_blogs_20210223_index.md\":\"e7082a64\",\"zh_blogs_20210409_index.md\":\"8932f02b\",\"zh_blogs_20200818_index.md\":\"73c493e7\",\"zh_blogs_20210123_index.md\":\"dd264596\",\"zh_blogs_20210715_index.md\":\"8feff090\",\"zh_blogs_20210412_index.md\":\"56cbe3d4\",\"zh_blogs_20210203_index.md\":\"ae20c5bd\",\"zh_blogs_20210120_index.md\":\"46bc3a68\",\"zh_blogs_20210728_index.md\":\"24673902\",\"zh_blogs_20201023_index.md\":\"4c94be96\",\"zh_blogs_20210706_index.md\":\"3a568024\",\"zh_blogs_20210822_index.md\":\"ad67ffd7\",\"zh_blogs_20210627_index.md\":\"9d791080\",\"zh_blogs_20210801_index.md\":\"18df2595\",\"zh_blogs_20191102_index.md\":\"53b363d5\",\"zh_blogs_20211002_index.md\":\"d511de03\",\"zh_blogs_20211210_index.md\":\"fd74a440\",\"zh_blogs_20220101_index.md\":\"ee91e215\",\"zh_blogs_20220224_index.md\":\"cddc3efe\",\"zh_blogs_20220105_index.md\":\"273f914b\",\"zh_blogs_20211120_index.md\":\"75f05041\",\"zh_blogs_20211130_index.md\":\"c8bce327\",\"zh_blogs_20191103_index.md\":\"816338e7\",\"zh_blogs_20221024_index.md\":\"28646e28\",\"zh_blogs_20220611_index.md\":\"c429e3ae\",\"zh_blogs_20220316_index.md\":\"92bf9247\",\"zh_blogs_20230121_index.md\":\"0c062d60\",\"zh_blogs_20230101_index.md\":\"8bcd8a38\",\"zh_blogs_20230126_index.md\":\"0b38611b\",\"zh_blogs_20230125_index.md\":\"b6270011\",\"zh_blogs_20221228_index.md\":\"b21decf6\",\"zh_blogs_20221108_index.md\":\"1ecb2c5e\",\"zh_blogs_20230201_index.md\":\"d65a7769\",\"zh_blogs_20240513_index.md\":\"56ad3f14\",\"zh_blogs_20230322_index.md\":\"8838bc28\",\"zh_blogs_20230201_repost.md\":\"d6a8bf08\",\"zh_blogs_20230209_index.md\":\"3a12ac32\",\"zh_blogs_20240526_index.md\":\"3f2d8a25\",\"zh_blogs_20240220_index.md\":\"6d320fea\",\"zh_blogs_20240620_index.md\":\"69e37ac2\",\"zh_blogs_20240622_index.md\":\"e693b38c\",\"zh_blogs_20240519_index.md\":\"3935f5d4\",\"zh_blogs_20240215_index.md\":\"4139d8e8\",\"zh_blogs_20240128_index.md\":\"6e1d760d\",\"zh_blogs_20240423_index.md\":\"6c1114d2\",\"zh_blogs_20240623_index.md\":\"c3cf8349\",\"zh_blogs_20240518_index.md\":\"d0555774\",\"zh_index.md\":\"8b832df8\",\"zh_blogs_20230601_index.md\":\"64d86526\",\"zh_blogs_20240626_index.md\":\"c36ba82d\",\"zh_blogs_20240413_index.md\":\"3307932c\",\"zh_blogs_20240427_index.md\":\"ab744de2\",\"zh_blogs_index.md\":\"a0e1cf19\",\"zh_blogs_tags_index.md\":\"95f669eb\"}")</script>
    <script type="module" async src="/assets/app.73f81c81.js"></script>
    
  </body>
</html>