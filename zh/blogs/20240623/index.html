<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>ollama/llama.cpp 源码阅读 | Sharlayan</title>
    <meta name="description" content="最开始是想看下为什么 cuda graph 没有被启用">
    <link rel="preload stylesheet" href="/assets/style.3ef9b918.css" as="style">
    <link rel="modulepreload" href="/assets/chunks/VPAlgoliaSearchBox.960fd572.js">
    <link rel="modulepreload" href="/assets/app.73f81c81.js">
    <link rel="modulepreload" href="/assets/zh_blogs_20240623_index.md.c3cf8349.lean.js">
    
    <script src="https://cdn.jsdelivr.net/gh/stevenjoezhang/live2d-widget@latest/autoload.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/4.0.0/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.staticfile.org/ionicons/2.0.1/css/ionicons.min.css">
  <link rel="icon" type="image/png" href="/logo.png">
  <meta name="author" content="Peihao Yang">
  <meta property="og:title" content="Home">
  <meta property="og:description" content="Home of Peihao Yang">
  <script id="check-dark-light">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
  </head>
  <body>
    <div id="app"><!--[--><div class="Layout" data-v-b617430f><!--[--><!--]--><!--[--><span tabindex="-1" data-v-d4120332></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-d4120332> Skip to content </a><!--]--><!----><header class="VPNav no-sidebar" data-v-b617430f data-v-aa1cde23><div class="VPNavBar" data-v-aa1cde23 data-v-cbdd8588><div class="container" data-v-cbdd8588><div class="title" data-v-cbdd8588><div class="VPNavBarTitle" data-v-cbdd8588 data-v-730d6dd1><a class="title" href="/zh/" data-v-730d6dd1><!--[--><!--]--><!--[--><img class="VPImage logo" src="/assets/logo.png" alt data-v-0f13a436><!--]--><!----><!--[--><!--]--></a></div></div><div class="content" data-v-cbdd8588><div class="curtain" data-v-cbdd8588></div><!--[--><!--]--><div class="VPNavBarSearch search" data-v-cbdd8588 style="--699c4559:&#39;Meta&#39;;"><div id="docsearch"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg class="DocSearch-Search-Icon" width="20" height="20" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-cbdd8588 data-v-2d3a777e><span id="main-nav-aria-label" class="visually-hidden" data-v-2d3a777e>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/" data-v-2d3a777e data-v-f559a019 data-v-1a0f9836><!--[-->🏡主页<!--]--><!----></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/about-me/" data-v-2d3a777e data-v-f559a019 data-v-1a0f9836><!--[-->🦹‍♂️关于我<!--]--><!----></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-2d3a777e data-v-fc33d832><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-fc33d832><span class="text" data-v-fc33d832><!----> 📓博客 <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-fc33d832><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-fc33d832><div class="VPMenu" data-v-fc33d832 data-v-ecf4e7d9><div class="items" data-v-ecf4e7d9><!--[--><!--[--><div class="VPMenuLink" data-v-ecf4e7d9 data-v-c7da634f><a class="VPLink link" href="/zh/blogs/" data-v-c7da634f data-v-1a0f9836><!--[-->📃所有博客<!--]--><!----></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-ecf4e7d9 data-v-c7da634f><a class="VPLink link" href="/zh/blogs/tags/" data-v-c7da634f data-v-1a0f9836><!--[-->🔖标签分类<!--]--><!----></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="https://forsworns.github.io/feed.xml" target="_blank" rel="noreferrer" data-v-2d3a777e data-v-f559a019 data-v-1a0f9836><!--[-->🔥RSS<!--]--><!----></a><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-cbdd8588 data-v-7cdc304e data-v-fc33d832><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-fc33d832><span class="text" data-v-fc33d832><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="option-icon" data-v-fc33d832><path d="M0 0h24v24H0z" fill="none"></path><path d=" M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z " class="css-c4d79v"></path></svg>  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-fc33d832><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-fc33d832><div class="VPMenu" data-v-fc33d832 data-v-ecf4e7d9><!----><!--[--><!--[--><div class="items" data-v-7cdc304e><p class="title" data-v-7cdc304e>中文</p><!--[--><div class="VPMenuLink" data-v-7cdc304e data-v-c7da634f><a class="VPLink link" href="/" data-v-c7da634f data-v-1a0f9836><!--[-->English<!--]--><!----></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-cbdd8588 data-v-7f24c201><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-7f24c201 data-v-2b897f09 data-v-ab7cdc04><span class="check" data-v-ab7cdc04><span class="icon" data-v-ab7cdc04><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-2b897f09><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-2b897f09><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-cbdd8588 data-v-a5afa74d data-v-80c99471><!--[--><a class="VPSocialLink" href="https://github.com/forsworns/blog-vitepress" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><a class="VPSocialLink" href="mailto:peihao.young@gmail.com" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="20">
            <path d="M874.666667 375.189333V746.666667a64 64 0 0 1-64 64H213.333333a64 64 0 0 1-64-64V375.189333l266.090667 225.6a149.333333 149.333333 0 0 0 193.152 0L874.666667 375.189333zM810.666667 213.333333a64.789333 64.789333 0 0 1 22.826666 4.181334 63.616 63.616 0 0 1 26.794667 19.413333 64.32 64.32 0 0 1 9.344 15.466667c2.773333 6.570667 4.48 13.696 4.906667 21.184L874.666667 277.333333v21.333334L553.536 572.586667a64 64 0 0 1-79.893333 2.538666l-3.178667-2.56L149.333333 298.666667v-21.333334a63.786667 63.786667 0 0 1 35.136-57.130666A63.872 63.872 0 0 1 213.333333 213.333333h597.333334z" ></path>
            </svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-cbdd8588 data-v-e459e5dc data-v-fc33d832><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-fc33d832><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-fc33d832><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-fc33d832><div class="VPMenu" data-v-fc33d832 data-v-ecf4e7d9><!----><!--[--><!--[--><div class="group" data-v-e459e5dc><p class="trans-title" data-v-e459e5dc>中文</p><!--[--><div class="VPMenuLink" data-v-e459e5dc data-v-c7da634f><a class="VPLink link" href="/" data-v-c7da634f data-v-1a0f9836><!--[-->English<!--]--><!----></a></div><!--]--></div><div class="group" data-v-e459e5dc><div class="item appearance" data-v-e459e5dc><p class="label" data-v-e459e5dc>Appearance</p><div class="appearance-action" data-v-e459e5dc><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-e459e5dc data-v-2b897f09 data-v-ab7cdc04><span class="check" data-v-ab7cdc04><span class="icon" data-v-ab7cdc04><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-2b897f09><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-2b897f09><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="group" data-v-e459e5dc><div class="item social-links" data-v-e459e5dc><div class="VPSocialLinks social-links-list" data-v-e459e5dc data-v-80c99471><!--[--><a class="VPSocialLink" href="https://github.com/forsworns/blog-vitepress" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><a class="VPSocialLink" href="mailto:peihao.young@gmail.com" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="20">
            <path d="M874.666667 375.189333V746.666667a64 64 0 0 1-64 64H213.333333a64 64 0 0 1-64-64V375.189333l266.090667 225.6a149.333333 149.333333 0 0 0 193.152 0L874.666667 375.189333zM810.666667 213.333333a64.789333 64.789333 0 0 1 22.826666 4.181334 63.616 63.616 0 0 1 26.794667 19.413333 64.32 64.32 0 0 1 9.344 15.466667c2.773333 6.570667 4.48 13.696 4.906667 21.184L874.666667 277.333333v21.333334L553.536 572.586667a64 64 0 0 1-79.893333 2.538666l-3.178667-2.56L149.333333 298.666667v-21.333334a63.786667 63.786667 0 0 1 35.136-57.130666A63.872 63.872 0 0 1 213.333333 213.333333h597.333334z" ></path>
            </svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-cbdd8588 data-v-d6834c14><span class="container" data-v-d6834c14><span class="top" data-v-d6834c14></span><span class="middle" data-v-d6834c14></span><span class="bottom" data-v-d6834c14></span></span></button></div></div></div><!----></header><!----><!----><div class="VPContent" id="VPContent" data-v-b617430f data-v-f32377af><div class="VPDoc has-aside" data-v-f32377af data-v-1e970af1><div class="container" data-v-1e970af1><div class="aside" data-v-1e970af1><div class="aside-curtain" data-v-1e970af1></div><div class="aside-container" data-v-1e970af1><div class="aside-content" data-v-1e970af1><div class="VPDocAside" data-v-1e970af1 data-v-b1723386><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" data-v-b1723386 data-v-0980ba1d><div class="content" data-v-0980ba1d><div class="outline-marker" data-v-0980ba1d></div><div class="outline-title" data-v-0980ba1d>On this page</div><nav aria-labelledby="doc-outline-aria-label" data-v-0980ba1d><span class="visually-hidden" id="doc-outline-aria-label" data-v-0980ba1d> Table of Contents for current page </span><ul class="root" data-v-0980ba1d data-v-6f4caaf4><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-b1723386></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-1e970af1><div class="content-container" data-v-1e970af1><!--[--><!--]--><main class="main" data-v-1e970af1><div style="position:relative;" class="vp-doc _zh_blogs_20240623_index" data-v-1e970af1><div><nav class="table-of-contents"><ul><li><a href="#ollama">ollama</a><ul><li><a href="#llm">llm/</a></li></ul></li><li><a href="#llama-cpp">llama.cpp</a><ul><li><a href="#llama-cpp-1">llama.cpp</a></li><li><a href="#ggml-c">ggml.c</a></li><li><a href="#ggml-backend-c">ggml-backend.c</a></li><li><a href="#ggml-blas-cpp">ggml-blas.cpp</a></li><li><a href="#ggml-cuda-cu">ggml-cuda.cu</a></li><li><a href="#ggml-cuda">ggml-cuda</a></li><li><a href="#examples">examples</a></li></ul></li></ul></nav><h2 id="ollama" tabindex="-1">ollama <a class="header-anchor" href="#ollama" aria-hidden="true">#</a></h2><p>ollama基于 <a href="https://github.com/ollama/ollama/blob/ccef9431c8aae4ecfd0eec6e10377d09cb42f634" target="_blank" rel="noreferrer">https://github.com/ollama/ollama/blob/ccef9431c8aae4ecfd0eec6e10377d09cb42f634</a></p><h3 id="llm" tabindex="-1">llm/ <a class="header-anchor" href="#llm" aria-hidden="true">#</a></h3><h4 id="server-go" tabindex="-1">server.go <a class="header-anchor" href="#server-go" aria-hidden="true">#</a></h4><p>go 写的 server，主体为 <a href="https://github.com/ollama/ollama/blob/ccef9431c8aae4ecfd0eec6e10377d09cb42f634/llm/server.go#L80" target="_blank" rel="noreferrer"><code>NewLlamaServer</code></a></p><p>它会拉起多个进程，分别执行下面的 ext_server/server.cpp 中，基于 llama.cpp 实现的真正做推理服务的 server。</p><h4 id="ext-server-server-cpp" tabindex="-1">ext_server/server.cpp <a class="header-anchor" href="#ext-server-server-cpp" aria-hidden="true">#</a></h4><p>把 llama.cpp 导入成了一个 submodule，基于 llama.cpp 开发的一个推理服务器。</p><h2 id="llama-cpp" tabindex="-1">llama.cpp <a class="header-anchor" href="#llama-cpp" aria-hidden="true">#</a></h2><p>llama.cpp 基于 <a href="https://github1s.com/ggerganov/llama.cpp/blob/45c0e2e4c1268c2d7c8c45536f15e3c9a731ecdc/llama.h" target="_blank" rel="noreferrer">https://github1s.com/ggerganov/llama.cpp/blob/45c0e2e4c1268c2d7c8c45536f15e3c9a731ecdc/llama.h</a></p><p>看编译脚本上是默认开 cuda graph 优化，但是用 ollama 起的服务器跑的时候没有用到。</p><p>CmakeLists.txt 里面声明了只要找到了 libcuda，就会定义 GGML_CUDA_USE_GRAPHS 开启 cuda graph 优化。Makefile 中同样，只要是声明了 <code>make LLAMA_CUDA=1</code> 就会定义 <code>GGML_CUDA_USE_GRAPHS</code> 开启 cuda graph 优化。</p><h3 id="llama-cpp-1" tabindex="-1">llama.cpp <a class="header-anchor" href="#llama-cpp-1" aria-hidden="true">#</a></h3><p>对外的 llama.cpp 库 API 实现</p><h3 id="ggml-c" tabindex="-1">ggml.c <a class="header-anchor" href="#ggml-c" aria-hidden="true">#</a></h3><p>被 llama.cpp 包了一层的内部 API</p><h3 id="ggml-backend-c" tabindex="-1">ggml-backend.c <a class="header-anchor" href="#ggml-backend-c" aria-hidden="true">#</a></h3><p>不同的后端通过 <code>ggml_backend_register</code> 注册自身，<code>ggml_backend_registry_init</code> 运行时分别调用他们，这里利用了一个技巧避免引入头文件。</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">GGML_CALL </span><span style="color:#C792EA;">static</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">void</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">ggml_backend_registry_init</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">void</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#82AAFF;">ggml_backend_register</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">CPU</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> ggml_backend_reg_cpu_init</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">ggml_backend_cpu_buffer_type</span><span style="color:#89DDFF;">(),</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">NULL);</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // add forward decls here to avoid including the backend headers</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">#ifdef</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">GGML_USE_CUDA</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">extern</span><span style="color:#F07178;"> GGML_CALL </span><span style="color:#C792EA;">void</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">ggml_backend_cuda_reg_devices</span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">void</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#82AAFF;">ggml_backend_cuda_reg_devices</span><span style="color:#89DDFF;">();</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">#endif</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // …</span></span>
<span class="line"><span style="color:#A6ACCD;">}</span></span>
<span class="line"></span></code></pre></div><p>实现 <code>static struct ggml_backend_i cpu_backend_i </code> 后端。</p><h3 id="ggml-blas-cpp" tabindex="-1">ggml-blas.cpp <a class="header-anchor" href="#ggml-blas-cpp" aria-hidden="true">#</a></h3><p>实现 <code>static struct ggml_backend_i blas_backend_i</code> 后端。</p><h3 id="ggml-cuda-cu" tabindex="-1"><a href="http://ggml-cuda.cu" target="_blank" rel="noreferrer">ggml-cuda.cu</a> <a class="header-anchor" href="#ggml-cuda-cu" aria-hidden="true">#</a></h3><p>实现 <code>static ggml_backend_i ggml_backend_cuda_interface</code> 后端。</p><p>cuda graph 是由 <a href="https://github.com/ggerganov/llama.cpp/commit/bc4bba364fb96d908f2698e908648df5e6f55e02" target="_blank" rel="noreferrer">https://github.com/ggerganov/llama.cpp/commit/bc4bba364fb96d908f2698e908648df5e6f55e02</a> 这个 commit-bc4b 引入的。</p><h3 id="ggml-cuda" tabindex="-1">ggml-cuda <a class="header-anchor" href="#ggml-cuda" aria-hidden="true">#</a></h3><h4 id="cpy-cuh" tabindex="-1">cpy.cuh <a class="header-anchor" href="#cpy-cuh" aria-hidden="true">#</a></h4><p>commit-bc4b 为 <code>struct ggml_backend_cuda_context</code> 新增了一个成员，<code>std::unique_ptr&lt;ggml_cuda_graph&gt; cuda_graph</code>。看上去一个 context 只会捕获出一个 cuda graph。</p><p>结构体 <code>ggml_cuda_graph</code> 在析构的时候会自动调用 <code>cudaGraphExecDestroy</code> 和 <code>cudaGraphDestroy</code> 清理之前捕获到的 cuda graph。它的定义比较简单，如下</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">struct</span><span style="color:#F07178;"> </span><span style="color:#FFCB6B;">ggml_cuda_graph</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#FFCB6B;">cudaGraph_t</span><span style="color:#F07178;"> graph </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">nullptr;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#FFCB6B;">cudaGraphExec_t</span><span style="color:#F07178;"> instance </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">nullptr;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">size_t</span><span style="color:#F07178;"> num_nodes </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#FFCB6B;">std</span><span style="color:#89DDFF;">::</span><span style="color:#F07178;">vector</span><span style="color:#89DDFF;">&lt;</span><span style="color:#FFCB6B;">cudaGraphNode_t</span><span style="color:#89DDFF;">&gt;</span><span style="color:#F07178;"> nodes</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#FFCB6B;">std</span><span style="color:#89DDFF;">::</span><span style="color:#F07178;">vector</span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;">cudaKernelNodeParams</span><span style="color:#89DDFF;">&gt;</span><span style="color:#F07178;"> params</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // 禁用该 feature 的几种可能的原因</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">bool</span><span style="color:#F07178;"> disable_due_to_gpu_arch </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">false;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // 如果当前用例中，图节点更新得太快，那图需要一直重建，建图的开销可能会大于 cuda graph 节省的开销。</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">bool</span><span style="color:#F07178;"> disable_due_to_too_many_updates </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">false;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">bool</span><span style="color:#F07178;"> disable_due_to_failed_graph_capture </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">false;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">int</span><span style="color:#F07178;"> number_consecutive_updates </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#FFCB6B;">std</span><span style="color:#89DDFF;">::</span><span style="color:#F07178;">vector</span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;">ggml_graph_node_properties</span><span style="color:#89DDFF;">&gt;</span><span style="color:#F07178;"> ggml_graph_properties</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#FFCB6B;">std</span><span style="color:#89DDFF;">::</span><span style="color:#F07178;">vector</span><span style="color:#89DDFF;">&lt;</span><span style="color:#C792EA;">char</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">**&gt;</span><span style="color:#F07178;"> updated_kernel_arg</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#89DDFF;">};</span></span>
<span class="line"></span>
<span class="line"><span style="color:#C792EA;">struct</span><span style="color:#F07178;"> </span><span style="color:#FFCB6B;">ggml_graph_node_properties</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">void</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">*</span><span style="color:#F07178;"> node_address</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // 同 `ggml_tensor` 上的 `ggml_op`，例如 `GGML_OP_CPY`、`GGML_OP_VIEW`</span></span>
<span class="line"><span style="color:#F07178;">    ggml_op node_op</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // 同 `ggml_tensor` 上的 `ne`</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">int64_t</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">ne</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">GGML_MAX_DIMS</span><span style="color:#89DDFF;">];</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // 同 `ggml_tensor` 上的 `nb`</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">size_t</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">nb</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">GGML_MAX_DIMS</span><span style="color:#89DDFF;">];</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // 同 `ggml_tensor` 上的 `src[i]-&gt;data`</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#C792EA;">void</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">*</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">src_address</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">GGML_MAX_SRC</span><span style="color:#89DDFF;">];</span></span>
<span class="line"><span style="color:#89DDFF;">};</span></span>
<span class="line"></span></code></pre></div><p><code>set_ggml_graph_node_properties</code> 从一个 <code>ggml_tensor</code> 构建一个 <code>ggml_graph_node_properties</code>，转换成图里的节点。 <code>ggml_graph_node_has_matching_properties</code> 比较 <code>ggml_tensor</code> 和 <code>ggml_graph_node_properties</code> 的成员，判断二者是否匹配。</p><p><code>ggml_backend_cuda_graph_compute</code> 中根据参数 <code>ggml_backend_t</code> 和 <code>ggml_cgraph</code> 去构建 <code>ggml_backend_t-&gt;ggml_backend_cuda_context-&gt;ggml_cuda_graph</code>。这个函数里面首先检查了是不是安培以下的 GPU，如果是，就不用 cuda graph 了。之前在 T4 上测试的，所以没用到 cuda graph。有点坑爹 release 模式下不开 <code>LLAMA_DEBUG</code>，这错误日志就不打了。</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">GGML_CALL </span><span style="color:#C792EA;">static</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">enum</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">ggml_status</span><span style="color:#A6ACCD;"> ggml_backend_cuda_graph_compute</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">ggml_backend_t</span><span style="color:#A6ACCD;"> backend</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> ggml_cgraph </span><span style="color:#89DDFF;">*</span><span style="color:#A6ACCD;"> cgraph</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">graph</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">nullptr)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">ggml_cuda_info</span><span style="color:#89DDFF;">().</span><span style="color:#A6ACCD;">devices</span><span style="color:#89DDFF;">[</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">device</span><span style="color:#89DDFF;">].</span><span style="color:#A6ACCD;">cc</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;"> CC_AMPERE</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">disable_due_to_gpu_arch</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">true;</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">#ifndef</span><span style="color:#F07178;"> </span><span style="color:#FFCB6B;">NDEBUG</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#82AAFF;">GGML_CUDA_LOG_WARN</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">%s: disabling CUDA graphs due to GPU architecture</span><span style="color:#A6ACCD;">\n</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> __func__</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">#endif</span></span>
<span class="line"><span style="color:#A6ACCD;">        }</span></span>
<span class="line"><span style="color:#A6ACCD;">    }</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#A6ACCD;">}</span></span>
<span class="line"></span></code></pre></div><p>如果启用 cuda graph，则比较当前传入的 <code>ggml_cgraph</code> 和之前当前的 cuda graph 是否相同</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">GGML_CALL </span><span style="color:#C792EA;">static</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">enum</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">ggml_status</span><span style="color:#A6ACCD;"> ggml_backend_cuda_graph_compute</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">ggml_backend_t</span><span style="color:#A6ACCD;"> backend</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> ggml_cgraph </span><span style="color:#89DDFF;">*</span><span style="color:#A6ACCD;"> cgraph</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">instance</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">nullptr)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        cuda_graph_update_required </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">true;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // Check if the graph size has changed</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">ggml_graph_properties</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">!=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">size_t</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">n_nodes</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        cuda_graph_update_required </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">true;</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">ggml_graph_properties</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">resize</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">n_nodes</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // Loop over nodes in GGML graph to determine if CUDA graph update is required</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // and store properties to allow this comparison for the next token</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">int</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">n_nodes</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i</span><span style="color:#89DDFF;">++)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#C792EA;">bool</span><span style="color:#F07178;"> has_matching_properties </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">true;</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(!</span><span style="color:#F07178;">cuda_graph_update_required</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            has_matching_properties </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">ggml_graph_node_has_matching_properties</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">],</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">ggml_graph_properties</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">]);</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(!</span><span style="color:#F07178;">has_matching_properties</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            cuda_graph_update_required </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">true;</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#82AAFF;">set_ggml_graph_node_properties</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">],</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">ggml_graph_properties</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">]);</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>再次遍历当前的 <code>ggml_cgraph</code>，更新 <code>GGML_OP_CPY</code> 类型节点的信息，因为拷贝操作的地址会随着 token 变化。</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">GGML_CALL </span><span style="color:#C792EA;">static</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">enum</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">ggml_status</span><span style="color:#A6ACCD;"> ggml_backend_cuda_graph_compute</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">ggml_backend_t</span><span style="color:#A6ACCD;"> backend</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> ggml_cgraph </span><span style="color:#89DDFF;">*</span><span style="color:#A6ACCD;"> cgraph</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // Loop over nodes in GGML graph to obtain info needed for CUDA graph</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">updated_kernel_arg</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">clear</span><span style="color:#89DDFF;">();</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">int</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">n_nodes</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i</span><span style="color:#89DDFF;">++)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        ggml_tensor </span><span style="color:#89DDFF;">*</span><span style="color:#F07178;"> node </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">];</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">src</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">]</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;&amp;</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">ggml_backend_buffer_is_cuda_split</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">src</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">]-&gt;</span><span style="color:#A6ACCD;">buffer</span><span style="color:#89DDFF;">))</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            use_cuda_graph </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">false;</span><span style="color:#676E95;font-style:italic;"> // Split buffers are not supported by CUDA graph capture</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> GGML_OP_MUL_MAT_ID</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            use_cuda_graph </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">false;</span><span style="color:#676E95;font-style:italic;"> // This node type is not supported by CUDA graph capture</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> GGML_OP_ADD </span><span style="color:#89DDFF;">&amp;&amp;</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">src</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">]</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;&amp;</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">src</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">]-&gt;</span><span style="color:#A6ACCD;">ne</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">]</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&gt;</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">            // disable CUDA graphs for batch size &gt; 1 for now.</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">            // Changes in batch size or context size can cause changes to the grid size of some kernels.</span></span>
<span class="line"><span style="color:#F07178;">            use_cuda_graph </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">false;</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> GGML_OP_CPY</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">            // store the copy op parameter which changes with each token.</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">updated_kernel_arg</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">push_back</span><span style="color:#89DDFF;">((</span><span style="color:#C792EA;">char</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">**)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;(</span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">src</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">]-&gt;</span><span style="color:#A6ACCD;">data</span><span style="color:#89DDFF;">));</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">            // store a pointer to each copy op CUDA kernel to identify it later</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#C792EA;">void</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">*</span><span style="color:#F07178;"> ptr </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">ggml_cuda_cpy_fn</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">src</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">],</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">src</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">]);</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">std</span><span style="color:#89DDFF;">::</span><span style="color:#82AAFF;">find</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">ggml_cuda_cpy_fn_ptrs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">begin</span><span style="color:#89DDFF;">(),</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">ggml_cuda_cpy_fn_ptrs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">end</span><span style="color:#89DDFF;">(),</span><span style="color:#F07178;"> ptr</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">ggml_cuda_cpy_fn_ptrs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">end</span><span style="color:#89DDFF;">())</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#A6ACCD;">ggml_cuda_cpy_fn_ptrs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">push_back</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">ptr</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(!</span><span style="color:#F07178;">use_cuda_graph</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;font-style:italic;">break</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // Disable CUDA graphs (from the next token) if the use-case is demanding too many consecutive graph updates.</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">use_cuda_graph </span><span style="color:#89DDFF;">&amp;&amp;</span><span style="color:#F07178;"> cuda_graph_update_required</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">number_consecutive_updates</span><span style="color:#89DDFF;">++;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;font-style:italic;">else</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">number_consecutive_updates</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // 连续四次 token 的推理（调用 `ggml_backend_cuda_graph_compute`），图都发生了变化，就放弃继续使用 cuda graph</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">number_consecutive_updates</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&gt;=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">4</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">disable_due_to_too_many_updates</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">true;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>然后开始借助 cudaStreamBeginCapture 捕获下面的推理过程中的 CUDA API 调用。注意如果没有开启 cuda graph，下面的这段是每次需要 eager 地执行</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">GGML_CALL </span><span style="color:#C792EA;">static</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">enum</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">ggml_status</span><span style="color:#A6ACCD;"> ggml_backend_cuda_graph_compute</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">ggml_backend_t</span><span style="color:#A6ACCD;"> backend</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> ggml_cgraph </span><span style="color:#89DDFF;">*</span><span style="color:#A6ACCD;"> cgraph</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // Only perform the graph execution if CUDA graphs are not enabled, or we are capturing the graph.</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // With the use of CUDA graphs, the execution will be performed by the graph launch.</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(!</span><span style="color:#F07178;">use_cuda_graph </span><span style="color:#89DDFF;">||</span><span style="color:#F07178;"> cuda_graph_update_required</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">int</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">n_nodes</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i</span><span style="color:#89DDFF;">++)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            ggml_tensor </span><span style="color:#89DDFF;">*</span><span style="color:#F07178;"> node </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cgraph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">];</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">ggml_is_empty</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">node</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">||</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> GGML_OP_RESHAPE </span><span style="color:#89DDFF;">||</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> GGML_OP_TRANSPOSE </span><span style="color:#89DDFF;">||</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> GGML_OP_VIEW </span><span style="color:#89DDFF;">||</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> GGML_OP_PERMUTE </span><span style="color:#89DDFF;">||</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> GGML_OP_NONE</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#89DDFF;font-style:italic;">continue</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#C792EA;">bool</span><span style="color:#F07178;"> ok </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">ggml_cuda_compute_forward</span><span style="color:#89DDFF;">(*</span><span style="color:#F07178;">cuda_ctx</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> node</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(!</span><span style="color:#F07178;">ok</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#82AAFF;">GGML_CUDA_LOG_ERROR</span><span style="color:#89DDFF;">(</span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">%s: op not supported %s (%s)</span><span style="color:#A6ACCD;">\n</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> __func__</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">name</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">ggml_op_name</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">node</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">op</span><span style="color:#89DDFF;">));</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#82AAFF;">GGML_ASSERT</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">ok</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>捕获完成，调用 <code>cudaGraphInstantiate</code> 实例化 cuda graph 成 <code>cudaGraphExec</code>，再根据上面统计到的 <code>GGML_OP_CPY</code> 相关的信息，更新 cuda graph，最后调用 <code>cudaGraphExecUpdate</code> 更新 <code>cudaGraphExec</code>。</p><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">GGML_CALL </span><span style="color:#C792EA;">static</span><span style="color:#A6ACCD;"> </span><span style="color:#C792EA;">enum</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">ggml_status</span><span style="color:#A6ACCD;"> ggml_backend_cuda_graph_compute</span><span style="color:#89DDFF;">(</span><span style="color:#FFCB6B;">ggml_backend_t</span><span style="color:#A6ACCD;"> backend</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> ggml_cgraph </span><span style="color:#89DDFF;">*</span><span style="color:#A6ACCD;"> cgraph</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">instance</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">nullptr)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span><span style="color:#676E95;font-style:italic;"> // Create executable graph from captured graph.</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#82AAFF;">CUDA_CHECK</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">cudaGraphInstantiate</span><span style="color:#89DDFF;">(&amp;</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">instance</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">graph</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">NULL,</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">NULL,</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">));</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // Perform update to graph (if required for this token), and change copy parameter (required for every token)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">cuda_graph_update_required</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        // Extract nodes from graph</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        // First call with null argument gets number of nodes in graph</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#82AAFF;">CUDA_CHECK</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">cudaGraphGetNodes</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">graph</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">nullptr,</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">num_nodes</span><span style="color:#89DDFF;">));</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">        // Subsequent call with non-null argument gets nodes</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">resize</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">num_nodes</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">params</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">resize</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">num_nodes</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">num_nodes</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&gt;</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#82AAFF;">CUDA_CHECK</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">cudaGraphGetNodes</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">graph</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">data</span><span style="color:#89DDFF;">(),</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">num_nodes</span><span style="color:#89DDFF;">));</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">            // Loop over nodes, and extract kernel parameters from each node</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">size_t</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">num_nodes</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i</span><span style="color:#89DDFF;">++)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">                cudaGraphNodeType node_type</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#82AAFF;">CUDA_CHECK</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">cudaGraphNodeGetType</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">],</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;</span><span style="color:#F07178;">node_type</span><span style="color:#89DDFF;">));</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">node_type </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> cudaGraphNodeTypeKernel</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">                    </span><span style="color:#FFCB6B;">cudaError_t</span><span style="color:#F07178;"> stat </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">cudaGraphKernelNodeGetParams</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">],</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">params</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">]);</span><span style="color:#676E95;font-style:italic;"> // Get params using runtime</span></span>
<span class="line"><span style="color:#F07178;">                    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">stat </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> cudaErrorInvalidDeviceFunction</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">                        // Fails due to incorrect handling by CUDA runtime of CUDA BLAS node.</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">                        // We don&#39;t need to update blas nodes, so clear error and move on.</span></span>
<span class="line"><span style="color:#F07178;">                        </span><span style="color:#82AAFF;">cudaGetLastError</span><span style="color:#89DDFF;">();</span></span>
<span class="line"><span style="color:#F07178;">                    </span><span style="color:#89DDFF;">}</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;font-style:italic;">else</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">                        </span><span style="color:#82AAFF;">GGML_ASSERT</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">stat </span><span style="color:#89DDFF;">==</span><span style="color:#F07178;"> cudaSuccess</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">                    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // One of the arguments to the copy kernel is updated for each token, hence we need to</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // replace that argument with the updated value in the CUDA graph</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(!</span><span style="color:#F07178;">cuda_graph_update_required</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span><span style="color:#676E95;font-style:italic;"> // on update steps, the live parameters will already be captured</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#C792EA;">int</span><span style="color:#F07178;"> k </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">(</span><span style="color:#C792EA;">size_t</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i </span><span style="color:#89DDFF;">&lt;</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">num_nodes</span><span style="color:#89DDFF;">;</span><span style="color:#F07178;"> i</span><span style="color:#89DDFF;">++)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">count</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">ggml_cuda_cpy_fn_ptrs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">begin</span><span style="color:#89DDFF;">(),</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">ggml_cuda_cpy_fn_ptrs</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">end</span><span style="color:#89DDFF;">(),</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">params</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">].</span><span style="color:#A6ACCD;">func</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&gt;</span><span style="color:#F07178;"> </span><span style="color:#F78C6C;">0</span><span style="color:#89DDFF;">)</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#C792EA;">char</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">**</span><span style="color:#F07178;"> updated_kernel_arg_ptr </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">updated_kernel_arg</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">at</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">k</span><span style="color:#89DDFF;">++);</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">params</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">].</span><span style="color:#A6ACCD;">kernelParams</span><span style="color:#89DDFF;">[</span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">]</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> updated_kernel_arg_ptr</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">                </span><span style="color:#82AAFF;">CUDA_CHECK</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">cudaGraphKernelNodeSetParams</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">nodes</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">],</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">params</span><span style="color:#89DDFF;">[</span><span style="color:#F07178;">i</span><span style="color:#89DDFF;">]));</span></span>
<span class="line"><span style="color:#F07178;">            </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">        </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // Update graph executable</span></span>
<span class="line"><span style="color:#F07178;">    cudaGraphExecUpdateResultInfo result_info</span><span style="color:#89DDFF;">;</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#FFCB6B;">cudaError_t</span><span style="color:#F07178;"> stat </span><span style="color:#89DDFF;">=</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">cudaGraphExecUpdate</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">instance</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">cuda_ctx</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">cuda_graph</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#A6ACCD;">graph</span><span style="color:#89DDFF;">,</span><span style="color:#F07178;"> </span><span style="color:#89DDFF;">&amp;</span><span style="color:#F07178;">result_info</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ...</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>有了新的 <code>cudaGraphExec</code>，就可以运行这个图了。</p><h4 id="cpy-cu" tabindex="-1"><a href="http://cpy.cu" target="_blank" rel="noreferrer">cpy.cu</a> <a class="header-anchor" href="#cpy-cu" aria-hidden="true">#</a></h4><p>commit-bc4b 定义了一个通用的函数 <code>ggml_cuda_cpy_fn</code> 根据张量类型选择拷贝函数。</p><h3 id="examples" tabindex="-1">examples <a class="header-anchor" href="#examples" aria-hidden="true">#</a></h3><h4 id="simple" tabindex="-1">simple <a class="header-anchor" href="#simple" aria-hidden="true">#</a></h4><p><a href="https://github1s.com/ggerganov/llama.cpp/blob/45c0e2e4c1268c2d7c8c45536f15e3c9a731ecdc/examples/simple/simple.cpp" target="_blank" rel="noreferrer">https://github1s.com/ggerganov/llama.cpp/blob/45c0e2e4c1268c2d7c8c45536f15e3c9a731ecdc/examples/simple/simple.cpp</a></p><p>一个简单 llama.cpp 应用用例。</p></div></div></main><!--[--><!--]--><!----><!--[--><!--[--><!--[--><!----><!--]--><!--]--><!--]--></div></div></div></div></div><!----><!--[--><!--]--></div><!----><footer data-v-4f0db67d> Powered by <a href="https://github.com/forsworns/" target="_blank" title="Author" data-v-4f0db67d>Peihao Yang</a> | Copyright © 2019-2024 | MIT License </footer><!--]--></div>
    <script>__VP_HASH_MAP__ = JSON.parse("{\"index.md\":\"e0ba6ad2\",\"zh_blogs_20190901_index.md\":\"506c415e\",\"about-me_index.md\":\"846ea80d\",\"zh_blogs_20190824_index.md\":\"c631fdf0\",\"zh_about-me_index.md\":\"3ebfac3a\",\"zh_blogs_20190919_index.md\":\"a9ef4728\",\"zh_blogs_20190721_index.md\":\"9a1992f7\",\"zh_blogs_20190908_index.md\":\"0ae17c24\",\"zh_blogs_20210120_index.md\":\"46bc3a68\",\"zh_blogs_20200616_index.md\":\"24a6c73e\",\"zh_blogs_20191112_index.md\":\"52603db5\",\"zh_blogs_20200816_index.md\":\"634fb620\",\"zh_blogs_20200817_index.md\":\"b22a8d7c\",\"zh_blogs_20201023_index.md\":\"4c94be96\",\"zh_blogs_20210204_index.md\":\"922002e3\",\"zh_blogs_20200818_index.md\":\"73c493e7\",\"zh_blogs_20210123_index.md\":\"dd264596\",\"zh_blogs_20191109_index.md\":\"80d0c840\",\"zh_blogs_20210203_index.md\":\"ae20c5bd\",\"zh_blogs_20230201_index.md\":\"d65a7769\",\"zh_blogs_20230201_repost.md\":\"d6a8bf08\",\"zh_blogs_20230209_index.md\":\"3a12ac32\",\"zh_blogs_20230322_index.md\":\"8838bc28\",\"zh_blogs_20230601_index.md\":\"64d86526\",\"zh_blogs_20240128_index.md\":\"6e1d760d\",\"zh_blogs_20240215_index.md\":\"4139d8e8\",\"zh_blogs_20240220_index.md\":\"6d320fea\",\"zh_blogs_20240427_index.md\":\"ab744de2\",\"zh_blogs_20240413_index.md\":\"3307932c\",\"zh_blogs_20240513_index.md\":\"56ad3f14\",\"zh_blogs_20210409_index.md\":\"8932f02b\",\"zh_blogs_20240622_index.md\":\"e693b38c\",\"zh_blogs_20240423_index.md\":\"6c1114d2\",\"zh_blogs_20240526_index.md\":\"3f2d8a25\",\"zh_blogs_20240620_index.md\":\"69e37ac2\",\"zh_blogs_20240626_index.md\":\"c36ba82d\",\"zh_blogs_index.md\":\"a0e1cf19\",\"zh_blogs_20210430_index.md\":\"9c389873\",\"zh_blogs_20210311_index.md\":\"7de1b4a4\",\"zh_blogs_20240518_index.md\":\"d0555774\",\"zh_blogs_20191102_index.md\":\"53b363d5\",\"zh_blogs_20230101_index.md\":\"8bcd8a38\",\"zh_blogs_20210310_index.md\":\"b02adbe1\",\"zh_blogs_20210412_index.md\":\"56cbe3d4\",\"zh_blogs_20210315_index.md\":\"4e8d51fd\",\"zh_blogs_20220224_index.md\":\"cddc3efe\",\"zh_blogs_20221108_index.md\":\"1ecb2c5e\",\"zh_blogs_20220611_index.md\":\"c429e3ae\",\"zh_blogs_20210312_index.md\":\"c84a09fb\",\"zh_blogs_20240519_index.md\":\"3935f5d4\",\"zh_blogs_20230125_index.md\":\"b6270011\",\"zh_blogs_20240623_index.md\":\"c3cf8349\",\"zh_blogs_20230121_index.md\":\"0c062d60\",\"zh_blogs_20210728_index.md\":\"24673902\",\"zh_blogs_20221228_index.md\":\"b21decf6\",\"zh_blogs_20210506_index.md\":\"8c999b21\",\"zh_blogs_20230126_index.md\":\"0b38611b\",\"zh_blogs_20211120_index.md\":\"75f05041\",\"zh_blogs_tags_index.md\":\"95f669eb\",\"zh_blogs_20211210_index.md\":\"fd74a440\",\"zh_blogs_20210706_index.md\":\"3a568024\",\"zh_blogs_20210224_index.md\":\"c500882b\",\"zh_blogs_20211130_index.md\":\"c8bce327\",\"zh_blogs_20210329_index.md\":\"810ef071\",\"zh_blogs_20210226_index.md\":\"417a3ac5\",\"zh_blogs_20210801_index.md\":\"18df2595\",\"zh_blogs_20210223_index.md\":\"e7082a64\",\"zh_blogs_20220105_index.md\":\"273f914b\",\"zh_blogs_20220101_index.md\":\"ee91e215\",\"zh_blogs_20210822_index.md\":\"ad67ffd7\",\"zh_index.md\":\"8b832df8\",\"zh_blogs_20220316_index.md\":\"92bf9247\",\"zh_blogs_20221024_index.md\":\"28646e28\",\"zh_blogs_20191103_index.md\":\"816338e7\",\"zh_blogs_20210715_index.md\":\"8feff090\",\"zh_blogs_20211002_index.md\":\"d511de03\",\"zh_blogs_20210627_index.md\":\"9d791080\"}")</script>
    <script type="module" async src="/assets/app.73f81c81.js"></script>
    
  </body>
</html>