<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>torch æºç é˜…è¯» | Sharlayan</title>
    <meta name="description" content="Pytorch2 recap">
    <link rel="preload stylesheet" href="/assets/style.3ef9b918.css" as="style">
    <link rel="modulepreload" href="/assets/chunks/VPAlgoliaSearchBox.df3ef109.js">
    <link rel="modulepreload" href="/assets/app.0f5a0ae1.js">
    <link rel="modulepreload" href="/assets/zh_blogs_20241008_index.md.cd84a035.lean.js">
    
    <script src="https://cdn.jsdelivr.net/gh/stevenjoezhang/live2d-widget@latest/autoload.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/4.0.0/github-markdown.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.staticfile.org/ionicons/2.0.1/css/ionicons.min.css">
  <link rel="icon" type="image/png" href="/logo.png">
  <meta name="author" content="Peihao Yang">
  <meta property="og:title" content="Home">
  <meta property="og:description" content="Home of Peihao Yang">
  <script id="check-dark-light">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
  </head>
  <body>
    <div id="app"><!--[--><div class="Layout" data-v-b617430f><!--[--><!--]--><!--[--><span tabindex="-1" data-v-d4120332></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-d4120332> Skip to content </a><!--]--><!----><header class="VPNav no-sidebar" data-v-b617430f data-v-aa1cde23><div class="VPNavBar" data-v-aa1cde23 data-v-cbdd8588><div class="container" data-v-cbdd8588><div class="title" data-v-cbdd8588><div class="VPNavBarTitle" data-v-cbdd8588 data-v-730d6dd1><a class="title" href="/zh/" data-v-730d6dd1><!--[--><!--]--><!--[--><img class="VPImage logo" src="/assets/logo.png" alt data-v-0f13a436><!--]--><!----><!--[--><!--]--></a></div></div><div class="content" data-v-cbdd8588><div class="curtain" data-v-cbdd8588></div><!--[--><!--]--><div class="VPNavBarSearch search" data-v-cbdd8588 style="--699c4559:&#39;Meta&#39;;"><div id="docsearch"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg class="DocSearch-Search-Icon" width="20" height="20" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-cbdd8588 data-v-2d3a777e><span id="main-nav-aria-label" class="visually-hidden" data-v-2d3a777e>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/" data-v-2d3a777e data-v-f559a019 data-v-1a0f9836><!--[-->ğŸ¡ä¸»é¡µ<!--]--><!----></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/zh/about-me/" data-v-2d3a777e data-v-f559a019 data-v-1a0f9836><!--[-->ğŸ¦¹â€â™‚ï¸å…³äºæˆ‘<!--]--><!----></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-2d3a777e data-v-fc33d832><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-fc33d832><span class="text" data-v-fc33d832><!----> ğŸ““åšå®¢ <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-fc33d832><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-fc33d832><div class="VPMenu" data-v-fc33d832 data-v-ecf4e7d9><div class="items" data-v-ecf4e7d9><!--[--><!--[--><div class="VPMenuLink" data-v-ecf4e7d9 data-v-c7da634f><a class="VPLink link" href="/zh/blogs/" data-v-c7da634f data-v-1a0f9836><!--[-->ğŸ“ƒæ‰€æœ‰åšå®¢<!--]--><!----></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-ecf4e7d9 data-v-c7da634f><a class="VPLink link" href="/zh/blogs/tags/" data-v-c7da634f data-v-1a0f9836><!--[-->ğŸ”–æ ‡ç­¾åˆ†ç±»<!--]--><!----></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="https://forsworns.github.io/feed.xml" target="_blank" rel="noreferrer" data-v-2d3a777e data-v-f559a019 data-v-1a0f9836><!--[-->ğŸ”¥RSS<!--]--><!----></a><!--]--><!--]--></nav><div class="VPFlyout VPNavBarTranslations translations" data-v-cbdd8588 data-v-7cdc304e data-v-fc33d832><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-fc33d832><span class="text" data-v-fc33d832><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="option-icon" data-v-fc33d832><path d="M0 0h24v24H0z" fill="none"></path><path d=" M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z " class="css-c4d79v"></path></svg>  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-fc33d832><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-fc33d832><div class="VPMenu" data-v-fc33d832 data-v-ecf4e7d9><!----><!--[--><!--[--><div class="items" data-v-7cdc304e><p class="title" data-v-7cdc304e>ä¸­æ–‡</p><!--[--><div class="VPMenuLink" data-v-7cdc304e data-v-c7da634f><a class="VPLink link" href="/" data-v-c7da634f data-v-1a0f9836><!--[-->English<!--]--><!----></a></div><!--]--></div><!--]--><!--]--></div></div></div><div class="VPNavBarAppearance appearance" data-v-cbdd8588 data-v-7f24c201><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-7f24c201 data-v-2b897f09 data-v-ab7cdc04><span class="check" data-v-ab7cdc04><span class="icon" data-v-ab7cdc04><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-2b897f09><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-2b897f09><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-cbdd8588 data-v-a5afa74d data-v-80c99471><!--[--><a class="VPSocialLink" href="https://github.com/forsworns/blog-vitepress" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><a class="VPSocialLink" href="mailto:peihao.young@gmail.com" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="20">
            <path d="M874.666667 375.189333V746.666667a64 64 0 0 1-64 64H213.333333a64 64 0 0 1-64-64V375.189333l266.090667 225.6a149.333333 149.333333 0 0 0 193.152 0L874.666667 375.189333zM810.666667 213.333333a64.789333 64.789333 0 0 1 22.826666 4.181334 63.616 63.616 0 0 1 26.794667 19.413333 64.32 64.32 0 0 1 9.344 15.466667c2.773333 6.570667 4.48 13.696 4.906667 21.184L874.666667 277.333333v21.333334L553.536 572.586667a64 64 0 0 1-79.893333 2.538666l-3.178667-2.56L149.333333 298.666667v-21.333334a63.786667 63.786667 0 0 1 35.136-57.130666A63.872 63.872 0 0 1 213.333333 213.333333h597.333334z" ></path>
            </svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-cbdd8588 data-v-e459e5dc data-v-fc33d832><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-fc33d832><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-fc33d832><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-fc33d832><div class="VPMenu" data-v-fc33d832 data-v-ecf4e7d9><!----><!--[--><!--[--><div class="group" data-v-e459e5dc><p class="trans-title" data-v-e459e5dc>ä¸­æ–‡</p><!--[--><div class="VPMenuLink" data-v-e459e5dc data-v-c7da634f><a class="VPLink link" href="/" data-v-c7da634f data-v-1a0f9836><!--[-->English<!--]--><!----></a></div><!--]--></div><div class="group" data-v-e459e5dc><div class="item appearance" data-v-e459e5dc><p class="label" data-v-e459e5dc>Appearance</p><div class="appearance-action" data-v-e459e5dc><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-e459e5dc data-v-2b897f09 data-v-ab7cdc04><span class="check" data-v-ab7cdc04><span class="icon" data-v-ab7cdc04><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-2b897f09><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-2b897f09><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="group" data-v-e459e5dc><div class="item social-links" data-v-e459e5dc><div class="VPSocialLinks social-links-list" data-v-e459e5dc data-v-80c99471><!--[--><a class="VPSocialLink" href="https://github.com/forsworns/blog-vitepress" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><a class="VPSocialLink" href="mailto:peihao.young@gmail.com" target="_blank" rel="noopener" data-v-80c99471 data-v-51b6609b><svg role="img" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="20">
            <path d="M874.666667 375.189333V746.666667a64 64 0 0 1-64 64H213.333333a64 64 0 0 1-64-64V375.189333l266.090667 225.6a149.333333 149.333333 0 0 0 193.152 0L874.666667 375.189333zM810.666667 213.333333a64.789333 64.789333 0 0 1 22.826666 4.181334 63.616 63.616 0 0 1 26.794667 19.413333 64.32 64.32 0 0 1 9.344 15.466667c2.773333 6.570667 4.48 13.696 4.906667 21.184L874.666667 277.333333v21.333334L553.536 572.586667a64 64 0 0 1-79.893333 2.538666l-3.178667-2.56L149.333333 298.666667v-21.333334a63.786667 63.786667 0 0 1 35.136-57.130666A63.872 63.872 0 0 1 213.333333 213.333333h597.333334z" ></path>
            </svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-cbdd8588 data-v-d6834c14><span class="container" data-v-d6834c14><span class="top" data-v-d6834c14></span><span class="middle" data-v-d6834c14></span><span class="bottom" data-v-d6834c14></span></span></button></div></div></div><!----></header><!----><!----><div class="VPContent" id="VPContent" data-v-b617430f data-v-f32377af><div class="VPDoc has-aside" data-v-f32377af data-v-1e970af1><div class="container" data-v-1e970af1><div class="aside" data-v-1e970af1><div class="aside-curtain" data-v-1e970af1></div><div class="aside-container" data-v-1e970af1><div class="aside-content" data-v-1e970af1><div class="VPDocAside" data-v-1e970af1 data-v-b1723386><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" data-v-b1723386 data-v-0980ba1d><div class="content" data-v-0980ba1d><div class="outline-marker" data-v-0980ba1d></div><div class="outline-title" data-v-0980ba1d>On this page</div><nav aria-labelledby="doc-outline-aria-label" data-v-0980ba1d><span class="visually-hidden" id="doc-outline-aria-label" data-v-0980ba1d> Table of Contents for current page </span><ul class="root" data-v-0980ba1d data-v-6f4caaf4><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-b1723386></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-1e970af1><div class="content-container" data-v-1e970af1><!--[--><!--]--><main class="main" data-v-1e970af1><div style="position:relative;" class="vp-doc _zh_blogs_20241008_index" data-v-1e970af1><div><nav class="table-of-contents"><ul><li><a href="#pytorch2-ä¸­æ‰“å¼€æ—¥å¿—">pytorch2 ä¸­æ‰“å¼€æ—¥å¿—</a></li><li><a href="#nvrtc">nvrtc</a></li><li><a href="#torch-compile-è¿‡ç¨‹ä¸­çš„-nvrtc-è°ƒç”¨">torch.compile è¿‡ç¨‹ä¸­çš„ nvrtc è°ƒç”¨</a></li><li><a href="#torch-compile-å…¶ä»–-api">torch.compile å…¶ä»– API</a><ul><li><a href="#å‰ç«¯-dynamo">å‰ç«¯ dynamo</a></li><li><a href="#åç«¯-inductor">åç«¯ inductor</a></li><li><a href="#å…¶ä»–">å…¶ä»–</a></li></ul></li><li><a href="#cuda-graph-ä»£ç ">cuda graph ä»£ç </a><ul><li><a href="#å‰ç«¯-dynamo-1">å‰ç«¯ dynamo</a></li><li><a href="#åœ¨åç«¯-inductor">åœ¨åç«¯ inductor</a></li></ul></li><li><a href="#torch-æ—¥å¿—æ‰“å°">torch æ—¥å¿—æ‰“å°</a></li><li><a href="#c10-cuda-cudacachingallocator-cpp">c10/cuda/CUDACachingAllocator.cpp</a></li><li><a href="#torch-nn-parallel-distributed-py">torch/nn/parallel/distributed.py</a></li><li><a href="#torch-utils-data-distributed-py">torch/utils/data/distributed.py</a></li><li><a href="#torch-distributed">torch/distributed/</a><ul><li><a href="#tensor-parallel-ddp-py">tensor/parallel/ddp.py</a></li><li><a href="#nn">nn</a></li><li><a href="#algorithms">algorithms</a></li></ul></li><li><a href="#torch-nn-parallel-data-parallel-py">torch/nn/parallel/data_parallel.py</a></li><li><a href="#torch-randn-çš„å®ç°">torch.randn() çš„å®ç°</a></li><li><a href="#aten-src-aten-native-tensorfactories-cpp">/aten/src/ATen/native/TensorFactories.cpp</a></li><li><a href="#aten-src-aten-native-convolution-cpp">/aten/src/ATen/native/Convolution.cpp</a></li><li><a href="#aten-src-aten-native-cuda-cudaloops-cuh">/aten/src/ATen/native/cuda/CUDALoops.cuh</a></li><li><a href="#aten-src-aten-native-native-functions-yaml">/aten/src/ATen/native/native_functions.yaml</a></li></ul></nav><h2 id="pytorch2-ä¸­æ‰“å¼€æ—¥å¿—" tabindex="-1">pytorch2 ä¸­æ‰“å¼€æ—¥å¿— <a class="header-anchor" href="#pytorch2-ä¸­æ‰“å¼€æ—¥å¿—" aria-hidden="true">#</a></h2><p>import torch import logging torch._logging.set_logs(all=logging.DEBUG)</p><h2 id="nvrtc" tabindex="-1">nvrtc <a class="header-anchor" href="#nvrtc" aria-hidden="true">#</a></h2><p>torch.compile æ—¶å¤§é‡çš„å­è¿›ç¨‹å ç”¨ GPU è®¾å¤‡ã€‚å·²çŸ¥å•çº¯è°ƒç”¨ libnvrtc å’Œ libnvJitLink ä¸ä¼šå¼•ç”¨ GPU è®¾å¤‡ã€‚</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">import torch</span></span>
<span class="line"><span style="color:#A6ACCD;">import logging</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;">torch._logging.set_logs(all=logging.DEBUG)</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;"># â€œreduce-overheadâ€ æ¨¡å¼æ‰ä¼šç”¨åˆ° cuda graph</span></span>
<span class="line"><span style="color:#A6ACCD;">@torch.compile(mode=&quot;reduce-overhead&quot;)</span></span>
<span class="line"><span style="color:#A6ACCD;">def my_model(x):</span></span>
<span class="line"><span style="color:#A6ACCD;">    y = torch.matmul(x, x).cuda()</span></span>
<span class="line"><span style="color:#A6ACCD;">    # side effect breaks graph construction</span></span>
<span class="line"><span style="color:#A6ACCD;">    input(&quot;during capture&quot;)</span></span>
<span class="line"><span style="color:#A6ACCD;">    y = torch.matmul(y, x).cuda()</span></span>
<span class="line"><span style="color:#A6ACCD;">    return y</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;">x = torch.randn(10, 10).cuda()</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;">print(&quot;graph exec 1&quot;, flush=True)</span></span>
<span class="line"><span style="color:#A6ACCD;">y = my_model(x)</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;">print(&quot;graph exec 2&quot;, flush=True)</span></span>
<span class="line"><span style="color:#A6ACCD;">y = my_model(x)</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;">print(&quot;y&quot;, y, flush=True)</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span></code></pre></div><p>Pytorch ä¸­é€šè¿‡å® AT_CUDA_NVRTC_CHECK è°ƒç”¨ nvrtc çš„ä»£ç å¤„ç†é”™è¯¯ã€‚<a href="http://libnvrtc.so" target="_blank" rel="noreferrer">libnvrtc.so</a> åº“ä¸­çš„ç¬¦å·ç±»ä¼¼ <a href="http://libcuda.so" target="_blank" rel="noreferrer">libcuda.so</a> ç­‰æ˜¯é€šè¿‡ä¸‹é¢çš„å‡½æ•°åŠ¨æ€è·å–çš„</p><div class="language-c++"><button title="Copy Code" class="copy"></button><span class="lang">c++</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">const at::cuda::NVRTC&amp; nvrtc() {</span></span>
<span class="line"><span style="color:#A6ACCD;">  return at::globalContext().getNVRTC();</span></span>
<span class="line"><span style="color:#A6ACCD;">}</span></span>
<span class="line"><span style="color:#A6ACCD;"></span></span></code></pre></div><p>CUDAHook å’Œ nvrtc çš„å…·ä½“çš„å®ç°æ˜¯åœ¨ <code>aten/src/ATen/cuda/detail/CUDAHooks.cpp</code> å’Œ <code>aten/src/ATen/cuda/detail/LazyNVRTC.cpp</code>ã€‚åœ¨ <code>aten/src/ATen/cuda/detail/LazyNVRTC.cpp</code> ä¸­ï¼Œ<code>lazyNVRTC</code> è¿™ä¸ªå…¨å±€å˜é‡ä¸Šé™¤äº† nvrtc çš„ APIï¼Œè¿˜ä¼šé€šè¿‡ <code>dlsym</code> æ‡’åŠ è½½ <a href="http://libcuda.so" target="_blank" rel="noreferrer">libcuda.so</a> ä¸­çš„éƒ¨åˆ† APIã€‚ å¯èƒ½å°±æ˜¯è¿™é‡Œ dlopen çš„ <a href="http://libcuda.so" target="_blank" rel="noreferrer">libcuda.so</a> ä»å ç”¨è®¾å¤‡ã€‚ä½†æ˜¯è¿™é‡Œä¹Ÿè¯´ä¸é€šï¼ŒcuInit å’Œ CUcontext ç›¸å…³çš„å‡½æ•°ä¸å¯èƒ½ä¸å…ˆè°ƒç”¨ã€‚</p><p>é™¤äº†ä¸‹é¢ torch.compile ç›¸å…³çš„ä¼šè°ƒç”¨åˆ° nvrtc ç¼–è¯‘ cuda ç®—å­ï¼Œaten åº“ä¸­ä¾‹å¦‚ <code>aten/src/ATen/native/cuda/jit_utils.cpp</code> çš„ <code>jit_pwise_function</code> ä¹Ÿè°ƒç”¨äº† nvrtcï¼Œä½†æ˜¯æ˜¯ç”¨æ¥åšä¸€äº›å¾ªç¯å±•å¼€ç­‰ç®—å­ä¼˜åŒ–çš„ã€‚</p><h2 id="torch-compile-è¿‡ç¨‹ä¸­çš„-nvrtc-è°ƒç”¨" tabindex="-1">torch.compile è¿‡ç¨‹ä¸­çš„ nvrtc è°ƒç”¨ <a class="header-anchor" href="#torch-compile-è¿‡ç¨‹ä¸­çš„-nvrtc-è°ƒç”¨" aria-hidden="true">#</a></h2><p>è¢«ç”¨äºä»£ç ç”Ÿæˆ</p><p>torch/_dynamo/trace_rules.py ä¸­å®šä¹‰äº†ä¸€ç³»åˆ—æŒ‡å¯¼å‰ç«¯çš„ trace ruleï¼Œå…¶ä¸­çš„ â€œtorch._C._te.construct_codegenâ€ æ˜¯å®šä¹‰åœ¨ torch/csrc/jit/tensorexpr/tensorexpr_init.cpp ä¸­çš„ <code>construct_codegen -&gt; CudaCodeGen</code>ï¼Œ<code>CudaCodeGen::Initialize -&gt; CudaCodeGen::CompileToNVRTC</code> å¯ä»¥è°ƒç”¨åˆ° nvrtc è¿›è¡Œå³æ—¶ç¼–è¯‘ã€‚</p><p>å°† CudaAnalysis åˆ†æå‡ºçš„ä»£æ ‘ï¼Œé€šè¿‡ CudaPrinterã€GPUMetaVarRewriter è¾…åŠ©ç»“æ„ä½“äº¤ç»™å…¶ä»–è¾…åŠ©å‡½æ•°åšé‡å†™ï¼Œç»“æœå†™å…¥åˆ°ä¸€ä¸ª ostringstreamï¼Œäº¤ç»™ nvrtc ç¼–è¯‘ã€‚</p><p>è¢«ç”¨äºç®—å­èåˆ</p><p>prim::FusionGroup ï¼ˆtorch/csrc/jit/runtime/register_prim_ops_fulljit.cppï¼‰ -&gt; runFusion -&gt; launchFusionï¼ˆtorch/csrc/jit/codegen/fuser/executor.cppï¼‰ -&gt; launch_raw compileKernel (torch/csrc/jit/codegen/fuser/compiler.cpp) è°ƒç”¨äº† getConstructor æ‹¿åˆ° registerFusionBackend ä¸­æ³¨å†Œåˆ°å…¨å±€çš„ <code>createFusionKernel</code>ï¼ˆtorch/csrc/jit/codegen/fuser/cuda/fused_kernel.cpp æˆ– <code>torch/csrc/jit/codegen/fuser/cpu/fused_kernel.cpp</code>ï¼‰ ã€‚</p><h2 id="torch-compile-å…¶ä»–-api" tabindex="-1">torch.compile å…¶ä»– API <a class="header-anchor" href="#torch-compile-å…¶ä»–-api" aria-hidden="true">#</a></h2><h3 id="å‰ç«¯-dynamo" tabindex="-1">å‰ç«¯ dynamo <a class="header-anchor" href="#å‰ç«¯-dynamo" aria-hidden="true">#</a></h3><p>torch/_dynamo/eval_frame.py ä¸­çš„ <code>_optimize()</code> å‰ç«¯ TorchDynamo çš„å…¥å£å‡½æ•°ã€‚</p><p>torch/_dynamo/convert_frame.py å®šä¹‰äº† <code>ConvertFrame</code>ã€ <code>ConvertFrameAssert</code> ç­‰ä»¿å‡½æ•°ç±»ï¼Œæœ€ç»ˆè°ƒç”¨åŒæ–‡ä»¶å†…çš„ <code>_compile</code> å‡½æ•°ï¼Œå°†æ ˆå¸§è½¬æ¢æˆ FX graphã€‚</p><h3 id="åç«¯-inductor" tabindex="-1">åç«¯ inductor <a class="header-anchor" href="#åç«¯-inductor" aria-hidden="true">#</a></h3><p>torch/_inductor/compile_fx.py ä¸­çš„ <code>compile_fx</code>ï¼Œåœ¨ torch/_dynamo/backends/inductor.py é‡Œé¢é€šè¿‡ <code>register_backend</code> æ³¨å†Œåˆ°å…¨å±€çš„åç«¯ fx graph å…¥å£å‡½æ•°ã€‚</p><p>torch/_inductor/async_compile.py å¼‚æ­¥ç¼–è¯‘ã€‚ç»´æŠ¤äº†ä¸€ä¸ªè¿›ç¨‹æ± ï¼Œä¾‹å¦‚ <code>triton</code> åç«¯å°±é€šè¿‡ä¸‹é¢è¿™ä¸ªå‡½æ•°ï¼Œå‘è¿›ç¨‹æ± æäº¤äº†ä¸€ä¸ªç¼–è¯‘ä»»åŠ¡ç„¶åè¿”å›äº†ä¸€ä¸ª futureã€‚</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">triton</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;font-style:italic;">self</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">kernel_name</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">str</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">source_code</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">str</span><span style="color:#89DDFF;">,</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">device_str</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">str</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">&quot;</span><span style="color:#C3E88D;">cuda</span><span style="color:#89DDFF;">&quot;</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">        kernel </span><span style="color:#89DDFF;">=</span><span style="color:#A6ACCD;"> TritonCodeCache</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">load</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">kernel_name</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> source_code</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">        </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">TritonFuture</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">            kernel</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#A6ACCD;">self</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">process_pool</span><span style="color:#89DDFF;">().</span><span style="color:#82AAFF;">submit</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">                _worker_compile_triton</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">                kernel</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">_reload_in_subproc</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">                extra_env</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">            </span><span style="color:#89DDFF;">),</span></span>
<span class="line"><span style="color:#82AAFF;">        </span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><p>æ‰€ä»¥èƒ½çœ‹å‡ºï¼Œå¤šå‡ºæ¥çš„é‚£äº›è¿›ç¨‹ï¼Œæ˜¯é€šè¿‡è¿™é‡Œç”Ÿæˆçš„ã€‚å¯ä»¥é€šè¿‡ä¸‹é¢çš„ <code>TORCHINDUCTOR_COMPILE_THREADS</code> ç¯å¢ƒå˜é‡ä¿®æ”¹ã€‚ ç„¶åè¿›ç¨‹æ± æ˜¯ä½¿ç”¨çš„ concurrent.futures.ProcessPoolExecutorï¼Œ<strong>åŸºäº forkï¼Œæ‰€ä»¥å¯èƒ½ cuda æ²¡æœ‰ç”¨åˆ°ï¼Œä½†æ˜¯ /dev/nvidia0 ç­‰ fd ä¹Ÿè¢«å ç”¨äº†</strong>ã€‚</p><p>torch/_inductor/codecache.py ä»£ç ç¼–è¯‘ç¼“å­˜ï¼Œä¾‹å¦‚ CUDACodeCache ä¸º cuda ä»£ç çš„ç¼–è¯‘ç¼“å­˜ã€‚å¦‚æœéœ€è¦ï¼Œè°ƒç”¨ <code>cuda_compile_command</code> å‡½æ•°è¿›è¡Œç¼–è¯‘ã€‚</p><h3 id="å…¶ä»–" tabindex="-1">å…¶ä»– <a class="header-anchor" href="#å…¶ä»–" aria-hidden="true">#</a></h3><p>torch/_inductor/config.py ä¸­ï¼Œé€šè¿‡ <code>decide_compile_threads</code> è·å–äº† cpu æ ¸å¿ƒæ•°ã€‚é€šè¿‡ç¯å¢ƒå˜é‡ <code>TORCHINDUCTOR_COMPILE_THREADS</code> å¯ä»¥ä¿®æ”¹ã€‚</p><h2 id="cuda-graph-ä»£ç " tabindex="-1">cuda graph ä»£ç  <a class="header-anchor" href="#cuda-graph-ä»£ç " aria-hidden="true">#</a></h2><h3 id="å‰ç«¯-dynamo-1" tabindex="-1">å‰ç«¯ dynamo <a class="header-anchor" href="#å‰ç«¯-dynamo-1" aria-hidden="true">#</a></h3><p>torch/_dynamo/backends/cudagraphs.py ä¸­çš„ <code>CudagraphsBackend</code>ï¼Œé€šè¿‡ <code>register_backend</code> æ³¨å†Œåˆ°å…¨å±€çš„ cuda graph åç«¯å…¥å£ã€‚</p><h3 id="åœ¨åç«¯-inductor" tabindex="-1">åœ¨åç«¯ inductor <a class="header-anchor" href="#åœ¨åç«¯-inductor" aria-hidden="true">#</a></h3><p>torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py</p><h2 id="torch-æ—¥å¿—æ‰“å°" tabindex="-1">torch æ—¥å¿—æ‰“å° <a class="header-anchor" href="#torch-æ—¥å¿—æ‰“å°" aria-hidden="true">#</a></h2><p>pytorch2 ä¸­ï¼Œ ç¯å¢ƒå˜é‡ TORCH_LOGS=&quot;+inductor,+dynamo&quot; æˆ–è€…ä¹Ÿå¯ä»¥é€šè¿‡ API ç›´æ¥è®¾ç½® torch._logging.set_logs(all=logging.DEBUG)</p><h2 id="c10-cuda-cudacachingallocator-cpp" tabindex="-1">c10/cuda/CUDACachingAllocator.cpp <a class="header-anchor" href="#c10-cuda-cudacachingallocator-cpp" aria-hidden="true">#</a></h2><p>torch çš„ä¸¤ä¸ªæ˜¾å­˜åˆ†é…å™¨å®ç°ï¼ŒPYTORCH_NO_CUDA_MEMORY_CACHING æ§åˆ¶ç‰ˆæœ¬ã€‚æœ‰ä¸€ä¸ªtorch ç‰ˆæœ¬çš„æ–°åˆ†é…å™¨ä¾èµ– NVMLï¼Œå¹³å°ä¸æ”¯æŒ NVML çš„æ—¶å€™å¯ä»¥å…ˆç”¨è¿™ä¸ªç¯å¢ƒå˜é‡å…³äº†æ–°åˆ†é…å™¨ã€‚</p><h2 id="torch-nn-parallel-distributed-py" tabindex="-1">torch/nn/parallel/distributed.py <a class="header-anchor" href="#torch-nn-parallel-distributed-py" aria-hidden="true">#</a></h2><p>å®Œå…¨åŸºäºé›†åˆé€šä¿¡åº“çš„ DP å®ç° <code>DistributedDataParallel</code>ï¼ˆDDPï¼‰ï¼Œå„ä¸ª rank ç‹¬è‡ªæŒæœ‰æ¨¡å‹ã€‚</p><p>é¦–å…ˆçœ‹ <code>DistributedDataParallel</code> çš„æ„é€ å‡½æ•°ã€‚<code>self._module_parameters</code> ä¸­å­˜äº†æ‰€æœ‰æœªè¢«å‚æ•°è¿‡æ»¤æ‰çš„æ¨¡å‹å‚æ•°ã€‚</p><p>åˆå§‹åŒ–é˜¶æ®µ <code>self._verify_param_shape_across_processes()</code>ã€<code>self._sync_module_states()</code> ç”¨æ¥åœ¨ä¸åŒ rank é—´æ£€æŸ¥ã€åŒæ­¥ module çš„åˆå§‹çŠ¶æ€ï¼Œè¿™ä¸€æ­¥ä¼šç”¨ broadcast é›†åˆé€šä¿¡ï¼Œbuffer å¤§å°åœ¨ä»£ç é‡Œå†™æ­»äº†æ˜¯ 250 MBã€‚æœ€å…³é”®çš„æ˜¯ <code>self._ddp_init_helper()</code> å»åˆå§‹åŒ– reducerï¼Œè¿™ä¸€æ­¥å®Œäº†åŸºæœ¬åˆå§‹åŒ–å°±ç»“æŸäº†ï¼Œå‰©ä¸‹çš„å°±æ˜¯ä¸€äº›æ··åˆç²¾åº¦ AMP ç›¸å…³çš„ä»£ç ç­‰ï¼Œè¯¥å‡½æ•°çš„æ³¨é‡Šå¦‚ä¸‹ï¼Œè§£é‡Šå¾—å¾ˆè¯¦ç»†äº†</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">DDP init helper function to manage parameters, grad hooks, logging, and SyncBatchNorm.</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">Initialization helper function that does the following:</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">(1) bucketing the parameters for reductions</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">(2) resetting the bucketing states</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">(3) registering the grad hooks</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">(4) Logging construction-time DDP logging data</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">(5) passing a handle of DDP to SyncBatchNorm Layer</span></span>
<span class="line"><span style="color:#89DDFF;font-style:italic;">&quot;&quot;&quot;</span></span>
<span class="line"></span></code></pre></div><p>DDP é€šè¿‡å°†å‚æ•°åˆ’åˆ†åˆ°ä¸åŒçš„æ¡¶é‡Œé¢ï¼Œæ¥å®ç°æ¢¯åº¦ reduction å’Œ åå‘ä¼ æ’­çš„ overlap ä¼˜åŒ–ï¼Œæ©ç›– reduction è€—æ—¶ï¼Œæ¡¶çš„é»˜è®¤å¤§å°æ˜¯ 25MBï¼Œå¯ä»¥é€šè¿‡ <code>bucket_cap_mb</code> å‚æ•°æ§åˆ¶ï¼Œæ¡¶çš„åˆå§‹åŒ–ä¹Ÿåœ¨ <code>self._ddp_init_helper()</code> ï¼Œè°ƒç”¨åˆ°çš„æ–¹æ³•æ˜¯ <code>torch/csrc/distributed/c10d/reducer.cpp</code> é‡Œé¢å®šä¹‰çš„ <code>compute_bucket_assignment_by_size</code>ï¼Œè·å–æ¯ä¸ªæ¡¶çš„ idx å’Œ sizeã€‚åˆ’åˆ†å¥½æ¡¶ä¹‹åï¼Œå†é€šè¿‡ <code>torch/csrc/distributed/c10d/reducer.cpp</code> ä¸­å®ç°çš„ <code>Reducer</code> ç±»å‹çš„æ„é€ å‡½æ•°ï¼Œå®Œæˆåˆå§‹åŒ–ã€‚</p><p><code>compute_bucket_assignment_by_size()</code>çš„é€»è¾‘æ˜¯ï¼šå…ˆæ„é€ ä¸€ä¸ªæ¡¶çš„å“ˆå¸Œè¡¨ï¼Œæ¯ä¸ªæ¡¶å†…å¯èƒ½æœ‰å¤šä¸ªå¼ é‡ï¼Œå“ˆå¸Œè¡¨çš„é”®æ˜¯é€šè¿‡å¼ é‡çš„æ•°æ®ç±»å‹å’Œå®ƒæ‰€åœ¨çš„è®¾å¤‡å“ˆå¸Œå‡ºæ¥çš„ï¼Œå¼ é‡æ•°æ®çš„å¤§å°è®¡ç®—æ–¹å¼å°±æ˜¯å¼ é‡è§„æ¨¡ä¹˜ä¸Šå®ƒçš„æ•°æ®ç±»å‹çš„å¤§å°ã€‚å½“ä¸€ä¸ªé”®å¯¹åº”çš„æ¡¶è¢«å¡æ»¡ï¼Œå°±è¦å°†å½“å‰çš„æ¡¶æ·»åŠ åˆ°è¿”å›åˆ—è¡¨é‡Œé¢ï¼Œç„¶åä¸ºç›¸åº”é”®é‡å»ºä¸€ä¸ªæ¡¶ã€‚æœ€åå†æŠŠå‰©ä½™çš„æ²¡æ»¡çš„æ¡¶å¡«å……åˆ°è¿”å›åˆ—è¡¨é‡Œé¢ã€‚è¿™ä¸ªå‡½æ•°çš„å®ç°ä¸Šæœ‰ä¸€ä¸ªæŠ€å·§ï¼Œå°±æ˜¯å¸Œæœ›å°½å¯èƒ½è®©è¿”å›çš„åˆ—è¡¨ä¸­æ¡¶çš„é¡ºåºæŒ‰æ¨¡å‹ä¸­å¼ é‡å‡ºç°çš„é¡ºåºæ’åˆ—ã€‚åœ¨æ²¡æœ‰ torch ä¸Šå±‚ä»£ç æä¾›æç¤ºçš„æƒ…å†µä¸‹ï¼Œè¿™ä¸ªå‡½æ•°é‡Œé¢å¯¹è¿”å›åˆ—è¡¨ä¸­çš„æ¯ä¸ªæ¡¶é‡Œé¢æœ€å°çš„å¼ é‡åºå·è¿›è¡Œäº†æ’åºï¼Œå‡è®¾åºå·å°çš„å¼ é‡æ˜¯ä¼˜å…ˆå‡ºç°åœ¨æ¨¡å‹ä¸­çš„å‚æ•°ã€‚å›åˆ°<code>self._ddp_init_helper()</code>ä¸­ï¼Œå®ƒåˆå°† <code>compute_bucket_assignment_by_size()</code> è¿”å›çš„åˆ—è¡¨ç¿»è½¬äº†ä¸€ä¸‹ï¼Œå¸Œæœ›ä¼˜å…ˆå¤„ç†å…ˆè¢«åå‘ä¼ æ’­è¿‡ç¨‹å¤„ç†åˆ°çš„å¼ é‡ã€‚</p><p>å‰é¢æåˆ°çš„ <code>torch/csrc/distributed/c10d/reducer.cpp</code> ä¸­çš„ <code>Reducer</code>ï¼Œè¿›è¡Œ all reduce é€šä¿¡çš„æ–¹æ³•å®é™…ä¸Šæ˜¯ <code>Reducer::run_comm_hook()</code>ã€‚å®ƒçš„è°ƒç”¨é“¾è·¯æ˜¯ <code>Reducer::autograd_hook()-&gt; Reducer::mark_variable_ready() -&gt; Reducer::mark_bucket_ready() -&gt; Reducer::all_reduce_bucket() -&gt; Reducer::run_comm_hook()</code>ã€‚<code>Reducer::autograd_hook()</code> è¢«æ³¨å†Œç»™äº† <code>torch::autograd::impl::grad_accumulator::add_post_hook()</code>ï¼Œä¼šåœ¨æ¢¯åº¦è®¡ç®—å®Œæ¯•ç´¯åŠ åˆ°äº†æ¢¯åº¦å¼ é‡åæ‰§è¡Œï¼Œè€Œä¸”åªä¼šåœ¨ pytorch çš„ autograd çº¿ç¨‹ä¸Šæ‰§è¡Œã€‚</p><blockquote><p>torch çš„ autograd çº¿ç¨‹åœ¨ torch/csrc/autograd/engine.cpp çš„ <code>Engine::start_device_threads()-&gt;Engine::thread_init()</code> åˆ›å»ºã€‚</p></blockquote><div class="language-cpp"><button title="Copy Code" class="copy"></button><span class="lang">cpp</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#FFCB6B;">c10</span><span style="color:#89DDFF;">::</span><span style="color:#FFCB6B;">intrusive_ptr</span><span style="color:#89DDFF;">&lt;</span><span style="color:#FFCB6B;">c10</span><span style="color:#89DDFF;">::</span><span style="color:#FFCB6B;">ivalue</span><span style="color:#89DDFF;">::</span><span style="color:#FFCB6B;">Future</span><span style="color:#89DDFF;">&gt;</span><span style="color:#A6ACCD;"> </span><span style="color:#FFCB6B;">Reducer</span><span style="color:#89DDFF;">::</span><span style="color:#82AAFF;">run_comm_hook</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#FFCB6B;">GradBucket</span><span style="color:#C792EA;">&amp;</span><span style="color:#A6ACCD;"> </span><span style="color:#A6ACCD;font-style:italic;">grad_bucket</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">  </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">comm_hook_ </span><span style="color:#89DDFF;">==</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">nullptr)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // `Reducer` æ„é€ æ—¶çš„å‚æ•°æ²¡æœ‰é…ç½®è¿‡çš„è¯ï¼Œä¼šä» `Reducer::process_group_` æ„é€ ä¸€ä¸ª `_AllReduceBySumCommHook`ï¼Œ</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">    // ç„¶åå¯¹æ¯ä¸ª bucket åš all reduceã€‚</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#F07178;"> </span><span style="color:#82AAFF;">run_allreduce_hook</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">grad_bucket</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">  </span><span style="color:#89DDFF;">}</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;font-style:italic;">else</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#F07178;">    </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#F07178;"> </span><span style="color:#A6ACCD;">comm_hook_</span><span style="color:#89DDFF;">-&gt;</span><span style="color:#82AAFF;">runHook</span><span style="color:#89DDFF;">(</span><span style="color:#F07178;">grad_bucket</span><span style="color:#89DDFF;">);</span></span>
<span class="line"><span style="color:#F07178;">  </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#89DDFF;">}</span></span>
<span class="line"></span></code></pre></div><p>åˆå§‹åŒ–å‡½æ•°é‡Œé¢è¿˜æœ‰ä¸€æ®µæœ‰è¶£çš„ä»£ç æ˜¯é€šè¿‡ <code>torch._dynamo.config._get_optimize_ddp_mode()</code> è·å–äº† torch 2 ä¸­ç¼–è¯‘å™¨å‰ç«¯ torchdynamo çš„è®¾ç½®ï¼Œé€‰æ‹©å¯ç”¨å®éªŒæ€§çš„ python reducerï¼Œè€Œä¸æ˜¯ç”¨é»˜è®¤çš„ cpp å®ç°çš„ reducerã€‚å»è¯» <code>torch/_dynamo/config.py</code> çš„è¯å°±ä¼šçŸ¥é“ï¼Œ<code>DistributedDataParallel</code> é»˜è®¤ä¼šå¯¹ DDP çš„é€šä¿¡å’Œè®¡ç®—åš overlap ä»¥æ©ç›–é€šä¿¡å¼€é”€ã€‚ä½†æ˜¯ç”±äº torch 2 ä¾èµ–äº PEP 523ï¼Œæ‰€ä»¥çº¯ python å®ç°çš„ reducer æ›´æœ‰åˆ©äº torch åšè‡ªåŠ¨åˆ†æå’Œä¼˜åŒ–ã€‚æ¯”å¦‚é€šå¸¸è·¯å¾„ä¸Š all_reduce æ“ä½œè°ƒç”¨çš„æ˜¯ <code>torch.distributed.distributed_c10d.all_reduce()</code>ï¼Œä½†æ˜¯ python ç‰ˆçš„æ˜¯ <code>torch.distributed._functional_collectives.all_reduce()</code> ä¸­çš„å®ç°ã€‚é€šè¿‡ <code>DistributedDataParallel._get_active_ddp_module()</code> ç±»æ–¹æ³•å¯ä»¥æŠŠ DDP å¯¹è±¡æš´éœ²ç»™ torchdynamoï¼Œ<code>DistributedDataParallel._inside_ddp_forward()</code> è¿™ä¸ª contextmanager åˆ™æ˜¯åœ¨è°ƒç”¨ <code>DistributedDataParallel._run_ddp_forward()</code> å‰å°±ç¦ç”¨äº† torchdynamoã€‚</p><p>å¯¹äº <code>torch.distributed.distributed_c10d.all_reduce()</code>ï¼Œå®ƒé»˜è®¤è°ƒç”¨çš„æ˜¯ <code>_get_default_group()</code> è·å–çš„ <code>ProcessGroup</code> ä¸Šçš„ <code>all_reduce()</code> æ¥å£ï¼Œè¿™å®ç°åœ¨ <code>torch/csrc/distributed/c10d/ProcessGroupWrapper.cpp</code> ä¸­çš„å…¬å…±æŠ½è±¡ <code>ProcessGroupWrapper</code>ï¼Œå®ƒç»§æ‰¿äº <code>Backend</code> ç±»å‹ã€‚torch æ”¯æŒäº†å¤šç§é›†åˆé€šä¿¡åº“ï¼Œæ¯ä¸ªå®ç°ä¹Ÿéƒ½ç»§æ‰¿äº† <code>Backend</code>ï¼Œå¦‚ NCCLã€GLOOã€UCC ç­‰ã€‚ä¸€èˆ¬æˆ‘ä»¬åªå…³å¿ƒ NCCLï¼Œå®ƒå®ç°åœ¨ <code>torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp</code>ã€‚</p><p>é€šè¿‡ <code>PYTORCH_DDP_USE_SIDE_STREAM</code> ç¯å¢ƒå˜é‡å¯ä»¥æ–°å¼€ä¸€ä¸ª cuda steram åš H2D çš„æ‹·è´ã€‚</p><h2 id="torch-utils-data-distributed-py" tabindex="-1">torch/utils/data/distributed.py <a class="header-anchor" href="#torch-utils-data-distributed-py" aria-hidden="true">#</a></h2><p><code>DistributedSampler</code> é…åˆ <code>DistributedDataParallel</code> ä½¿ç”¨ï¼Œå¯¹è¾“å…¥è¿›è¡Œåˆ†ç‰‡ã€‚ å®ç°å¾ˆç®€å•ï¼Œå°±æ˜¯åœ¨ <code>self.__iter__</code> å‡½æ•°ä¸­ <code>indices[self.rank : self.total_size : self.num_replicas]</code>ï¼Œå¯¹æ•´ä¸ª <code>self.total_size</code> é•¿çš„æ•°æ®ï¼Œé—´éš” <code>self.num_replicas</code> æŒ‰ <code>self.rank</code> é€‰ä¸€ä¸ªã€‚<code>self.rank</code> å¯ä»¥é€šè¿‡ <code>dist.get_rank</code> è·å–ã€‚</p><h2 id="torch-distributed" tabindex="-1">torch/distributed/ <a class="header-anchor" href="#torch-distributed" aria-hidden="true">#</a></h2><p><code>DTensor</code> çš„å„ç§ TP æ–¹å¼çš„å®ç°ï¼Œå’Œä¸å…¶ä»–å¹¶è¡Œæ–¹å¼çš„é›†æˆã€‚DTensor æœ¬èº«å®ç°åœ¨ <code>torch/distributed/_tensor/api.py</code>ã€‚</p><h3 id="tensor-parallel-ddp-py" tabindex="-1">tensor/parallel/ddp.py <a class="header-anchor" href="#tensor-parallel-ddp-py" aria-hidden="true">#</a></h3><p><code>DistributedDataParallel</code> ä¸­è°ƒç”¨çš„ <code>_pre_dp_module_transform()</code> çš„å®ç°ï¼Œä¾¿äº DDP å’Œ TP ç»“åˆï¼ˆtorch çš„ TP ä¾èµ–äº DTensorï¼‰ã€‚å®ƒæ³¨å†Œäº†ä¸¤ä¸ªæ›´æ–° DTensor çš„é’©å­ï¼Œä¸€ä¸ªç”¨äºåœ¨å‰å‘ä¼ æ’­ä¹‹å‰å°†æœ¬åœ°å¼ é‡è½¬æ¢å› DTensorï¼Œå¦ä¸€ä¸ªç”¨äºåœ¨å‰å‘ä¼ æ’­ä¹‹åå°† DTensor è½¬æ¢å›å¼ é‡ã€‚é¿å… DDP å¯¹ DTensor å‚æ•°çš„ç‰¹æ®Šå¤„ç†ï¼Œå¹¶ä½¿ DTensor çš„æ¢¯åº¦èƒ½å¤Ÿä¼ é€’å› DDP çš„æ¢¯åº¦æ¡¶ã€‚</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#C792EA;">def</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">_pre_dp_module_transform</span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;font-style:italic;">module</span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> nn</span><span style="color:#89DDFF;">.</span><span style="color:#F07178;">Module</span><span style="color:#89DDFF;">):</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#82AAFF;">_localize_dtensor</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">module</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">None,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">None)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># Recontruct DTensor parameters from local tensors</span></span>
<span class="line"><span style="color:#A6ACCD;">    module</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">register_forward_pre_hook</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">_reconstruct_dtensor</span><span style="color:#89DDFF;">)</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#676E95;font-style:italic;"># Convert DTensor parameters to local tensors</span></span>
<span class="line"><span style="color:#A6ACCD;">    module</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">register_forward_hook</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">_localize_dtensor</span><span style="color:#89DDFF;">)</span></span>
<span class="line"></span></code></pre></div><h3 id="nn" tabindex="-1">nn <a class="header-anchor" href="#nn" aria-hidden="true">#</a></h3><p>å®šä¹‰äº† torch ç”¨æˆ·å¯ä»¥ä¸»åŠ¨ä½¿ç”¨çš„é›†åˆé€šä¿¡æ¥å£ <code>torch.distributed.nn.functional</code>ï¼Œä¸»åŠ¨åˆ›å»ºä½äºè¿œç«¯è¿›ç¨‹çš„ module <code>torch.distributed.nn.RemoteModule</code>ã€‚</p><h3 id="algorithms" tabindex="-1">algorithms <a class="header-anchor" href="#algorithms" aria-hidden="true">#</a></h3><p>ä¸€äº›åˆ†å¸ƒå¼ä¸‹çš„ç®—æ³•å®ç°ã€‚</p><p>å¦‚ <code>torch/distributed/algorithms/model_averaging/averagers.py</code> å®šä¹‰äº†ç”¨æˆ·å¯ä»¥ç›´æ¥è°ƒç”¨çš„å¯¹å„ä¸ª rank çš„å‚æ•°åšå‡å€¼çš„ <code>PeriodicModelAverager</code>ï¼Œå¯ä»¥ç”¨äºä¸»åŠ¨åŒæ­¥æ¨¡å‹å‚æ•°ã€å’Œ PostLocalSGDOptimizer ç»“åˆç”¨äºä¼˜åŒ–å™¨ç­‰ã€‚</p><p><code>torch/distributed/optim/</code> å®ç°åˆ†å¸ƒå¼çš„ä¼˜åŒ–å™¨ï¼Œä¾‹å¦‚ <code>PostLocalSGDOptimizer</code>ã€<code>ZeroRedundancyOptimizer</code>ã€‚</p><p><code>torch/distributed/algorithms/_comm_hooks/default_hooks.py</code> æ˜¯åˆ†å¸ƒå¼è®­ç»ƒä¸­é»˜è®¤çš„ hookï¼Œå¯ä»¥é€šè¿‡ <code>model.register_comm_hook</code> æ³¨å†Œåˆ«çš„ hookã€‚å¦‚ <code>torch/distributed/algorithms/ddp_comm_hooks/</code> ä¸‹çš„ <code>allreduce_hook()</code>ã€<code>post_localSGD_hook()</code>ã€‚æ³¨å†Œ hook çš„å‡½æ•°æ˜¯ <code>DistributedDataParallel.register_comm_hook()</code></p><h2 id="torch-nn-parallel-data-parallel-py" tabindex="-1">torch/nn/parallel/data_parallel.py <a class="header-anchor" href="#torch-nn-parallel-data-parallel-py" aria-hidden="true">#</a></h2><p>å®ç°äº† <code>DataParallel</code>ï¼Œä»…æŒæœ‰ä¸€ä»½æ¨¡å‹ï¼Œæ¯æ¬¡å‰å‘æ›´æ–°éƒ½ä¼šåœ¨ä¸åŒè®¾å¤‡é—´æ‹·è´éœ€è¦å¹¶è¡Œçš„å‚æ•°ï¼Œä¸€èˆ¬å·²ä¸ä½¿ç”¨ã€‚</p><p><code>torch.nn.parallel.replicate()</code> ç”¨äºåœ¨å„ä¸ªè®¾å¤‡ä¸Šå¤åˆ¶æ¨¡å‹ï¼Œå®ƒä¸»è¦è°ƒç”¨äº† <code>_broadcast_coalesced_reshape() -&gt; comm._broadcast_coalesced()</code>ã€‚<code>broadcast_coalesced()</code> æ˜¯ä¸ª C å‡½æ•°ï¼Œå®ç°åœ¨ <code>torch/csrc/cuda/comm.cpp</code>ã€‚å…·ä½“å®ç°åœ¨ <code>_broadcast_out_impl</code>ï¼Œè¿™é‡Œé€šè¿‡ä¸€ä¸ªå®æ§åˆ¶äº†æ˜¯å¦ä½¿ç”¨ NCCLï¼Œå¦åˆ™å°±æ˜¯ç›´æ¥ CUDA D2D æ‹·è´ã€‚</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki material-palenight"><code><span class="line"><span style="color:#A6ACCD;">static inline std</span><span style="color:#89DDFF;">::</span><span style="color:#A6ACCD;">vector</span><span style="color:#89DDFF;">&lt;</span><span style="color:#A6ACCD;">Tensor</span><span style="color:#89DDFF;">&gt;&amp;</span><span style="color:#A6ACCD;"> </span><span style="color:#82AAFF;">_broadcast_out_impl</span><span style="color:#89DDFF;">(</span></span>
<span class="line"><span style="color:#82AAFF;">    const Tensor</span><span style="color:#89DDFF;">&amp;</span><span style="color:#82AAFF;"> tensor</span><span style="color:#89DDFF;">,</span></span>
<span class="line"><span style="color:#82AAFF;">    std</span><span style="color:#89DDFF;">::</span><span style="color:#82AAFF;">vector</span><span style="color:#89DDFF;">&lt;</span><span style="color:#82AAFF;">Tensor</span><span style="color:#89DDFF;">&gt;&amp;</span><span style="color:#82AAFF;"> out_tensors</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">#ifdef USE_NCCL</span></span>
<span class="line"><span style="color:#A6ACCD;">  std</span><span style="color:#89DDFF;">::</span><span style="color:#A6ACCD;">vector</span><span style="color:#89DDFF;">&lt;</span><span style="color:#A6ACCD;">Tensor</span><span style="color:#89DDFF;">&gt;</span><span style="color:#A6ACCD;"> nccl_list;</span></span>
<span class="line"><span style="color:#A6ACCD;">  nccl_list</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">reserve</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">out_tensors</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">size</span><span style="color:#89DDFF;">()</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">+</span><span style="color:#82AAFF;"> </span><span style="color:#F78C6C;">1</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;">;</span></span>
<span class="line"><span style="color:#A6ACCD;">  nccl_list</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">emplace_back</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">tensor</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;">;</span></span>
<span class="line"><span style="color:#A6ACCD;">  </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">auto</span><span style="color:#89DDFF;">&amp;</span><span style="color:#A6ACCD;"> out_tensor </span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> out_tensors</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    nccl_list</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">emplace_back</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">out_tensor</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;">;</span></span>
<span class="line"><span style="color:#A6ACCD;">  </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#A6ACCD;">  </span><span style="color:#89DDFF;font-style:italic;">if</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">nccl</span><span style="color:#89DDFF;">::</span><span style="color:#82AAFF;">is_available</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">nccl_list</span><span style="color:#89DDFF;">))</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">    nccl</span><span style="color:#89DDFF;">::</span><span style="color:#82AAFF;">broadcast</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">nccl_list</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;">;</span></span>
<span class="line"><span style="color:#A6ACCD;">  </span><span style="color:#89DDFF;">}</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;font-style:italic;">else</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">#else</span></span>
<span class="line"><span style="color:#A6ACCD;">  </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#676E95;font-style:italic;">#endif</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;font-style:italic;">for</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">(</span><span style="color:#A6ACCD;">auto</span><span style="color:#89DDFF;">&amp;</span><span style="color:#A6ACCD;"> out_tensor </span><span style="color:#89DDFF;">:</span><span style="color:#A6ACCD;"> out_tensors</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;"> </span><span style="color:#89DDFF;">{</span></span>
<span class="line"><span style="color:#A6ACCD;">      out_tensor</span><span style="color:#89DDFF;">.</span><span style="color:#82AAFF;">copy_</span><span style="color:#89DDFF;">(</span><span style="color:#82AAFF;">tensor</span><span style="color:#89DDFF;">,</span><span style="color:#82AAFF;"> </span><span style="color:#89DDFF;">/*</span><span style="color:#A6ACCD;font-style:italic;">non_blocking</span><span style="color:#89DDFF;">=*/</span><span style="color:#82AAFF;">true</span><span style="color:#89DDFF;">)</span><span style="color:#A6ACCD;">;</span></span>
<span class="line"><span style="color:#A6ACCD;">    </span><span style="color:#89DDFF;">}</span></span>
<span class="line"><span style="color:#A6ACCD;">  }</span></span>
<span class="line"><span style="color:#A6ACCD;">  </span><span style="color:#89DDFF;font-style:italic;">return</span><span style="color:#A6ACCD;"> out_tensors;</span></span>
<span class="line"><span style="color:#A6ACCD;">}</span></span>
<span class="line"></span></code></pre></div><p><code>torch.nn.parallel.parallel_apply()</code> ç”¨äºåœ¨ä¸åŒè®¾å¤‡ä¸Šå¹¶è¡Œè®¡ç®—ï¼Œèµ°å¾—å°±æ˜¯åœ¨ä¸åŒçº¿ç¨‹ã€ä¸åŒ <code>torch.cuda.device()ã€torch.cuda.stream()</code> ä¸‹è°ƒç”¨ module çš„æ–¹å¼ã€‚</p><h2 id="torch-randn-çš„å®ç°" tabindex="-1">torch.randn() çš„å®ç° <a class="header-anchor" href="#torch-randn-çš„å®ç°" aria-hidden="true">#</a></h2><p>randnçš„å…·ä½“å®ç°æ–¹å¼ /aten/src/ATen/native/TensorFactories.cpp: Tensor rand() -&gt; /aten/src/ATen/native/Distributions.cpp: Tensor&amp; uniform_() -&gt; /aten/src/ATen/native/DistributionTemplates.h: at::Tensor&amp; uniform_impl_() -&gt; /aten/src/ATen/native/cuda/DistributionUniform.cu: void uniform_kernel() -&gt; /aten/src/ATen/native/cuda/DistributionTemplates.h: void uniform_kernel(), void uniform_and_transform()ï¼Œvoid distribution_nullary_kernel() void uniform_and_transform() é‡Œé¢æ ¹æ®æ•°æ®ç±»å‹ï¼Œé€šè¿‡ distribution_nullary_kernel()åŠ è½½äº†ä¸€ä¸ªéå† Tensor çš„æ ¸å‡½æ•°ï¼Œé€é¡¹è°ƒç”¨ curand API curand_uniform4 æˆ– curand_uniform2_doubleè¿›è¡Œå¡«å……ã€‚ åŠ¨æ€è·å– cuda API ç¬¦å· <a href="https://github.com/pytorch/pytorch/blob/main/c10/cuda/driver_api.cpp#L40" target="_blank" rel="noreferrer">https://github.com/pytorch/pytorch/blob/main/c10/cuda/driver_api.cpp#L40</a> åœ¨å•ä¸ªå•ä¾‹ä¸­ï¼Œæœç´¢ cuda driver API å’Œ nvml API</p><h2 id="aten-src-aten-native-tensorfactories-cpp" tabindex="-1">/aten/src/ATen/native/TensorFactories.cpp <a class="header-anchor" href="#aten-src-aten-native-tensorfactories-cpp" aria-hidden="true">#</a></h2><p>å¼ é‡çš„å·¥å‚ç±»</p><h2 id="aten-src-aten-native-convolution-cpp" tabindex="-1">/aten/src/ATen/native/Convolution.cpp <a class="header-anchor" href="#aten-src-aten-native-convolution-cpp" aria-hidden="true">#</a></h2><p>å·ç§¯å®ç°ï¼Œ ä¾‹å¦‚æ­£å‘æ¨ç†çš„ cudnn å®ç° cudnn_convolution -&gt; cudnn_convolution_forward -&gt; raw_cudnn_convolution_forward_out -&gt; raw_cudnn_convolution_forward_out_32bit -&gt; cudnnConvolutionForward <a href="https://github1s.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/cudnn/ConvShared.cpp#L180" target="_blank" rel="noreferrer">https://github1s.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/cudnn/ConvShared.cpp#L180</a><a href="https://github1s.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/cudnn/Conv_v7.cpp#L629" target="_blank" rel="noreferrer">https://github1s.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/cudnn/Conv_v7.cpp#L629</a> ä¾‹å¦‚åå‘ä¼ æ’­çš„å®ç° <a href="https://github1s.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/Convolution.cpp#L1974-L1978" target="_blank" rel="noreferrer">https://github1s.com/pytorch/pytorch/blob/v2.1.0/aten/src/ATen/native/Convolution.cpp#L1974-L1978</a> è¿™é‡Œå»æ ¹æ®åç«¯é€‰æ‹©å¯¹åº”å®ç°ï¼Œä¾‹å¦‚ cudnn_convolution_backward_stubï¼Œå®ç°åœ¨ /aten/src/ATen/native/cudnn/ConvShared.cppã€‚</p><h2 id="aten-src-aten-native-cuda-cudaloops-cuh" tabindex="-1">/aten/src/ATen/native/cuda/CUDALoops.cuh <a class="header-anchor" href="#aten-src-aten-native-cuda-cudaloops-cuh" aria-hidden="true">#</a></h2><p>å€ŸåŠ© cuda éå† Tensor ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼Œä¸Šè¿°å·¥å‚ç±»ä¸­ä¼šé€šè¿‡ Tensor::fill_()æ–¹æ³•è°ƒç”¨åˆ°å®ƒï¼Œå¯¹å¼ é‡è¿›è¡Œèµ‹å€¼ã€‚</p><h2 id="aten-src-aten-native-native-functions-yaml" tabindex="-1">/aten/src/ATen/native/native_functions.yaml <a class="header-anchor" href="#aten-src-aten-native-native-functions-yaml" aria-hidden="true">#</a></h2><p>æ¯ä¸ª native ç®—å­æœ‰å¤šä¸ªåç«¯çš„ native å®ç°ï¼Œè¯¥æ–‡ä»¶æè¿°äº†è¿™äº›å˜ä½“ã€‚ ä¾‹å¦‚ fft å˜æ¢ï¼Œ/aten/src/ATen/native/SpectralOps.cpp ä¸­çš„ Tensor stft()è°ƒç”¨äº†å¯¹åº”çš„ native ç®—å­ _fft_r2cï¼Œè¿™åˆå¯¹åº”äº†ä¸¤ç±»åç«¯å®ç°ï¼Œ_fft_r2c_cufft å’Œ _fft_r2c_mkl</p><ul><li>func: _fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -&gt; Tensor variants: function dispatch: CPU: _fft_r2c_mkl CUDA: _fft_r2c_cufft</li></ul></div></div></main><!--[--><!--]--><!----><!--[--><!--[--><!--[--><!----><!--]--><!--]--><!--]--></div></div></div></div></div><!----><!--[--><!--]--></div><!----><footer data-v-4f0db67d> Powered by <a href="https://github.com/forsworns/" target="_blank" title="Author" data-v-4f0db67d>Peihao Yang</a> | Copyright Â© 2019-2024 | MIT License </footer><!--]--></div>
    <script>__VP_HASH_MAP__ = JSON.parse("{\"about-me_index.md\":\"ae155f90\",\"zh_blogs_20190721_index.md\":\"4f1bd34e\",\"index.md\":\"431bcd57\",\"zh_about-me_index.md\":\"cb485e9f\",\"zh_blogs_20190901_index.md\":\"0f58967a\",\"zh_blogs_20190824_index.md\":\"c9e31211\",\"zh_blogs_20190919_index.md\":\"660a84aa\",\"zh_blogs_20190908_index.md\":\"b837c695\",\"zh_blogs_20191109_index.md\":\"f3f202fd\",\"zh_blogs_20191112_index.md\":\"20a903ef\",\"zh_blogs_20210204_index.md\":\"97569ac5\",\"zh_blogs_20201023_index.md\":\"848aa516\",\"zh_blogs_20210120_index.md\":\"87b674e6\",\"zh_blogs_20210203_index.md\":\"6662f4c7\",\"zh_blogs_20210123_index.md\":\"1fe092d1\",\"zh_blogs_20200816_index.md\":\"f570af2a\",\"zh_blogs_20200818_index.md\":\"e18d8843\",\"zh_blogs_20200616_index.md\":\"1909c34c\",\"zh_blogs_20200817_index.md\":\"e7f1158c\",\"zh_blogs_20191102_index.md\":\"e1722464\",\"zh_blogs_20191103_index.md\":\"9b710856\",\"zh_blogs_20210310_index.md\":\"bb00b880\",\"zh_blogs_20210226_index.md\":\"043359b1\",\"zh_blogs_20210311_index.md\":\"2bdad88d\",\"zh_blogs_20210223_index.md\":\"a04bbf06\",\"zh_blogs_20210224_index.md\":\"8bf1d3c9\",\"zh_blogs_20210430_index.md\":\"7d887266\",\"zh_blogs_20210506_index.md\":\"9a4cc383\",\"zh_blogs_20230125_index.md\":\"0e4ab5d9\",\"zh_blogs_20230126_index.md\":\"961f0979\",\"zh_blogs_20230121_index.md\":\"1e80c99d\",\"zh_blogs_20230201_repost.md\":\"16684978\",\"zh_blogs_20230322_index.md\":\"8ba4df8b\",\"zh_blogs_20230209_index.md\":\"d6571f6a\",\"zh_blogs_20230201_index.md\":\"5a07f4b5\",\"zh_blogs_20230601_index.md\":\"cbd6f55d\",\"zh_blogs_20240220_index.md\":\"6f23a2ab\",\"zh_blogs_20240413_index.md\":\"6cfc1faa\",\"zh_blogs_20240427_index.md\":\"92f8531f\",\"zh_blogs_20240423_index.md\":\"e39dc142\",\"zh_blogs_20240513_index.md\":\"5054ded0\",\"zh_blogs_20210706_index.md\":\"e46538a1\",\"zh_blogs_20210312_index.md\":\"a97c91e4\",\"zh_blogs_20240215_index.md\":\"29b27448\",\"zh_blogs_20240626_index.md\":\"6d490338\",\"zh_blogs_20240909_index.md\":\"9e9ac428\",\"zh_blogs_20240924_index.md\":\"c16775d2\",\"zh_blogs_20240916_index.md\":\"34a833d6\",\"zh_blogs_20240526_index.md\":\"5cc8945c\",\"zh_blogs_index.md\":\"284bf08e\",\"zh_blogs_20210409_index.md\":\"3374f5af\",\"zh_blogs_20220105_index.md\":\"057ef58c\",\"zh_blogs_20220101_index.md\":\"28fca584\",\"zh_blogs_20211210_index.md\":\"5cf5ec3b\",\"zh_blogs_20220224_index.md\":\"cdd187f4\",\"zh_blogs_20240622_index.md\":\"4921f51d\",\"zh_blogs_20210329_index.md\":\"48cb84e7\",\"zh_blogs_20220316_index.md\":\"09c75bb7\",\"zh_blogs_20241008_index.md\":\"cd84a035\",\"zh_blogs_20240623_index.md\":\"02de25e1\",\"zh_blogs_20221108_index.md\":\"e5b7be76\",\"zh_blogs_20221024_index.md\":\"8ebba006\",\"zh_blogs_20210315_index.md\":\"13799d08\",\"zh_blogs_20240128_index.md\":\"9236e79c\",\"zh_blogs_20230101_index.md\":\"e779c786\",\"zh_blogs_20210728_index.md\":\"e67450f5\",\"zh_blogs_20211120_index.md\":\"8e740225\",\"zh_blogs_20210627_index.md\":\"1767f8a9\",\"zh_blogs_20210822_index.md\":\"6aba4591\",\"zh_blogs_20220611_index.md\":\"48a2162f\",\"zh_blogs_20240519_index.md\":\"917234d5\",\"zh_blogs_20240518_index.md\":\"cd515693\",\"zh_blogs_20210715_index.md\":\"16ce33cc\",\"zh_blogs_20210412_index.md\":\"cb1d48be\",\"zh_blogs_20240620_index.md\":\"cadc4632\",\"zh_index.md\":\"bd0115db\",\"zh_blogs_tags_index.md\":\"08460bbb\",\"zh_blogs_20211130_index.md\":\"e06fc018\",\"zh_blogs_20210801_index.md\":\"640475cd\",\"zh_blogs_20221228_index.md\":\"9dc40de8\",\"zh_blogs_20241020_index.md\":\"8fc01acc\",\"zh_blogs_20211002_index.md\":\"c2dacf15\"}")</script>
    <script type="module" async src="/assets/app.0f5a0ae1.js"></script>
    
  </body>
</html>